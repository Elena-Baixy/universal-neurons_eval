{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b735a6d2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aedbdae3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ff2c0c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original repo structure ===\n",
      "__pycache__\n",
      "dataframes\n",
      "summary.py\n",
      "entropy_intervention.py\n",
      "paper_notebooks\n",
      "activations.py\n",
      "LICENSE\n",
      "requirements.txt\n",
      ".git\n",
      "make_dataset.py\n",
      "attention_deactivation_qpos.py\n",
      "correlations_parallel.py\n",
      "evaluation\n",
      "summary_viewer.py\n",
      "weights.py\n",
      "attention_deactivation.py\n",
      "correlations_fast.py\n",
      "documentation.pdf\n",
      ".gitignore\n",
      "explain.py\n",
      "CodeWalkthrough.md\n",
      "utils.py\n",
      "plan.md\n",
      "correlations.py\n",
      "analysis\n",
      "intervention.py\n",
      "slurm\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the original repo structure first\n",
    "original_repo = '/net/scratch2/smallyan/universal-neurons_eval'\n",
    "replication_dir = '/net/scratch2/smallyan/universal-neurons_eval/evaluation/replications'\n",
    "\n",
    "# List files in original repo\n",
    "print(\"=== Original repo structure ===\")\n",
    "for item in os.listdir(original_repo):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc470e24",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Looking for documentation files ===\n",
      "/net/scratch2/smallyan/universal-neurons_eval/documentation.pdf\n",
      "/net/scratch2/smallyan/universal-neurons_eval/CodeWalkthrough.md\n",
      "/net/scratch2/smallyan/universal-neurons_eval/plan.md\n",
      "/net/scratch2/smallyan/universal-neurons_eval/evaluation/replications/documentation_replication.md\n",
      "/net/scratch2/smallyan/universal-neurons_eval/evaluation/replications/evaluation_replication.md\n"
     ]
    }
   ],
   "source": [
    "# Check for documentation files in original repo\n",
    "print(\"=== Looking for documentation files ===\")\n",
    "for root, dirs, files in os.walk(original_repo):\n",
    "    for f in files:\n",
    "        if 'documentation' in f.lower() or f.endswith('.md'):\n",
    "            print(os.path.join(root, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80eabcc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CodeWalkthrough.md ===\n",
      "# Universal Neurons\n",
      "All supporting data and code for Universal Neurons in GPT2 Language Models by Gurnee et al. (2024).\n",
      "\n",
      "## Contents\n",
      "* `dataframes/neuron_dfs` contains dataframes with neuron statistics for all neurons for the main models studies.\n",
      "* `paper_notebooks` contains much of the plotting code to generate the figures in the paper.\n",
      "* `correlations_fast.py` contains the script to compute neuron correlations.\n",
      "* `summary.py` and `weights.py` contain scripts to compute neuron activation and weight statistic summaries for use of our summary viewer (contained in `summary_viewer.py`). See next section for more information on the data generated.\n",
      "* `activations.py` contains scripts to cache neuron activations.\n",
      "* `explain.py` contains script to compute our reduction in variance explanations.\n",
      "* `attention_deactivation.py`, `entropy_intervention.py`, and `intervention.py` contain scripts for our functional neuron experiments.\n",
      "* The `analysis` directory contains further plotting and analysis code.\n",
      "* The `slurm` directory contains the shell scripts used to run the experiments on the cluster. These are not necessary to run the code, but may be useful to reference if replicating our experiments in a different environment.\n",
      "\n",
      "\n",
      "## Summary Viewer\n",
      "For this project, we leveraged precomputed activation data to explore neurons with our neuron viewer.\n",
      "\n",
      "This data can either be recomputed using `summary.py` and `weights.py` or by downloading the data from our [box](TODO:add link) link. Add this to the top level of the directory. It is organized as follows:\n",
      "\n",
      "```python\n",
      "# Summary data for neuron weights in each model\n",
      "summary_data/model_name/weights/data_file\n",
      "\n",
      "# Summary data for activations of each model within different datasets\n",
      "summary_data/model_name/activations/dataset_name/data_file\n",
      "\n",
      "# This data can be loaded with via the following functions\n",
      "from summary_viewer import load_all_summaries, load_weights_summary\n",
      "dataset_summaries = load_all_summaries(model_name)\n",
      "weight_summaries = load_weights_summary(model_name)\n",
      "```\n",
      "\n",
      "A common pattern in the summary data is \"compressing\" a distribution by binning it while saving the tails. In particular, we compute the following:\n",
      "- `bin_counts`: a histogram of the distribution (where there is either a corresponding `bin_edges` or some standard bin size, look at the code in `weights.py` for details)\n",
      "- `max_ix`: the indices of the top k elements of the distribution\n",
      "- `max_vals`: the values of the top k elements of the distribution\n",
      "- `min_ix`: the indices of the bottom k elements of the distribution\n",
      "- `min_vals`: the values of the bottom k elements of the distribution\n",
      "Though note the naming convention is not always consistent.\n",
      "\n",
      "In particular, within the `weights` directory there is\n",
      "- `neuron_comps.pt` which is a dictionary with keys, `in_in`, `in_out`, `out_in`, `out_out` with values also being a dictionary with 'top_neuron_value', 'top_neuron_ix', 'bottom_neuron_value', 'bottom_neuron_ix', 'comp_hist' corresponding to the summary format described above. The distribution here is the cosine similarity between the {input, output} and {input, output} weight vectors of every pair of neurons in the model. Hence each of these will be a `n_layers x d_mlp x {k or n_bins}` tensor.\n",
      "- `vocab_comps.pt` which is a dict with keys 'E_in', 'U_in', 'E_out', 'U_out' corresponding to the cosine similarities betweein the {Embedding, Unembedding} and the neuron {input, output} weight vectors. Similar to the above, the values of these keys are also a dictionary with a summary data structures above, named 'top_vocab_value', 'top_vocab_ix', 'bottom_vocab_value', 'bottom_vocab_ix', 'comp_hist' again with shape  `n_layers x d_mlp x {k or n_bins}` tensor.\n",
      "- `{k, q, v, o}_comps.pt`: each of these are tensors with shape `n_layers x d_mlp x n_layers x n_heads_per_layer`. They give the composition scores for each combination of neuron and attention head for the {key, query, value, output} vectors. For example, ||W_QK @ n_out|| / (||W_QK|| ||n_out||) for k_comp.\n",
      "\n",
      "Activations are similar, with binned activations histograms for each distribution, as well as mean and max vocab activations for each neuron, and max activating dataset examples.\n",
      "\n",
      "\n",
      "## Cite us\n",
      "```\n",
      "@article{gurnee2024universal,\n",
      "  title={Universal neurons in gpt2 language models},\n",
      "  author={Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},\n",
      "  journal={arXiv preprint arXiv:2401.12181},\n",
      "  year={2024}\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's read CodeWalkthrough.md as the original documentation\n",
    "with open(os.path.join(original_repo, 'CodeWalkthrough.md'), 'r') as f:\n",
    "    original_doc = f.read()\n",
    "print(\"=== CodeWalkthrough.md ===\")\n",
    "print(original_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5149b33d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== plan.md ===\n",
      "# Plan\n",
      "## Objective\n",
      "Study the universality of individual neurons across GPT2 language models trained from different random seeds to identify interpretable neurons and understand whether neural mechanisms are universal across models.\n",
      "\n",
      "## Hypothesis\n",
      "1. Universal neurons (those that consistently activate on the same inputs across different models) are more likely to be monosemantic and interpretable than non-universal neurons.\n",
      "2. Neurons with high activation correlation across models will have clear interpretations and can be taxonomized into a small number of neuron families.\n",
      "3. Universal neurons exhibit specific statistical properties in their weights and activations that distinguish them from non-universal neurons, including large negative input bias, high pre-activation skew and kurtosis, and large weight norm.\n",
      "\n",
      "## Methodology\n",
      "1. Compute pairwise Pearson correlations of neuron activations over 100 million tokens from the Pile test set for every neuron pair across five GPT2 models trained from different random seeds to identify universal neurons with excess correlation above baseline.\n",
      "2. Analyze statistical properties of universal neurons (excess correlation > 0.5) including activation statistics (mean, skew, kurtosis, sparsity) and weight statistics (input bias, cosine similarity between input and output weights, weight decay penalty).\n",
      "3. Develop automated tests using algorithmically generated labels from vocabulary elements and NLP tools (spaCy) to classify neurons into families by computing reduction in activation variance when conditioned on binary test explanations.\n",
      "4. Study neuron functional roles through weight analysis using logit attribution (WU*wout) to identify prediction, suppression, and partition neurons, and analyze moment statistics (kurtosis, skew, variance) of vocabulary effects.\n",
      "5. Perform causal interventions by fixing neuron activations to specific values and measuring effects on layer norm scale, next token entropy, attention head output norms, and BOS attention patterns through path ablation.\n",
      "\n",
      "## Experiments\n",
      "### Neuron correlation analysis across random seeds\n",
      "- What varied: Neuron pairs across five GPT2 models (GPT2-small, GPT2-medium, Pythia-160m) trained from different random initializations\n",
      "- Metric: Pairwise Pearson correlation of neuron activations over 100 million tokens; excess correlation (difference from random baseline)\n",
      "- Main result: Only 1-5% of neurons are universal (excess correlation > 0.5): GPT2-medium 1.23%, Pythia-160M 1.26%, GPT2-small 4.16%. Universal neurons show depth specialization, with most correlated neuron pairs occurring in similar layers.\n",
      "\n",
      "### Statistical properties of universal neurons\n",
      "- What varied: Universal (ϱ>0.5) vs non-universal neurons across GPT2-medium-a, GPT2-small-a, and Pythia-160m\n",
      "- Metric: Activation statistics (mean, skew, kurtosis, sparsity), weight statistics (bias, cosine similarity, weight norm, WU kurtosis), reported as percentiles within layer\n",
      "- Main result: Universal neurons have large weight norm, large negative input bias, high pre-activation skew and kurtosis (monosemantic signature), and lower activation frequency compared to non-universal neurons which show Gaussian-like distributions.\n",
      "\n",
      "### Taxonomization of universal neuron families\n",
      "- What varied: Universal neurons (ϱ>0.5) classified by automated tests using vocabulary properties and NLP labels\n",
      "- Metric: Reduction in activation variance when conditioned on binary test explanations (token properties, syntax, semantics, position)\n",
      "- Main result: Universal neurons cluster into families: unigram neurons (activate for specific tokens, concentrated in layers 0-1), alphabet neurons (18/26 letters), previous token neurons (layers 4-6), position neurons (layers 0-2), syntax neurons (linguistic features), and semantic/context neurons (topics, languages, domains).\n",
      "\n",
      "### Prediction neuron analysis via logit attribution\n",
      "- What varied: Neurons across layers analyzed by moments (kurtosis, skew, variance) of WU*wout distribution\n",
      "- Metric: Kurtosis and skew of vocabulary logit effects; layer-wise percentiles across GPT2-medium models and Pythia models (410M-6.9B)\n",
      "- Main result: After network midpoint, prediction neurons (high kurtosis, positive skew) become prevalent, peaking before final layers where suppression neurons (high kurtosis, negative skew) dominate. Pattern consistent across different seeds and model sizes. Suppression neurons activate more when next token is from the suppressed set.\n",
      "\n",
      "### Entropy modulation neurons via causal intervention\n",
      "- What varied: Fixed activation values (0-6) for entropy neuron L23.945 and anti-entropy neuron L22.2882 vs 20 random neurons from final two layers\n",
      "- Metric: Layer norm scale, next token prediction entropy, reciprocal rank of true token, cross-entropy loss\n",
      "- Main result: Entropy neuron (L23.945) has high weight norm and low logit variance. Increasing activation dramatically increases layer norm scale and prediction entropy while preserving token ranking. Anti-entropy neuron (L22.2882) has opposite effect with cos similarity -0.886 to entropy neuron.\n",
      "\n",
      "### Attention head deactivation neurons via path ablation\n",
      "- What varied: Path ablation of neuron L4.3594 on attention head L5.H0; analyzed all neuron-head pairs using heuristic score hn=WTout*WTQ*kBOS\n",
      "- Metric: Change in BOS attention and head output norm when neuron contribution is ablated; heuristic score distribution vs random baseline\n",
      "- Main result: Heuristic identifies neurons controlling BOS attention. Neuron L4.3594 increases BOS attention and decreases head L5.H0 output norm when activated (deactivation neuron). Median head has WO*vBOS norm 19.4x smaller than other tokens, enabling heads to turn off by attending to BOS.\n"
     ]
    }
   ],
   "source": [
    "# Let's also read the plan.md\n",
    "with open(os.path.join(original_repo, 'plan.md'), 'r') as f:\n",
    "    plan_doc = f.read()\n",
    "print(\"=== plan.md ===\")\n",
    "print(plan_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decbbbbc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== documentation_replication.md ===\n",
      "# Documentation: Universal Neurons Replication\n",
      "\n",
      "## Goal\n",
      "\n",
      "Replicate the key experiments from \"Universal Neurons in GPT2 Language Models\" by Gurnee et al. (2024). The paper investigates whether neurons that consistently activate on the same inputs across different GPT2 models trained from different random seeds are more interpretable.\n",
      "\n",
      "## Data\n",
      "\n",
      "### Source Data\n",
      "- **Pre-computed neuron statistics**: Located in `dataframes/neuron_dfs/` for three models:\n",
      "  - `pythia-160m.csv` (36,864 neurons)\n",
      "  - `stanford-gpt2-small-a.csv` (36,864 neurons)\n",
      "  - `stanford-gpt2-medium-a.csv` (98,304 neurons)\n",
      "\n",
      "### Data Fields\n",
      "Each CSV contains per-neuron statistics:\n",
      "- `layer`, `neuron`: Neuron identification\n",
      "- `max_corr`, `mean_corr`, `min_corr`: Correlation statistics across 5 random seeds\n",
      "- `max_baseline`, `mean_baseline`: Baseline correlation under random rotation\n",
      "- `w_in_norm`, `w_out_norm`: Weight norms\n",
      "- `input_bias`: MLP input bias\n",
      "- `in_out_sim`: Cosine similarity between input/output weights\n",
      "- `l2_penalty`: L2 regularization penalty\n",
      "- `mean`, `var`, `skew`, `kurt`: Activation statistics\n",
      "- `vocab_mean`, `vocab_var`, `vocab_skew`, `vocab_kurt`: Vocabulary effect statistics\n",
      "- `sparsity`: Activation frequency\n",
      "\n",
      "### Computed Metrics\n",
      "- `excess_corr = mean_corr - mean_baseline`: Key universality metric\n",
      "- `is_universal = excess_corr > 0.5`: Binary classification threshold\n",
      "\n",
      "## Method\n",
      "\n",
      "### Experiment 1: Universal Neuron Identification\n",
      "1. Load pre-computed correlation data from CSV files\n",
      "2. Compute excess correlation (mean_corr - mean_baseline)\n",
      "3. Classify neurons as universal if excess_corr > 0.5\n",
      "4. Calculate percentage of universal neurons per model\n",
      "\n",
      "### Experiment 2: Statistical Properties Analysis\n",
      "1. For each metric, compute percentiles within each layer (fair normalization)\n",
      "2. Compare distributions between universal and non-universal neurons\n",
      "3. Verify five key claims:\n",
      "   - Universal neurons have lower sparsity (activate less frequently)\n",
      "   - Universal neurons have higher activation skew\n",
      "   - Universal neurons have higher activation kurtosis\n",
      "   - Universal neurons have more negative input bias\n",
      "   - Universal neurons have larger L2 penalty (weight norm)\n",
      "\n",
      "### Experiment 3: Layer Distribution\n",
      "1. Count universal neurons per layer\n",
      "2. Compute percentage per layer\n",
      "3. Visualize depth specialization patterns\n",
      "\n",
      "## Results\n",
      "\n",
      "### Universal Neuron Percentages\n",
      "| Model | Expected | Replicated | Match |\n",
      "|-------|----------|------------|-------|\n",
      "| pythia-160m | 1.26% | 1.26% | ✓ |\n",
      "| stanford-gpt2-small-a | 4.16% | 4.16% | ✓ |\n",
      "| stanford-gpt2-medium-a | 1.23% | 1.23% | ✓ |\n",
      "\n",
      "### Statistical Properties Verification\n",
      "All five properties verified across all three models:\n",
      "1. **Lower activation frequency**: ✓ PASS (all models)\n",
      "2. **High pre-activation skew**: ✓ PASS (all models)\n",
      "3. **High pre-activation kurtosis**: ✓ PASS (all models)\n",
      "4. **Large negative input bias**: ✓ PASS (all models)\n",
      "5. **Large weight norm (L2 penalty)**: ✓ PASS (all models)\n",
      "\n",
      "### Generated Figures\n",
      "1. `universal_neurons_properties.png`: Box plots showing percentile distributions of universal neuron properties\n",
      "2. `layer_distribution.png`: Bar charts showing universal neuron distribution across layers\n",
      "\n",
      "## Analysis\n",
      "\n",
      "### Key Findings\n",
      "1. The replication exactly matches the paper's reported percentages of universal neurons\n",
      "2. All statistical property claims are verified: universal neurons have distinctive signatures\n",
      "3. The \"monosemantic signature\" (high skew and kurtosis) is confirmed\n",
      "4. Layer distribution shows depth specialization patterns\n",
      "\n",
      "### Methodology Notes\n",
      "- Percentile normalization within layers is important for fair comparison\n",
      "- The 0.5 excess correlation threshold clearly separates two populations\n",
      "- Pre-computed data from the original repository was used for efficiency\n",
      "\n",
      "### Limitations\n",
      "- Full correlation computation was not re-run (would require significant compute)\n",
      "- Model weight verification was attempted but faced kernel output issues\n",
      "- Detailed neuron family taxonomy was not fully replicated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's read the replicated documentation\n",
    "with open(os.path.join(replication_dir, 'documentation_replication.md'), 'r') as f:\n",
    "    replication_doc = f.read()\n",
    "print(\"=== documentation_replication.md ===\")\n",
    "print(replication_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1488b7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== evaluation_replication.md ===\n",
      "# Evaluation: Universal Neurons Replication\n",
      "\n",
      "## Reflection\n",
      "\n",
      "This replication successfully verified the core claims from \"Universal Neurons in GPT2 Language Models\" by Gurnee et al. (2024). The repository provided well-organized pre-computed data and clear documentation in the plan.md and CodeWalkthrough.md files.\n",
      "\n",
      "### What Worked Well\n",
      "1. **Clear plan documentation**: The plan.md file explicitly stated hypotheses, methodology, and expected results\n",
      "2. **Pre-computed data availability**: CSV files with neuron statistics enabled direct verification\n",
      "3. **Consistent data format**: All three models used identical CSV schemas\n",
      "4. **Exact numerical matches**: Universal neuron percentages matched precisely\n",
      "\n",
      "### Challenges Encountered\n",
      "1. **Kernel output suppression**: After loading transformer_lens models, Jupyter output was intermittently suppressed\n",
      "2. **Large model sizes**: Full correlation recomputation would require significant GPU time\n",
      "3. **External dependencies**: Some summary data referenced in CodeWalkthrough.md requires separate download\n",
      "\n",
      "### Deviations from Original\n",
      "- Used pre-computed correlation statistics rather than recomputing from scratch\n",
      "- Did not replicate the full neuron family taxonomy analysis\n",
      "- Model weight verification was limited due to kernel issues\n",
      "\n",
      "---\n",
      "\n",
      "## Replication Evaluation - Binary Checklist\n",
      "\n",
      "### RP1. Implementation Reconstructability\n",
      "\n",
      "**PASS**\n",
      "\n",
      "**Rationale**: The experiment can be fully reconstructed from the plan.md and CodeWalkthrough.md files. The plan clearly specifies:\n",
      "- Hypotheses to test\n",
      "- Methodology (Pearson correlation, thresholds, metrics)\n",
      "- Expected results (exact percentages)\n",
      "- Data sources and processing steps\n",
      "\n",
      "The code in correlations_fast.py, utils.py, and analysis/ modules implements the described methodology. No major guesswork was required - the 0.5 excess correlation threshold and statistical property comparisons were explicitly documented.\n",
      "\n",
      "---\n",
      "\n",
      "### RP2. Environment Reproducibility\n",
      "\n",
      "**PASS**\n",
      "\n",
      "**Rationale**: The environment can be reproduced successfully:\n",
      "- requirements.txt lists all dependencies (torch, transformer-lens, pandas, numpy, etc.)\n",
      "- All required packages installed without conflicts\n",
      "- Pre-computed dataframes loaded correctly\n",
      "- Model loading via transformer_lens worked (though with kernel output issues)\n",
      "- CUDA GPU was available and utilized\n",
      "\n",
      "No irrecoverable dependency issues were encountered. The only minor issue was intermittent output suppression in Jupyter after model loading, which did not prevent verification.\n",
      "\n",
      "---\n",
      "\n",
      "### RP3. Determinism and Stability\n",
      "\n",
      "**PASS**\n",
      "\n",
      "**Rationale**: Results are deterministic and stable:\n",
      "- Universal neuron percentages matched exactly across runs (1.26%, 4.16%, 1.23%)\n",
      "- Statistical property comparisons (median comparisons) yielded consistent PASS results\n",
      "- Pre-computed data ensures reproducibility of correlation analysis\n",
      "- No random seeds affect the verification analysis (percentiles are computed deterministically)\n",
      "\n",
      "The original correlation computation uses controlled random seeds for baseline comparisons, and the stored results are stable.\n",
      "\n",
      "---\n",
      "\n",
      "### RP4. Demo Presentation\n",
      "\n",
      "**NA**\n",
      "\n",
      "**Rationale**: This repository is not demo-only. It provides:\n",
      "- Full implementation code (correlations_fast.py, analysis/, etc.)\n",
      "- Pre-computed data for verification\n",
      "- Paper notebooks for generating figures\n",
      "- SLURM scripts for full experiment execution\n",
      "\n",
      "The replication used the pre-computed data path, which is the intended workflow per CodeWalkthrough.md. The repository supports both demo-style verification and full replication.\n",
      "\n",
      "---\n",
      "\n",
      "## Summary\n",
      "\n",
      "The replication was **successful**. All core claims from the plan were verified:\n",
      "\n",
      "1. **Universal Neuron Percentages**: Exact match (1.26%, 4.16%, 1.23%)\n",
      "2. **Statistical Properties**: All 5 properties verified across 3 models (15/15 tests passed)\n",
      "3. **Layer Distribution**: Visualized and consistent with depth specialization claims\n",
      "\n",
      "The repository provides excellent documentation and reproducible data. The pre-computed statistics enable efficient verification without requiring the full compute-intensive correlation analysis.\n",
      "\n",
      "| Criterion | Result |\n",
      "|-----------|--------|\n",
      "| RP1: Implementation Reconstructability | PASS |\n",
      "| RP2: Environment Reproducibility | PASS |\n",
      "| RP3: Determinism and Stability | PASS |\n",
      "| RP4: Demo Presentation | NA |\n",
      "\n",
      "**Overall Assessment**: High-quality replication with exact numerical verification.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's also check evaluation_replication.md for more context\n",
    "with open(os.path.join(replication_dir, 'evaluation_replication.md'), 'r') as f:\n",
    "    eval_replication_doc = f.read()\n",
    "print(\"=== evaluation_replication.md ===\")\n",
    "print(eval_replication_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be10755",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENTATION EVALUATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DE1: RESULT FIDELITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL DOCUMENTATION RESULTS (from plan.md):\n",
      "----------------------------------------------\n",
      "1. Neuron correlation analysis:\n",
      "   - GPT2-medium: 1.23% universal neurons\n",
      "   - Pythia-160M: 1.26% universal neurons  \n",
      "   - GPT2-small: 4.16% universal neurons\n",
      "\n",
      "2. Statistical properties of universal neurons:\n",
      "   - Large weight norm\n",
      "   - Large negative input bias\n",
      "   - High pre-activation skew and kurtosis (monosemantic signature)\n",
      "   - Lower activation frequency compared to non-universal neurons\n",
      "\n",
      "REPLICATED DOCUMENTATION RESULTS (from documentation_replication.md):\n",
      "--------------------------------------------------------------------\n",
      "1. Universal Neuron Percentages:\n",
      "   - pythia-160m: 1.26% (expected 1.26%) ✓\n",
      "   - stanford-gpt2-small-a: 4.16% (expected 4.16%) ✓\n",
      "   - stanford-gpt2-medium-a: 1.23% (expected 1.23%) ✓\n",
      "\n",
      "2. Statistical Properties Verification:\n",
      "   - Lower activation frequency: PASS (all models)\n",
      "   - High pre-activation skew: PASS (all models)\n",
      "   - High pre-activation kurtosis: PASS (all models)\n",
      "   - Large negative input bias: PASS (all models)\n",
      "   - Large weight norm (L2 penalty): PASS (all models)\n",
      "\n",
      "VERDICT: The replicated results exactly match the original results.\n",
      "All numerical values are identical and all statistical property claims are verified.\n",
      "\n",
      "\n",
      "DE1 RESULT: PASS\n",
      "Rationale: All replicated results (universal neuron percentages and statistical\n",
      "property verifications) match the original documentation exactly within stated tolerance.\n"
     ]
    }
   ],
   "source": [
    "# Now let's perform the evaluation\n",
    "# Let's analyze the original vs replicated documentation\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DOCUMENTATION EVALUATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DE1: RESULT FIDELITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ORIGINAL DOCUMENTATION RESULTS (from plan.md):\n",
    "----------------------------------------------\n",
    "1. Neuron correlation analysis:\n",
    "   - GPT2-medium: 1.23% universal neurons\n",
    "   - Pythia-160M: 1.26% universal neurons  \n",
    "   - GPT2-small: 4.16% universal neurons\n",
    "\n",
    "2. Statistical properties of universal neurons:\n",
    "   - Large weight norm\n",
    "   - Large negative input bias\n",
    "   - High pre-activation skew and kurtosis (monosemantic signature)\n",
    "   - Lower activation frequency compared to non-universal neurons\n",
    "\n",
    "REPLICATED DOCUMENTATION RESULTS (from documentation_replication.md):\n",
    "--------------------------------------------------------------------\n",
    "1. Universal Neuron Percentages:\n",
    "   - pythia-160m: 1.26% (expected 1.26%) ✓\n",
    "   - stanford-gpt2-small-a: 4.16% (expected 4.16%) ✓\n",
    "   - stanford-gpt2-medium-a: 1.23% (expected 1.23%) ✓\n",
    "\n",
    "2. Statistical Properties Verification:\n",
    "   - Lower activation frequency: PASS (all models)\n",
    "   - High pre-activation skew: PASS (all models)\n",
    "   - High pre-activation kurtosis: PASS (all models)\n",
    "   - Large negative input bias: PASS (all models)\n",
    "   - Large weight norm (L2 penalty): PASS (all models)\n",
    "\n",
    "VERDICT: The replicated results exactly match the original results.\n",
    "All numerical values are identical and all statistical property claims are verified.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDE1 RESULT: PASS\")\n",
    "print(\"Rationale: All replicated results (universal neuron percentages and statistical\")\n",
    "print(\"property verifications) match the original documentation exactly within stated tolerance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42adf235",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DE2: CONCLUSION CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL DOCUMENTATION CONCLUSIONS (from plan.md):\n",
      "--------------------------------------------------\n",
      "1. HYPOTHESIS 1: Universal neurons (those that consistently activate on the same \n",
      "   inputs across different models) are more likely to be monosemantic and \n",
      "   interpretable than non-universal neurons.\n",
      "\n",
      "2. HYPOTHESIS 2: Neurons with high activation correlation across models will have \n",
      "   clear interpretations and can be taxonomized into a small number of neuron families.\n",
      "\n",
      "3. HYPOTHESIS 3: Universal neurons exhibit specific statistical properties in their \n",
      "   weights and activations that distinguish them from non-universal neurons, including \n",
      "   large negative input bias, high pre-activation skew and kurtosis, and large weight norm.\n",
      "\n",
      "KEY FINDINGS:\n",
      "- Only 1-5% of neurons are universal (excess correlation > 0.5)\n",
      "- Universal neurons show depth specialization\n",
      "- Universal neurons have \"monosemantic signature\" (high skew and kurtosis)\n",
      "\n",
      "REPLICATED DOCUMENTATION CONCLUSIONS (from documentation_replication.md):\n",
      "------------------------------------------------------------------------\n",
      "1. \"The replication exactly matches the paper's reported percentages of universal neurons\"\n",
      "\n",
      "2. \"All statistical property claims are verified: universal neurons have distinctive signatures\"\n",
      "\n",
      "3. \"The 'monosemantic signature' (high skew and kurtosis) is confirmed\"\n",
      "\n",
      "4. \"Layer distribution shows depth specialization patterns\"\n",
      "\n",
      "5. Stated limitations:\n",
      "   - \"Full correlation computation was not re-run (would require significant compute)\"\n",
      "   - \"Detailed neuron family taxonomy was not fully replicated\"\n",
      "\n",
      "VERDICT: The conclusions are consistent with the original documentation.\n",
      "The replication confirms the core hypothesis about universal neurons having\n",
      "distinctive statistical signatures (monosemantic properties). The stated\n",
      "limitations are appropriate acknowledgments of partial replication scope.\n",
      "\n",
      "\n",
      "DE2 RESULT: PASS\n",
      "Rationale: The replicated conclusions are consistent with the original.\n",
      "Key claims (monosemantic signature, statistical properties, universality\n",
      "percentages) are confirmed. Limitations are appropriately acknowledged.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DE2: CONCLUSION CONSISTENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ORIGINAL DOCUMENTATION CONCLUSIONS (from plan.md):\n",
    "--------------------------------------------------\n",
    "1. HYPOTHESIS 1: Universal neurons (those that consistently activate on the same \n",
    "   inputs across different models) are more likely to be monosemantic and \n",
    "   interpretable than non-universal neurons.\n",
    "\n",
    "2. HYPOTHESIS 2: Neurons with high activation correlation across models will have \n",
    "   clear interpretations and can be taxonomized into a small number of neuron families.\n",
    "\n",
    "3. HYPOTHESIS 3: Universal neurons exhibit specific statistical properties in their \n",
    "   weights and activations that distinguish them from non-universal neurons, including \n",
    "   large negative input bias, high pre-activation skew and kurtosis, and large weight norm.\n",
    "\n",
    "KEY FINDINGS:\n",
    "- Only 1-5% of neurons are universal (excess correlation > 0.5)\n",
    "- Universal neurons show depth specialization\n",
    "- Universal neurons have \"monosemantic signature\" (high skew and kurtosis)\n",
    "\n",
    "REPLICATED DOCUMENTATION CONCLUSIONS (from documentation_replication.md):\n",
    "------------------------------------------------------------------------\n",
    "1. \"The replication exactly matches the paper's reported percentages of universal neurons\"\n",
    "\n",
    "2. \"All statistical property claims are verified: universal neurons have distinctive signatures\"\n",
    "\n",
    "3. \"The 'monosemantic signature' (high skew and kurtosis) is confirmed\"\n",
    "\n",
    "4. \"Layer distribution shows depth specialization patterns\"\n",
    "\n",
    "5. Stated limitations:\n",
    "   - \"Full correlation computation was not re-run (would require significant compute)\"\n",
    "   - \"Detailed neuron family taxonomy was not fully replicated\"\n",
    "\n",
    "VERDICT: The conclusions are consistent with the original documentation.\n",
    "The replication confirms the core hypothesis about universal neurons having\n",
    "distinctive statistical signatures (monosemantic properties). The stated\n",
    "limitations are appropriate acknowledgments of partial replication scope.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDE2 RESULT: PASS\")\n",
    "print(\"Rationale: The replicated conclusions are consistent with the original.\")\n",
    "print(\"Key claims (monosemantic signature, statistical properties, universality\") \n",
    "print(\"percentages) are confirmed. Limitations are appropriately acknowledged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71a013f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DE3: NO EXTERNAL OR HALLUCINATED INFORMATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ANALYSIS OF REPLICATED DOCUMENTATION FOR EXTERNAL/HALLUCINATED CONTENT:\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "1. DATA SOURCES CITED:\n",
      "   - All data comes from repository dataframes/neuron_dfs/ (original source)\n",
      "   - File names match original repo: pythia-160m.csv, stanford-gpt2-small-a.csv, \n",
      "     stanford-gpt2-medium-a.csv\n",
      "\n",
      "2. METHODOLOGY REFERENCES:\n",
      "   - Excess correlation formula matches original: excess_corr = mean_corr - mean_baseline\n",
      "   - Threshold (0.5) matches original documentation\n",
      "   - Statistical properties (skew, kurtosis, bias, weight norm) all from original\n",
      "\n",
      "3. RESULTS CITED:\n",
      "   - All percentages (1.26%, 4.16%, 1.23%) match original plan.md exactly\n",
      "   - No new metrics or measurements introduced\n",
      "   - Statistical property claims directly map to original hypotheses\n",
      "\n",
      "4. EXTERNAL REFERENCES CHECK:\n",
      "   - Paper citation: \"Universal Neurons in GPT2 Language Models\" by Gurnee et al. (2024) \n",
      "     ✓ Matches original citation in CodeWalkthrough.md\n",
      "   - No additional papers or external sources referenced\n",
      "   - No invented findings or metrics\n",
      "\n",
      "5. FIGURES MENTIONED:\n",
      "   - universal_neurons_properties.png: Visualizes data from original CSV files\n",
      "   - layer_distribution.png: Visualizes layer distribution from original data\n",
      "   - Both are legitimate visualizations of original data\n",
      "\n",
      "VERDICT: No external references, invented findings, or hallucinated details found.\n",
      "All information in the replicated documentation traces back to the original repo.\n",
      "\n",
      "\n",
      "DE3 RESULT: PASS\n",
      "Rationale: All content in the replicated documentation derives from the\n",
      "original repository data, documentation, and methodology. No external\n",
      "references or invented information was introduced.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DE3: NO EXTERNAL OR HALLUCINATED INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ANALYSIS OF REPLICATED DOCUMENTATION FOR EXTERNAL/HALLUCINATED CONTENT:\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "1. DATA SOURCES CITED:\n",
    "   - All data comes from repository dataframes/neuron_dfs/ (original source)\n",
    "   - File names match original repo: pythia-160m.csv, stanford-gpt2-small-a.csv, \n",
    "     stanford-gpt2-medium-a.csv\n",
    "\n",
    "2. METHODOLOGY REFERENCES:\n",
    "   - Excess correlation formula matches original: excess_corr = mean_corr - mean_baseline\n",
    "   - Threshold (0.5) matches original documentation\n",
    "   - Statistical properties (skew, kurtosis, bias, weight norm) all from original\n",
    "\n",
    "3. RESULTS CITED:\n",
    "   - All percentages (1.26%, 4.16%, 1.23%) match original plan.md exactly\n",
    "   - No new metrics or measurements introduced\n",
    "   - Statistical property claims directly map to original hypotheses\n",
    "\n",
    "4. EXTERNAL REFERENCES CHECK:\n",
    "   - Paper citation: \"Universal Neurons in GPT2 Language Models\" by Gurnee et al. (2024) \n",
    "     ✓ Matches original citation in CodeWalkthrough.md\n",
    "   - No additional papers or external sources referenced\n",
    "   - No invented findings or metrics\n",
    "\n",
    "5. FIGURES MENTIONED:\n",
    "   - universal_neurons_properties.png: Visualizes data from original CSV files\n",
    "   - layer_distribution.png: Visualizes layer distribution from original data\n",
    "   - Both are legitimate visualizations of original data\n",
    "\n",
    "VERDICT: No external references, invented findings, or hallucinated details found.\n",
    "All information in the replicated documentation traces back to the original repo.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDE3 RESULT: PASS\")\n",
    "print(\"Rationale: All content in the replicated documentation derives from the\")\n",
    "print(\"original repository data, documentation, and methodology. No external\")\n",
    "print(\"references or invented information was introduced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ff0c87",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: /net/scratch2/smallyan/universal-neurons_eval/evaluation/replication_eval\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation output directory\n",
    "eval_output_dir = '/net/scratch2/smallyan/universal-neurons_eval/evaluation/replication_eval'\n",
    "os.makedirs(eval_output_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {eval_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557d90cb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: /net/scratch2/smallyan/universal-neurons_eval/evaluation/replication_eval/documentation_evaluation_summary.md\n",
      "\n",
      "================================================================================\n",
      "CONTENT OF documentation_evaluation_summary.md:\n",
      "================================================================================\n",
      "# Documentation Evaluation Summary\n",
      "\n",
      "## Results Comparison\n",
      "\n",
      "The replicated documentation faithfully reproduces the results from the original experiment documentation. The key quantitative results match exactly:\n",
      "\n",
      "- **Universal neuron percentages** are identical across all three models:\n",
      "  - Pythia-160M: 1.26% (original) vs 1.26% (replicated)\n",
      "  - GPT2-small: 4.16% (original) vs 4.16% (replicated)\n",
      "  - GPT2-medium: 1.23% (original) vs 1.23% (replicated)\n",
      "\n",
      "- **Statistical property verifications** all pass: Universal neurons exhibit the documented \"monosemantic signature\" with lower activation frequency, high pre-activation skew and kurtosis, large negative input bias, and large weight norm (L2 penalty).\n",
      "\n",
      "## Conclusions Comparison\n",
      "\n",
      "The replicated documentation presents conclusions that are fully consistent with the original documentation:\n",
      "\n",
      "1. Both documents confirm that only 1-5% of neurons are universal (excess correlation > 0.5)\n",
      "2. Both confirm the distinctive statistical signatures of universal neurons\n",
      "3. Both acknowledge depth specialization patterns in layer distribution\n",
      "4. The replication appropriately notes limitations (no full correlation recomputation, partial taxonomy analysis)\n",
      "\n",
      "The replicated conclusions do not contradict or omit any essential claims from the original.\n",
      "\n",
      "## External or Hallucinated Information\n",
      "\n",
      "**None detected.** All information in the replicated documentation traces directly to:\n",
      "- Pre-computed data from `dataframes/neuron_dfs/` in the original repository\n",
      "- Methodology and results documented in `plan.md` and `CodeWalkthrough.md`\n",
      "- The original paper citation (Gurnee et al., 2024)\n",
      "\n",
      "No external references, invented findings, or hallucinated details were introduced.\n",
      "\n",
      "## Evaluation Summary Table\n",
      "\n",
      "| Criterion | Result |\n",
      "|-----------|--------|\n",
      "| DE1: Result Fidelity | **PASS** |\n",
      "| DE2: Conclusion Consistency | **PASS** |\n",
      "| DE3: No External/Hallucinated Information | **PASS** |\n",
      "\n",
      "## Final Verdict\n",
      "\n",
      "**PASS**\n",
      "\n",
      "The replicated documentation faithfully reproduces the results and conclusions of the original experiment. All numerical results match exactly, conclusions are consistent, and no external or hallucinated information was introduced.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write documentation_evaluation_summary.md\n",
    "summary_md = \"\"\"# Documentation Evaluation Summary\n",
    "\n",
    "## Results Comparison\n",
    "\n",
    "The replicated documentation faithfully reproduces the results from the original experiment documentation. The key quantitative results match exactly:\n",
    "\n",
    "- **Universal neuron percentages** are identical across all three models:\n",
    "  - Pythia-160M: 1.26% (original) vs 1.26% (replicated)\n",
    "  - GPT2-small: 4.16% (original) vs 4.16% (replicated)\n",
    "  - GPT2-medium: 1.23% (original) vs 1.23% (replicated)\n",
    "\n",
    "- **Statistical property verifications** all pass: Universal neurons exhibit the documented \"monosemantic signature\" with lower activation frequency, high pre-activation skew and kurtosis, large negative input bias, and large weight norm (L2 penalty).\n",
    "\n",
    "## Conclusions Comparison\n",
    "\n",
    "The replicated documentation presents conclusions that are fully consistent with the original documentation:\n",
    "\n",
    "1. Both documents confirm that only 1-5% of neurons are universal (excess correlation > 0.5)\n",
    "2. Both confirm the distinctive statistical signatures of universal neurons\n",
    "3. Both acknowledge depth specialization patterns in layer distribution\n",
    "4. The replication appropriately notes limitations (no full correlation recomputation, partial taxonomy analysis)\n",
    "\n",
    "The replicated conclusions do not contradict or omit any essential claims from the original.\n",
    "\n",
    "## External or Hallucinated Information\n",
    "\n",
    "**None detected.** All information in the replicated documentation traces directly to:\n",
    "- Pre-computed data from `dataframes/neuron_dfs/` in the original repository\n",
    "- Methodology and results documented in `plan.md` and `CodeWalkthrough.md`\n",
    "- The original paper citation (Gurnee et al., 2024)\n",
    "\n",
    "No external references, invented findings, or hallucinated details were introduced.\n",
    "\n",
    "## Evaluation Summary Table\n",
    "\n",
    "| Criterion | Result |\n",
    "|-----------|--------|\n",
    "| DE1: Result Fidelity | **PASS** |\n",
    "| DE2: Conclusion Consistency | **PASS** |\n",
    "| DE3: No External/Hallucinated Information | **PASS** |\n",
    "\n",
    "## Final Verdict\n",
    "\n",
    "**PASS**\n",
    "\n",
    "The replicated documentation faithfully reproduces the results and conclusions of the original experiment. All numerical results match exactly, conclusions are consistent, and no external or hallucinated information was introduced.\n",
    "\"\"\"\n",
    "\n",
    "# Write the summary file\n",
    "summary_path = os.path.join(eval_output_dir, 'documentation_evaluation_summary.md')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary_md)\n",
    "    \n",
    "print(f\"Written: {summary_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTENT OF documentation_evaluation_summary.md:\")\n",
    "print(\"=\"*80)\n",
    "print(summary_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26aa0a1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: /net/scratch2/smallyan/universal-neurons_eval/evaluation/replication_eval/documentation_eval_summary.json\n",
      "\n",
      "================================================================================\n",
      "CONTENT OF documentation_eval_summary.json:\n",
      "================================================================================\n",
      "{\n",
      "  \"Checklist\": {\n",
      "    \"DE1_ResultFidelity\": \"PASS\",\n",
      "    \"DE2_ConclusionConsistency\": \"PASS\",\n",
      "    \"DE3_NoExternalInformation\": \"PASS\"\n",
      "  },\n",
      "  \"Rationale\": {\n",
      "    \"DE1_ResultFidelity\": \"All replicated results match the original documentation exactly. Universal neuron percentages are identical (Pythia-160M: 1.26%, GPT2-small: 4.16%, GPT2-medium: 1.23%). All five statistical property verifications (lower activation frequency, high skew, high kurtosis, negative input bias, large weight norm) passed across all three models.\",\n",
      "    \"DE2_ConclusionConsistency\": \"The replicated conclusions are consistent with the original. Both confirm 1-5% universal neurons, distinctive statistical signatures (monosemantic properties), and depth specialization patterns. The replication appropriately acknowledges scope limitations without contradicting original claims.\",\n",
      "    \"DE3_NoExternalInformation\": \"All information in the replicated documentation derives from the original repository (dataframes/neuron_dfs/, plan.md, CodeWalkthrough.md). No external references, invented findings, or hallucinated details were introduced. The paper citation matches the original.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create the JSON summary\n",
    "summary_json = {\n",
    "    \"Checklist\": {\n",
    "        \"DE1_ResultFidelity\": \"PASS\",\n",
    "        \"DE2_ConclusionConsistency\": \"PASS\",\n",
    "        \"DE3_NoExternalInformation\": \"PASS\"\n",
    "    },\n",
    "    \"Rationale\": {\n",
    "        \"DE1_ResultFidelity\": \"All replicated results match the original documentation exactly. Universal neuron percentages are identical (Pythia-160M: 1.26%, GPT2-small: 4.16%, GPT2-medium: 1.23%). All five statistical property verifications (lower activation frequency, high skew, high kurtosis, negative input bias, large weight norm) passed across all three models.\",\n",
    "        \"DE2_ConclusionConsistency\": \"The replicated conclusions are consistent with the original. Both confirm 1-5% universal neurons, distinctive statistical signatures (monosemantic properties), and depth specialization patterns. The replication appropriately acknowledges scope limitations without contradicting original claims.\",\n",
    "        \"DE3_NoExternalInformation\": \"All information in the replicated documentation derives from the original repository (dataframes/neuron_dfs/, plan.md, CodeWalkthrough.md). No external references, invented findings, or hallucinated details were introduced. The paper citation matches the original.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write the JSON file\n",
    "json_path = os.path.join(eval_output_dir, 'documentation_eval_summary.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(summary_json, f, indent=2)\n",
    "\n",
    "print(f\"Written: {json_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTENT OF documentation_eval_summary.json:\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(summary_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b3cfe6b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICATION OF OUTPUT FILES\n",
      "================================================================================\n",
      "✓ documentation_evaluation_summary.md (2183 bytes)\n",
      "✓ documentation_eval_summary.json (1161 bytes)\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Output directory: /net/scratch2/smallyan/universal-neurons_eval/evaluation/replication_eval\n",
      "\n",
      "Final Verdict: PASS\n",
      "All criteria (DE1, DE2, DE3) passed. The replicated documentation\n",
      "faithfully reproduces the original experiment results and conclusions.\n"
     ]
    }
   ],
   "source": [
    "# Verify the output files exist\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION OF OUTPUT FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for filename in ['documentation_evaluation_summary.md', 'documentation_eval_summary.json']:\n",
    "    filepath = os.path.join(eval_output_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"✓ {filename} ({size} bytes)\")\n",
    "    else:\n",
    "        print(f\"✗ {filename} NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOutput directory: {eval_output_dir}\")\n",
    "print(\"\\nFinal Verdict: PASS\")\n",
    "print(\"All criteria (DE1, DE2, DE3) passed. The replicated documentation\")\n",
    "print(\"faithfully reproduces the original experiment results and conclusions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191bbc7",
   "metadata": {},
   "source": [
    "# Documentation Evaluation Complete\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook performed a **Replicator-Documentation Evaluation** to determine whether the replicator's documentation faithfully reproduces the results and conclusions of the original Universal Neurons experiment.\n",
    "\n",
    "### Files Evaluated\n",
    "- **Original**: `plan.md`, `CodeWalkthrough.md` from `/net/scratch2/smallyan/universal-neurons_eval`\n",
    "- **Replicated**: `documentation_replication.md` from `/net/scratch2/smallyan/universal-neurons_eval/evaluation/replications`\n",
    "\n",
    "### Evaluation Results\n",
    "\n",
    "| Criterion | Result |\n",
    "|-----------|--------|\n",
    "| DE1: Result Fidelity | **PASS** |\n",
    "| DE2: Conclusion Consistency | **PASS** |\n",
    "| DE3: No External/Hallucinated Information | **PASS** |\n",
    "\n",
    "### Final Verdict: **PASS**\n",
    "\n",
    "### Output Files\n",
    "- `evaluation/replication_eval/documentation_evaluation_summary.md`\n",
    "- `evaluation/replication_eval/documentation_eval_summary.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-09-13-06_CircuitAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
