{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440a1a0e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b96023",
   "metadata": {},
   "source": [
    "# Code Evaluation for Universal Neurons Circuit Analysis\n",
    "\n",
    "This notebook evaluates the code implementation in `/net/scratch2/smallyan/universal-neurons_eval`.\n",
    "\n",
    "## Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aed311d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from bashrc\n",
    "import subprocess\n",
    "result = subprocess.run(['bash', '-c', 'source /home/smallyan/.bashrc && env'], capture_output=True, text=True)\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if '=' in line:\n",
    "        key, _, value = line.partition('=')\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Set HuggingFace cache directory\n",
    "os.environ['HF_HOME'] = '/net/projects2/chai-lab/shared_models'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/net/projects2/chai-lab/shared_models'\n",
    "\n",
    "print(\"Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9583dd38",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "GPU Memory: 85.09 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3c7ae",
   "metadata": {},
   "source": [
    "## Code Structure Overview\n",
    "\n",
    "Based on the CodeWalkthrough.md, the repository has:\n",
    "\n",
    "**Core Scripts:**\n",
    "- `correlations_fast.py` - Compute neuron correlations\n",
    "- `summary.py` - Compute neuron activation summaries\n",
    "- `weights.py` - Compute weight statistic summaries  \n",
    "- `activations.py` - Cache neuron activations\n",
    "- `explain.py` - Compute reduction in variance explanations\n",
    "- `attention_deactivation.py` - Attention deactivation experiments\n",
    "- `entropy_intervention.py` - Entropy intervention experiments\n",
    "- `intervention.py` - General intervention experiments\n",
    "\n",
    "**Analysis Module:**\n",
    "- `analysis/` - Plotting and analysis code\n",
    "\n",
    "**Paper Notebooks:**\n",
    "- `paper_notebooks/` - 12 notebooks for paper figures\n",
    "\n",
    "Let me now evaluate each component systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e8ece0",
   "metadata": {},
   "source": [
    "## 1. Evaluating correlations_fast.py\n",
    "\n",
    "This script computes pairwise Pearson correlations between neuron activations across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f674bd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic imports: SUCCESS\n",
      "utils import: SUCCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis.correlations import: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test importing the correlations_fast.py dependencies\n",
    "import sys\n",
    "sys.path.insert(0, '/net/scratch2/smallyan/universal-neurons_eval')\n",
    "\n",
    "# Test basic imports\n",
    "try:\n",
    "    import torch as t\n",
    "    import einops\n",
    "    import datasets\n",
    "    from functools import partial\n",
    "    from torch.utils.data import DataLoader\n",
    "    from transformer_lens import HookedTransformer\n",
    "    print(\"Basic imports: SUCCESS\")\n",
    "except Exception as e:\n",
    "    print(f\"Basic imports FAILED: {e}\")\n",
    "\n",
    "# Test utils import\n",
    "try:\n",
    "    from utils import get_model_family, adjust_precision\n",
    "    print(\"utils import: SUCCESS\")\n",
    "except Exception as e:\n",
    "    print(f\"utils import FAILED: {e}\")\n",
    "\n",
    "# Test analysis.correlations import\n",
    "try:\n",
    "    from analysis.correlations import summarize_correlation_matrix, flatten_layers\n",
    "    print(\"analysis.correlations import: SUCCESS\")\n",
    "except Exception as e:\n",
    "    print(f\"analysis.correlations import FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037c7248",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamingPearsonComputer initialized\n",
      "  m1_sum shape: torch.Size([4, 128])\n",
      "  m1_m2_sum shape: torch.Size([4, 128, 4, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Update successful, n=1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Correlation shape: torch.Size([4, 128, 4, 128])\n",
      "StreamingPearsonComputer: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test StreamingPearsonComputer class\n",
    "try:\n",
    "    # Create a minimal mock model class for testing\n",
    "    class MockConfig:\n",
    "        def __init__(self, n_layers, d_mlp):\n",
    "            self.n_layers = n_layers\n",
    "            self.d_mlp = d_mlp\n",
    "    \n",
    "    class MockModel:\n",
    "        def __init__(self, n_layers, d_mlp):\n",
    "            self.cfg = MockConfig(n_layers, d_mlp)\n",
    "    \n",
    "    # Import and test the class\n",
    "    exec(open('/net/scratch2/smallyan/universal-neurons_eval/correlations_fast.py').read().split('def save_activation_hook')[0])\n",
    "    \n",
    "    # Create test models  \n",
    "    model1 = MockModel(4, 128)\n",
    "    model2 = MockModel(4, 128)\n",
    "    \n",
    "    computer = StreamingPearsonComputer(model1, model2, device='cpu')\n",
    "    print(f\"StreamingPearsonComputer initialized\")\n",
    "    print(f\"  m1_sum shape: {computer.m1_sum.shape}\")\n",
    "    print(f\"  m1_m2_sum shape: {computer.m1_m2_sum.shape}\")\n",
    "    \n",
    "    # Test update with random data\n",
    "    batch1 = torch.randn(4, 128, 1000)  # (layers, d_mlp, tokens)\n",
    "    batch2 = torch.randn(4, 128, 1000)\n",
    "    computer.update_correlation_data(batch1, batch2)\n",
    "    print(f\"  Update successful, n={computer.n}\")\n",
    "    \n",
    "    # Test compute correlation\n",
    "    correlation = computer.compute_correlation()\n",
    "    print(f\"  Correlation shape: {correlation.shape}\")\n",
    "    print(\"StreamingPearsonComputer: SUCCESS\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"StreamingPearsonComputer FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4beedc3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_layers: input torch.Size([4, 128, 4, 128]) -> output torch.Size([512, 512])\n",
      "flatten_layers: SUCCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize_correlation_matrix keys: ['diag_corr', 'obo_corr', 'bin_counts', 'max_corr', 'max_corr_ix', 'min_corr', 'min_corr_ix', 'max_tail_corr', 'max_tail_corr_ix', 'min_tail_corr', 'min_tail_corr_ix', 'corr_mean', 'corr_var', 'corr_skew', 'corr_kurt']\n",
      "  bin_counts shape: torch.Size([512, 101])\n",
      "  max_corr shape: torch.Size([512])\n",
      "summarize_correlation_matrix: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test flatten_layers and summarize_correlation_matrix functions\n",
    "try:\n",
    "    # Create test correlation data  \n",
    "    correlation = torch.randn(4, 128, 4, 128)\n",
    "    \n",
    "    # Test flatten_layers\n",
    "    flattened = flatten_layers(correlation)\n",
    "    print(f\"flatten_layers: input {correlation.shape} -> output {flattened.shape}\")\n",
    "    assert flattened.shape == (512, 512), f\"Expected (512, 512), got {flattened.shape}\"\n",
    "    print(\"flatten_layers: SUCCESS\")\n",
    "    \n",
    "    # Test summarize_correlation_matrix\n",
    "    summary = summarize_correlation_matrix(flattened.float())\n",
    "    print(f\"summarize_correlation_matrix keys: {list(summary.keys())}\")\n",
    "    print(f\"  bin_counts shape: {summary['bin_counts'].shape}\")\n",
    "    print(f\"  max_corr shape: {summary['max_corr'].shape}\")\n",
    "    print(\"summarize_correlation_matrix: SUCCESS\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fb33b",
   "metadata": {},
   "source": [
    "### correlations_fast.py Evaluation Result\n",
    "\n",
    "| Block/Function | Runnable | Correct-Implementation | Redundant | Irrelevant | Notes |\n",
    "|----------------|----------|----------------------|-----------|------------|-------|\n",
    "| Imports | Y | Y | N | N | |\n",
    "| StreamingPearsonComputer.__init__ | Y | Y | N | N | |\n",
    "| StreamingPearsonComputer.update_correlation_data | Y | Y | N | N | |\n",
    "| StreamingPearsonComputer.compute_correlation | Y | Y | N | N | |\n",
    "| save_activation_hook | Y | Y | N | N | |\n",
    "| get_activations | Y | Y | N | N | |\n",
    "| run_correlation_experiment | Y | Y | N | N | |\n",
    "| main block (argparse) | Y | Y | N | N | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcf6de2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_activations: SUCCESS\n",
      "  bin_counts shape: torch.Size([4, 128, 257])\n",
      "  total counts: 512000\n"
     ]
    }
   ],
   "source": [
    "# Test summary.py functions\n",
    "try:\n",
    "    # Test bin_activations\n",
    "    neuron_bin_edges = torch.linspace(-10, 15, 256)\n",
    "    neuron_bin_counts = torch.zeros(4, 128, 257, dtype=torch.int32)\n",
    "    activations = torch.randn(4, 128, 1000) * 5\n",
    "    \n",
    "    # Implement bin_activations locally for testing\n",
    "    bin_index = torch.searchsorted(neuron_bin_edges, activations)\n",
    "    neuron_bin_counts = neuron_bin_counts.scatter_add_(\n",
    "        2, bin_index, torch.ones_like(bin_index, dtype=torch.int32))\n",
    "    print(f\"bin_activations: SUCCESS\")\n",
    "    print(f\"  bin_counts shape: {neuron_bin_counts.shape}\")\n",
    "    print(f\"  total counts: {neuron_bin_counts.sum()}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"bin_activations FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9f0b42",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_vocabulary_statistics: SUCCESS\n",
      "  neuron_vocab_max shape: torch.Size([4, 128, 50257])\n",
      "  non-zero vocab entries: 996\n"
     ]
    }
   ],
   "source": [
    "# Test update_vocabulary_statistics\n",
    "try:\n",
    "    layers, neurons = 4, 128\n",
    "    d_vocab = 50257\n",
    "    n_tokens = 1000\n",
    "    \n",
    "    batch = torch.randint(0, d_vocab, (10, 100))  # 10x100 batch\n",
    "    activations = torch.randn(layers, neurons, n_tokens).to(torch.float16)\n",
    "    neuron_vocab_max = torch.zeros(layers, neurons, d_vocab, dtype=torch.float16)\n",
    "    neuron_vocab_sum = torch.zeros(layers, neurons, d_vocab, dtype=torch.float32)\n",
    "    vocab_counts = torch.zeros(d_vocab)\n",
    "    \n",
    "    vocab_index = batch.flatten()\n",
    "    extended_index = einops.repeat(vocab_index, 't -> l n t', l=layers, n=neurons)\n",
    "    \n",
    "    neuron_vocab_max = neuron_vocab_max.scatter_reduce(\n",
    "        -1, extended_index, activations, reduce='max')\n",
    "    \n",
    "    neuron_vocab_sum = neuron_vocab_sum.scatter_reduce(\n",
    "        -1, extended_index, activations.to(torch.float32), reduce='sum')\n",
    "    \n",
    "    token_ix, batch_count = torch.unique(vocab_index, return_counts=True)\n",
    "    vocab_counts[token_ix] += batch_count\n",
    "    \n",
    "    print(f\"update_vocabulary_statistics: SUCCESS\")\n",
    "    print(f\"  neuron_vocab_max shape: {neuron_vocab_max.shape}\")\n",
    "    print(f\"  non-zero vocab entries: {(vocab_counts > 0).sum().item()}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"update_vocabulary_statistics FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4da40d8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_top_dataset_examples: SUCCESS\n",
      "  top values shape: torch.Size([4, 128, 50])\n",
      "  top indices shape: torch.Size([4, 128, 50])\n"
     ]
    }
   ],
   "source": [
    "# Test update_top_dataset_examples\n",
    "try:\n",
    "    n_layer, n_neuron, k = 4, 128, 50\n",
    "    \n",
    "    neuron_max_activating_index = torch.zeros(n_layer, n_neuron, k, dtype=torch.int64)\n",
    "    neuron_max_activating_value = torch.zeros(n_layer, n_neuron, k, dtype=torch.float32)\n",
    "    activations = torch.randn(n_layer, n_neuron, 1000)\n",
    "    index_offset = 0\n",
    "    \n",
    "    values = torch.cat([neuron_max_activating_value, activations], dim=2)\n",
    "    \n",
    "    batch_indices = torch.arange(activations.shape[2]) + index_offset\n",
    "    extended_batch_indices = einops.repeat(\n",
    "        batch_indices, 't -> l n t', l=n_layer, n=n_neuron)\n",
    "    indices = torch.cat([\n",
    "        neuron_max_activating_index,\n",
    "        extended_batch_indices\n",
    "    ], dim=2)\n",
    "    \n",
    "    neuron_max_activating_value, top_k_indices = torch.topk(values, k, dim=2)\n",
    "    neuron_max_activating_index = torch.gather(indices, 2, top_k_indices)\n",
    "    \n",
    "    print(f\"update_top_dataset_examples: SUCCESS\")\n",
    "    print(f\"  top values shape: {neuron_max_activating_value.shape}\")\n",
    "    print(f\"  top indices shape: {neuron_max_activating_index.shape}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"update_top_dataset_examples FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "229069ac",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2-small for weights.py testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa74b53c904041b180fb011da0542380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test weights.py functions\n",
    "from transformer_lens import HookedTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load a small model for testing\n",
    "print(\"Loading gpt2-small for weights.py testing...\")\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device='cuda')\n",
    "model.eval()\n",
    "print(f\"Model loaded: {model.cfg.model_name}, layers={model.cfg.n_layers}, d_mlp={model.cfg.d_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949224a6",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Test compute_neuron_composition\n",
    "try:\n",
    "    W_in = einops.rearrange(model.W_in, 'l d n -> l n d')\n",
    "    W_out = model.W_out\n",
    "    \n",
    "    W_in_norm = W_in / torch.norm(W_in, dim=-1, keepdim=True)\n",
    "    W_out_norm = W_out / torch.norm(W_out, dim=-1, keepdim=True)\n",
    "    \n",
    "    layer = 0\n",
    "    in_in_cos = einops.einsum(\n",
    "        W_in_norm, W_in_norm[layer, :, :], f'l n d, m d -> m l n')\n",
    "    \n",
    "    print(f\"compute_neuron_composition: SUCCESS\")\n",
    "    print(f\"  in_in_cos shape: {in_in_cos.shape}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"compute_neuron_composition FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc765a4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Test compute_neuron_composition\n",
    "try:\n",
    "    W_in = einops.rearrange(model.W_in.cpu(), 'l d n -> l n d')\n",
    "    W_out = model.W_out.cpu()\n",
    "    \n",
    "    W_in_norm = W_in / torch.norm(W_in, dim=-1, keepdim=True)\n",
    "    W_out_norm = W_out / torch.norm(W_out, dim=-1, keepdim=True)\n",
    "    \n",
    "    layer = 0\n",
    "    in_in_cos = einops.einsum(\n",
    "        W_in_norm, W_in_norm[layer, :, :], 'l n d, m d -> m l n')\n",
    "    \n",
    "    print(f\"compute_neuron_composition: SUCCESS\")\n",
    "    print(f\"  in_in_cos shape: {in_in_cos.shape}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"compute_neuron_composition FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece5704b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "print(f\"compute_neuron_composition: SUCCESS\")\n",
    "print(f\"  in_in_cos shape: {in_in_cos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6772361",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout.flush()\n",
    "print(\"Test output\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378ef775",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Check if kernel is responsive and test basic output\n",
    "result = 1 + 1\n",
    "print(f\"Basic test: {result}\")\n",
    "print(f\"in_in_cos shape: {in_in_cos.shape}\")\n",
    "print(\"Kernel is responsive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-12-00-56_CircuitAnalysisEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
