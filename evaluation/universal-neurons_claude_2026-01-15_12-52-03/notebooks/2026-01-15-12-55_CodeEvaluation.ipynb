{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe32d75",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956dc641",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H100 NVL\n",
      "Memory: 93.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Set environment for using cached models\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/net/projects2/chai-lab/shared_models'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/net/projects2/chai-lab/shared_models'\n",
    "\n",
    "# Check for CUDA availability\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0616e41",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cells: 50\n",
      "Code cells: 42\n",
      "Markdown cells: 8\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the notebook to identify all code cells\n",
    "import json\n",
    "\n",
    "notebook_path = '/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13/notebooks/2026-01-05-00-38_typo_circuit_analysis.ipynb'\n",
    "\n",
    "with open(notebook_path, 'r') as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "# Count cells by type\n",
    "code_cells = []\n",
    "markdown_cells = []\n",
    "\n",
    "for i, cell in enumerate(notebook['cells']):\n",
    "    cell_type = cell['cell_type']\n",
    "    source = ''.join(cell['source'])\n",
    "    \n",
    "    if cell_type == 'code':\n",
    "        code_cells.append({\n",
    "            'index': i,\n",
    "            'source': source,\n",
    "            'cell_num': len(code_cells)\n",
    "        })\n",
    "    elif cell_type == 'markdown':\n",
    "        markdown_cells.append({\n",
    "            'index': i,\n",
    "            'source': source\n",
    "        })\n",
    "\n",
    "print(f\"Total cells: {len(notebook['cells'])}\")\n",
    "print(f\"Code cells: {len(code_cells)}\")\n",
    "print(f\"Markdown cells: {len(markdown_cells)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7501ef63",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Code Cell 0 (notebook index 0) ===\n",
      "import os\n",
      "os.chdir('/home/smallyan/eval_agent')\n",
      "print(f\"Working directory: {os.getcwd()}\")\n",
      "--- Length: 90 chars, 3 lines ---\n",
      "\n",
      "=== Code Cell 1 (notebook index 1) ===\n",
      "# Check GPU availability and set up environment\n",
      "import torch\n",
      "print(f\"PyTorch version: {torch.__version__}\")\n",
      "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
      "if torch.cuda.is_available():\n",
      "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "    print(f\"CUDA memory: {torch.cuda.get_device_p...\n",
      "--- Length: 419 chars, 11 lines ---\n",
      "\n",
      "=== Code Cell 2 (notebook index 2) ===\n",
      "# Create necessary directories\n",
      "import os\n",
      "\n",
      "os.makedirs('logs', exist_ok=True)\n",
      "os.makedirs('notebooks', exist_ok=True)\n",
      "--- Length: 249 chars, 9 lines ---\n",
      "\n",
      "=== Code Cell 3 (notebook index 4) ===\n",
      "# Load GPT-2 Medium via HookedTransformer\n",
      "from transformer_lens import HookedTransformer\n",
      "import torch\n",
      "\n",
      "print(\"Loading GPT-2 Medium...\")\n",
      "model = HookedTransformer.from_pretrained(\"gpt2-medium\", device=device)\n",
      "print(f\"Model loaded: {model.cfg.model_name}\")\n",
      "print(f\"Number of layers: {model.cfg.n_layers...\n",
      "--- Length: 455 chars, 11 lines ---\n",
      "\n",
      "=== Code Cell 4 (notebook index 5) ===\n",
      "# Test that the model works with a simple completion\n",
      "test_prompt = \"The capital of France is\"\n",
      "tokens = model.to_tokens(test_prompt)\n",
      "logits = model(tokens)\n",
      "next_token = logits[0, -1].argmax()\n",
      "print(f\"Prompt: '{test_prompt}'\")\n",
      "print(f\"Next token prediction: '{model.to_string(next_token)}'\")\n",
      "\n",
      "# Test wi...\n",
      "--- Length: 583 chars, 15 lines ---\n",
      "\n",
      "=== Code Cell 5 (notebook index 6) ===\n",
      "# Let's check top-k predictions for both cases\n",
      "import torch.nn.functional as F\n",
      "\n",
      "def get_top_predictions(logits, k=10):\n",
      "    \"\"\"Get top-k predicted tokens and their probabilities\"\"\"\n",
      "    probs = F.softmax(logits[0, -1], dim=-1)\n",
      "    top_probs, top_indices = torch.topk(probs, k)\n",
      "    return [(model.to_str...\n",
      "--- Length: 1093 chars, 26 lines ---\n",
      "\n",
      "=== Code Cell 6 (notebook index 7) ===\n",
      "# Let's try different types of typos to understand the model's behavior better\n",
      "# and find cases where it shows more robustness\n",
      "\n",
      "test_cases = [\n",
      "    # (clean, typoed, expected_completion)\n",
      "    (\"The cat sat on the\", \"The cat sat on teh\", \" mat\"),  # simple transposition\n",
      "    (\"I want to eat an apple\", \"...\n",
      "--- Length: 1663 chars, 34 lines ---\n",
      "\n",
      "=== Code Cell 7 (notebook index 9) ===\n",
      "# Let's examine the tokenization of clean vs typoed words to understand the issue\n",
      "import random\n",
      "\n",
      "def show_tokenization(text):\n",
      "    \"\"\"Show how text is tokenized\"\"\"\n",
      "    tokens = model.to_tokens(text, prepend_bos=False)\n",
      "    token_strs = [model.to_string(t) for t in tokens[0]]\n",
      "    return token_strs\n",
      "\n",
      "# C...\n",
      "--- Length: 839 chars, 28 lines ---\n",
      "\n",
      "=== Code Cell 8 (notebook index 10) ===\n",
      "# Create a systematic typo dataset\n",
      "# We'll categorize by typo type and tokenization impact\n",
      "\n",
      "import random\n",
      "from typing import List, Dict, Tuple\n",
      "\n",
      "def create_typo_substitution(word: str, idx: int = None) -> str:\n",
      "    \"\"\"Create a typo by substituting a character\"\"\"\n",
      "    if len(word) < 2:\n",
      "        return wo...\n",
      "--- Length: 3808 chars, 101 lines ---\n",
      "\n",
      "=== Code Cell 9 (notebook index 11) ===\n",
      "# Evaluate model performance on clean vs typo sentences\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "results = []\n",
      "\n",
      "for entry in dataset:\n",
      "    clean_tokens = model.to_tokens(entry[\"clean\"])\n",
      "    typo_tokens = model.to_tokens(entry[\"typo\"])\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        clean_logits = mo...\n",
      "--- Length: 2191 chars, 59 lines ---\n",
      "\n",
      "=== Code Cell 10 (notebook index 12) ===\n",
      "# Let's look at specific cases: best and worst typo handling\n",
      "print(\"Cases with BEST typo handling (highest prob ratio):\\n\")\n",
      "sorted_results = sorted(results, key=lambda x: x[\"prob_ratio\"], reverse=True)\n",
      "for r in sorted_results[:5]:\n",
      "    print(f\"Ratio: {r['prob_ratio']:.3f}\")\n",
      "    print(f\"  Clean: {r['c...\n",
      "--- Length: 905 chars, 20 lines ---\n",
      "\n",
      "=== Code Cell 11 (notebook index 14) ===\n",
      "# Phase 1: Layer-wise representation similarity analysis\n",
      "# Key question: At what layer do typoed word representations converge to clean representations?\n",
      "\n",
      "from transformer_lens import ActivationCache\n",
      "import torch.nn.functional as F\n",
      "from typing import Dict, List\n",
      "import numpy as np\n",
      "\n",
      "def get_residual_st...\n",
      "--- Length: 1690 chars, 44 lines ---\n",
      "\n",
      "=== Code Cell 12 (notebook index 15) ===\n",
      "# The tokenization differs - clean has \" weather\" as one token, typo has \" w\" and \"ether\"\n",
      "# Let's compare: the position AFTER the target word (where both sequences align again)\n",
      "\n",
      "# For clean: position 3 is \" is\" (index after \" weather\")\n",
      "# For typo: position 4 is \" is\" (index after \"ether\")\n",
      "\n",
      "# Track s...\n",
      "--- Length: 994 chars, 24 lines ---\n",
      "\n",
      "=== Code Cell 13 (notebook index 16) ===\n",
      "# Interesting! Similarity DECREASES then increases again in late layers\n",
      "# Let's visualize this and check more examples\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Check similarity at the LAST position (where prediction happens)\n",
      "clean_last_pos = len(clean_toks) - 1  # \"today\"\n",
      "typo_last_pos = len(typo_toks) -...\n",
      "--- Length: 1383 chars, 40 lines ---\n",
      "\n",
      "=== Code Cell 14 (notebook index 17) ===\n",
      "# Let's analyze multiple examples to see if the pattern holds\n",
      "# Focus on cases where we can match positions meaningfully\n",
      "\n",
      "def analyze_similarity_pattern(clean_text: str, typo_text: str, compare_at: str = 'last'):\n",
      "    \"\"\"\n",
      "    Analyze similarity pattern across layers.\n",
      "    compare_at: 'last' for final ...\n",
      "--- Length: 2965 chars, 78 lines ---\n",
      "\n",
      "=== Code Cell 15 (notebook index 18) ===\n",
      "# Key observation: Well-handled typos maintain HIGH similarity throughout all layers\n",
      "# Poorly-handled typos show LOWER similarity, especially in middle-to-late layers\n",
      "\n",
      "# Let's look at what differentiates these cases\n",
      "print(\"Characteristics of Well-handled vs Poorly-handled typos:\\n\")\n",
      "\n",
      "good_examples =...\n",
      "--- Length: 1575 chars, 36 lines ---\n",
      "\n",
      "=== Code Cell 16 (notebook index 19) ===\n",
      "# The divergence is small! Both good and bad typos maintain high similarity\n",
      "# This suggests the MODEL is robust in representations - the difference is elsewhere\n",
      "\n",
      "# Let's look at a more fine-grained analysis:\n",
      "# Instead of just last position, look at the TYPOED TOKEN position specifically\n",
      "\n",
      "def analyze...\n",
      "--- Length: 2946 chars, 75 lines ---\n",
      "\n",
      "=== Code Cell 17 (notebook index 20) ===\n",
      "# The position finding logic was wrong. Let's fix it properly\n",
      "# Focus on comparing ALIGNED positions where tokens match\n",
      "\n",
      "def find_aligned_positions(clean_toks, typo_toks, model):\n",
      "    \"\"\"Find positions where tokens align between clean and typo versions\"\"\"\n",
      "    clean_str = [model.to_string(t) for t in ...\n",
      "--- Length: 1755 chars, 48 lines ---\n",
      "\n",
      "=== Code Cell 18 (notebook index 21) ===\n",
      "# Interesting pattern: \n",
      "# - Token RIGHT AFTER typo (\" is\") shows lowest similarity (especially mid-layers)\n",
      "# - Similarity increases for tokens further from typo\n",
      "# - This suggests the typo \"corrupts\" nearby context but effect diminishes with distance\n",
      "\n",
      "# Let's verify this pattern across multiple examp...\n",
      "--- Length: 3286 chars, 90 lines ---\n",
      "\n",
      "=== Code Cell 19 (notebook index 23) ===\n",
      "# Phase 2: Activation Patching\n",
      "# Key question: Which components are causally responsible for typo handling?\n",
      "\n",
      "# We'll patch activations from CLEAN run into TYPO run to see which components\n",
      "# help recover the clean model behavior\n",
      "\n",
      "from transformer_lens import patching\n",
      "import torch\n",
      "\n",
      "# Choose a good tes...\n",
      "--- Length: 872 chars, 20 lines ---\n",
      "\n",
      "=== Code Cell 20 (notebook index 24) ===\n",
      "# Let's first verify the baseline behavior and understand tokenization\n",
      "clean_text = test_entry['clean']\n",
      "typo_text = test_entry['typo']\n",
      "\n",
      "clean_tokens = model.to_tokens(clean_text)\n",
      "typo_tokens = model.to_tokens(typo_text)\n",
      "\n",
      "print(\"Tokenization:\")\n",
      "print(f\"  Clean: {[model.to_string(t) for t in clean_tok...\n",
      "--- Length: 849 chars, 25 lines ---\n",
      "\n",
      "=== Code Cell 21 (notebook index 25) ===\n",
      "# Activation patching: Patch clean activations into the typo run\n",
      "# and measure how much it helps recover clean behavior\n",
      "\n",
      "def patch_residual_stream(typo_tokens, clean_cache, layer, pos=None):\n",
      "    \"\"\"\n",
      "    Run typo tokens but patch in clean residual stream at a specific layer.\n",
      "    If pos is None, patch...\n",
      "--- Length: 2383 chars, 61 lines ---\n",
      "\n",
      "=== Code Cell 22 (notebook index 26) ===\n",
      "# Use the correct TransformerLens hook API\n",
      "from functools import partial\n",
      "\n",
      "def patch_residual_hook(activation, hook, clean_cache, pos=None):\n",
      "    \"\"\"Hook function for patching residual stream\"\"\"\n",
      "    if pos is None:\n",
      "        # Patch all positions - handle length mismatch\n",
      "        min_len = min(activation...\n",
      "--- Length: 1456 chars, 40 lines ---\n",
      "\n",
      "=== Code Cell 23 (notebook index 27) ===\n",
      "# Plot recovery by layer\n",
      "recoveries = [r['recovery'] for r in patch_results]\n",
      "\n",
      "plt.figure(figsize=(12, 5))\n",
      "plt.bar(layers, recoveries, alpha=0.7, color='steelblue')\n",
      "plt.axhline(0, color='r', linestyle='--', label='Typo baseline')\n",
      "plt.axhline(1, color='g', linestyle='--', label='Clean baseline')\n",
      "plt.x...\n",
      "--- Length: 967 chars, 24 lines ---\n",
      "\n",
      "=== Code Cell 24 (notebook index 28) ===\n",
      "# Interesting! Early layers (0-4) give excellent recovery, late layers don't help\n",
      "# This suggests typo damage happens early and propagates forward\n",
      "\n",
      "# Let's do position-specific patching to understand WHERE the corruption matters\n",
      "# Focus on the typo position vs other positions\n",
      "\n",
      "# Token positions:\n",
      "# C...\n",
      "--- Length: 2097 chars, 49 lines ---\n",
      "\n",
      "=== Code Cell 25 (notebook index 29) ===\n",
      "# KEY FINDING: Position 4 (the typo position) is critical!\n",
      "# Patching just position 4 gives 63% recovery, while other positions give near 0%\n",
      "\n",
      "# Let's visualize this as a heatmap: layer x position\n",
      "print(\"Building layer x position patching heatmap...\")\n",
      "\n",
      "layer_pos_recovery = np.zeros((model.cfg.n_layer...\n",
      "--- Length: 989 chars, 23 lines ---\n",
      "\n",
      "=== Code Cell 26 (notebook index 30) ===\n",
      "# Visualize the layer x position heatmap\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "\n",
      "# Get token labels\n",
      "token_labels = [model.to_string(clean_tokens[0, i]) for i in range(clean_tokens.shape[1])]\n",
      "\n",
      "im = ax.imshow(layer_pos_recovery, aspect='auto', cmap='RdYlGn', vmin=-0....\n",
      "--- Length: 1238 chars, 33 lines ---\n",
      "\n",
      "=== Code Cell 27 (notebook index 32) ===\n",
      "# Now let's look at attention heads and MLPs specifically\n",
      "# Which components at the critical position (4) are most important?\n",
      "\n",
      "# Attention head patching at the typo position\n",
      "print(\"Attention head patching at position 4 (typo word)...\\n\")\n",
      "\n",
      "head_recovery = np.zeros((model.cfg.n_layers, model.cfg.n_hea...\n",
      "--- Length: 1339 chars, 30 lines ---\n",
      "\n",
      "=== Code Cell 28 (notebook index 33) ===\n",
      "# Visualize attention head importance\n",
      "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
      "\n",
      "# Heatmap\n",
      "ax1 = axes[0]\n",
      "im = ax1.imshow(head_recovery, aspect='auto', cmap='RdYlGn', vmin=-0.1, vmax=0.3)\n",
      "ax1.set_xlabel('Head')\n",
      "ax1.set_ylabel('Layer')\n",
      "ax1.set_title('Recovery from Patching Individual Attention H...\n",
      "--- Length: 1160 chars, 38 lines ---\n",
      "\n",
      "=== Code Cell 29 (notebook index 34) ===\n",
      "# Individual heads show very small effects! This suggests the effect is distributed\n",
      "# Let's check MLP layers instead\n",
      "\n",
      "print(\"MLP patching at position 4 (typo word)...\\n\")\n",
      "\n",
      "mlp_recovery = []\n",
      "\n",
      "def patch_mlp_output(activation, hook, clean_cache, pos):\n",
      "    \"\"\"Patch MLP output at a specific position\"\"\"\n",
      " ...\n",
      "--- Length: 1300 chars, 33 lines ---\n",
      "\n",
      "=== Code Cell 30 (notebook index 35) ===\n",
      "# KEY FINDING: MLP Layer 0 is responsible for ~58% of the recovery!\n",
      "# This is a major localized effect - the first MLP layer is critical for typo handling\n",
      "\n",
      "# Let's visualize MLP vs Attention contributions\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
      "\n",
      "# MLP recovery\n",
      "ax1 = axes[0]\n",
      "ax1.bar(range(mod...\n",
      "--- Length: 1235 chars, 33 lines ---\n",
      "\n",
      "=== Code Cell 31 (notebook index 37) ===\n",
      "# Let's verify this finding across multiple examples\n",
      "# Does MLP0 consistently show high importance for typo handling?\n",
      "\n",
      "print(\"Testing MLP0 importance across multiple examples...\\n\")\n",
      "\n",
      "mlp0_importance = []\n",
      "for entry in results[:15]:  # Test on 15 examples\n",
      "    clean_tokens_ex = model.to_tokens(entry['c...\n",
      "--- Length: 2232 chars, 60 lines ---\n",
      "\n",
      "=== Code Cell 32 (notebook index 38) ===\n",
      "# Average MLP0 recovery of 89% confirms this is a robust finding!\n",
      "# Now let's probe for \"clean word\" representations to understand what MLP0 is doing\n",
      "\n",
      "# Create a probing experiment:\n",
      "# Can we linearly decode the \"intended word\" from activations at different layers?\n",
      "\n",
      "from sklearn.linear_model import L...\n",
      "--- Length: 2126 chars, 54 lines ---\n",
      "\n",
      "=== Code Cell 33 (notebook index 39) ===\n",
      "# Train probes at each layer to predict the clean word from (possibly typoed) activations\n",
      "# This tests: At which layer can we recover the intended word identity?\n",
      "\n",
      "# Prepare data\n",
      "word_to_idx = {word: i for i, word in enumerate(probe_words)}\n",
      "\n",
      "# Train probe at each layer\n",
      "probe_accuracies = []\n",
      "\n",
      "for laye...\n",
      "--- Length: 1285 chars, 36 lines ---\n",
      "\n",
      "=== Code Cell 34 (notebook index 40) ===\n",
      "# Perfect accuracy at all layers! The probe is likely overfitting to this small dataset.\n",
      "# Let's try a more challenging probe: can we classify typo vs clean?\n",
      "\n",
      "# Also, let's look at the actual representation similarity between typo and clean versions\n",
      "\n",
      "print(\"Representation similarity (cosine) between...\n",
      "--- Length: 1040 chars, 24 lines ---\n",
      "\n",
      "=== Code Cell 35 (notebook index 41) ===\n",
      "# Interesting pattern: similarity is HIGH at layer 0, DROPS in middle layers, then RECOVERS at layer 23\n",
      "# This matches our earlier observation about distributed processing\n",
      "\n",
      "# Let's visualize this U-shaped pattern\n",
      "avg_sim_by_layer = []\n",
      "for layer in range(model.cfg.n_layers):\n",
      "    sims = []\n",
      "    for wor...\n",
      "--- Length: 1820 chars, 35 lines ---\n",
      "\n",
      "=== Code Cell 36 (notebook index 43) ===\n",
      "# Let's do one more analysis: look at what MLP0 neurons are doing\n",
      "# Specifically, which neurons in MLP0 are most important for typo recovery?\n",
      "\n",
      "print(\"Analyzing MLP0 neuron contributions to typo recovery...\\n\")\n",
      "\n",
      "# Use the same test case\n",
      "clean_text = \"I need to explain this problem\"\n",
      "typo_text = \"I nee...\n",
      "--- Length: 1357 chars, 33 lines ---\n",
      "\n",
      "=== Code Cell 37 (notebook index 44) ===\n",
      "# Interesting! Many neurons are more active for the TYPO than for clean\n",
      "# Let's look at the projection of these differences to output logits\n",
      "\n",
      "# Get the output weights of MLP0\n",
      "mlp0_out_weights = model.W_out[0]  # [d_mlp, d_model]\n",
      "\n",
      "print(f\"MLP0 output weights shape: {mlp0_out_weights.shape}\")\n",
      "\n",
      "# The d...\n",
      "--- Length: 1109 chars, 25 lines ---\n",
      "\n",
      "=== Code Cell 38 (notebook index 45) ===\n",
      "# Let's check if these top neurons are consistently important across different typo examples\n",
      "\n",
      "print(\"Checking consistency of top MLP0 neurons across examples...\\n\")\n",
      "\n",
      "neuron_importance_counts = {}\n",
      "\n",
      "for entry in results[:20]:\n",
      "    clean_tokens_ex = model.to_tokens(entry['clean'])\n",
      "    typo_tokens_ex = m...\n",
      "--- Length: 1918 chars, 47 lines ---\n",
      "\n",
      "=== Code Cell 39 (notebook index 46) ===\n",
      "# Neurons 2527, 2066, and 3244 appear most frequently\n",
      "# Let's investigate what these neurons respond to\n",
      "\n",
      "# For now, let's summarize our findings and create the documentation\n",
      "\n",
      "print(\"=\"*60)\n",
      "print(\"SUMMARY OF FINDINGS\")\n",
      "print(\"=\"*60)\n",
      "\n",
      "print(\"\"\"\n",
      "1. BEHAVIORAL FINDINGS:\n",
      "   - GPT-2 Medium shows ~60-70% r...\n",
      "--- Length: 1599 chars, 39 lines ---\n",
      "\n",
      "=== Code Cell 40 (notebook index 48) ===\n",
      "# Save final summary figure\n",
      "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
      "\n",
      "# 1. U-shaped similarity curve\n",
      "ax1 = axes[0, 0]\n",
      "ax1.plot(range(model.cfg.n_layers), avg_sim_by_layer, 'b-o', markersize=5)\n",
      "ax1.set_xlabel('Layer')\n",
      "ax1.set_ylabel('Cosine Similarity')\n",
      "ax1.set_title('Typo vs Clean Representa...\n",
      "--- Length: 2288 chars, 68 lines ---\n",
      "\n",
      "=== Code Cell 41 (notebook index 49) ===\n",
      "# List all files created\n",
      "import os\n",
      "\n",
      "print(\"Files created during this analysis:\\n\")\n",
      "\n",
      "print(\"Logs:\")\n",
      "for f in os.listdir('logs'):\n",
      "    print(f\"  logs/{f}\")\n",
      "\n",
      "print(\"\\nNotebooks:\")\n",
      "for f in os.listdir('notebooks'):\n",
      "    if f.endswith('.png') or f.endswith('.ipynb'):\n",
      "        print(f\"  notebooks/{f}\")\n",
      "\n",
      "prin...\n",
      "--- Length: 462 chars, 21 lines ---\n"
     ]
    }
   ],
   "source": [
    "# Print summary of each code cell to understand the structure\n",
    "for cell in code_cells:\n",
    "    source = cell['source']\n",
    "    # Get first few lines for summary\n",
    "    lines = source.split('\\n')\n",
    "    preview = '\\n'.join(lines[:5]) if len(lines) > 5 else source\n",
    "    if len(source) > 300:\n",
    "        preview = source[:300] + '...'\n",
    "    print(f\"\\n=== Code Cell {cell['cell_num']} (notebook index {cell['index']}) ===\")\n",
    "    print(preview)\n",
    "    print(f\"--- Length: {len(source)} chars, {len(lines)} lines ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac295694",
   "metadata": {},
   "source": [
    "# Code Evaluation: Typo Correction Mechanism Analysis\n",
    "\n",
    "This notebook evaluates the code implementation in `/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13`\n",
    "\n",
    "## Project Goal\n",
    "Based on the plan and codewalk files, this project aims to analyze how GPT-2 Medium handles typos, specifically:\n",
    "1. Understand the behavioral patterns of typo handling\n",
    "2. Analyze representation similarity across layers\n",
    "3. Perform activation patching to find causal components\n",
    "4. Identify the key circuit for typo handling (MLP0 at typo position)\n",
    "\n",
    "## Evaluation Structure\n",
    "- Total code cells in original notebook: 42\n",
    "- We will execute each cell and record: Runnable, Correct-Implementation, Redundant, Irrelevant flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767d2e76",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total code cells to evaluate: 42\n"
     ]
    }
   ],
   "source": [
    "# Initialize tracking data structures for evaluation\n",
    "evaluation_results = []\n",
    "\n",
    "# Store original notebook cells for reference\n",
    "notebook_path = '/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13/notebooks/2026-01-05-00-38_typo_circuit_analysis.ipynb'\n",
    "\n",
    "import json\n",
    "with open(notebook_path, 'r') as f:\n",
    "    original_notebook = json.load(f)\n",
    "\n",
    "# Get only code cells\n",
    "original_code_cells = []\n",
    "for i, cell in enumerate(original_notebook['cells']):\n",
    "    if cell['cell_type'] == 'code':\n",
    "        source = ''.join(cell['source'])\n",
    "        original_code_cells.append({\n",
    "            'cell_index': len(original_code_cells),\n",
    "            'notebook_index': i,\n",
    "            'source': source,\n",
    "            'cell_summary': source.split('\\n')[0][:80] if source else 'Empty'\n",
    "        })\n",
    "\n",
    "print(f\"Total code cells to evaluate: {len(original_code_cells)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e85df4",
   "metadata": {},
   "source": [
    "## Cell-by-Cell Execution and Evaluation\n",
    "\n",
    "Now executing each code cell from the original notebook and recording results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ca7b50",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 0: Setup working directory ===\n",
      "Working directory: /home/smallyan/eval_agent\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Setup working directory\n",
    "print(\"=== Evaluating Cell 0: Setup working directory ===\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    os.chdir('/home/smallyan/eval_agent')\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    cell0_runnable = \"Y\"\n",
    "    cell0_error = \"\"\n",
    "except Exception as e:\n",
    "    cell0_runnable = \"N\"\n",
    "    cell0_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell0_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c75d8e0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 1: Check GPU availability ===\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA H100 NVL\n",
      "CUDA memory: 99.95 GB\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check GPU availability\n",
    "print(\"=== Evaluating Cell 1: Check GPU availability ===\")\n",
    "\n",
    "try:\n",
    "    # Check GPU availability and set up environment\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    cell1_runnable = \"Y\"\n",
    "    cell1_error = \"\"\n",
    "except Exception as e:\n",
    "    cell1_runnable = \"N\"\n",
    "    cell1_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell1_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6c2de1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 2: Create necessary directories ===\n",
      "Directories created/verified: logs, notebooks\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Create necessary directories\n",
    "print(\"=== Evaluating Cell 2: Create necessary directories ===\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('notebooks', exist_ok=True)\n",
    "    print(\"Directories created/verified: logs, notebooks\")\n",
    "    cell2_runnable = \"Y\"\n",
    "    cell2_error = \"\"\n",
    "except Exception as e:\n",
    "    cell2_runnable = \"N\"\n",
    "    cell2_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell2_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4129ec",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 3: Load GPT-2 Medium via HookedTransformer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 Medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "Model loaded: gpt2-medium\n",
      "Number of layers: 24\n",
      "Number of attention heads: 16\n",
      "Hidden size: 1024\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load GPT-2 Medium via HookedTransformer\n",
    "print(\"=== Evaluating Cell 3: Load GPT-2 Medium via HookedTransformer ===\")\n",
    "\n",
    "try:\n",
    "    from transformer_lens import HookedTransformer\n",
    "    import torch\n",
    "\n",
    "    print(\"Loading GPT-2 Medium...\")\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2-medium\", device=device)\n",
    "    print(f\"Model loaded: {model.cfg.model_name}\")\n",
    "    print(f\"Number of layers: {model.cfg.n_layers}\")\n",
    "    print(f\"Number of attention heads: {model.cfg.n_heads}\")\n",
    "    print(f\"Hidden size: {model.cfg.d_model}\")\n",
    "    cell3_runnable = \"Y\"\n",
    "    cell3_error = \"\"\n",
    "except Exception as e:\n",
    "    cell3_runnable = \"N\"\n",
    "    cell3_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell3_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e396c6bc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 4: Test model with simple completion ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is'\n",
      "Next token prediction: ' the'\n",
      "\n",
      "Typo prompt: 'The capital of Frnace is'\n",
      "Next token prediction: ' the'\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test model with simple completion\n",
    "print(\"=== Evaluating Cell 4: Test model with simple completion ===\")\n",
    "\n",
    "try:\n",
    "    # Test that the model works with a simple completion\n",
    "    test_prompt = \"The capital of France is\"\n",
    "    tokens = model.to_tokens(test_prompt)\n",
    "    logits = model(tokens)\n",
    "    next_token = logits[0, -1].argmax()\n",
    "    print(f\"Prompt: '{test_prompt}'\")\n",
    "    print(f\"Next token prediction: '{model.to_string(next_token)}'\")\n",
    "\n",
    "    # Test with a typo\n",
    "    typo_prompt = \"The capital of Frnace is\"\n",
    "    tokens_typo = model.to_tokens(typo_prompt)\n",
    "    logits_typo = model(tokens_typo)\n",
    "    next_token_typo = logits_typo[0, -1].argmax()\n",
    "    print(f\"\\nTypo prompt: '{typo_prompt}'\")\n",
    "    print(f\"Next token prediction: '{model.to_string(next_token_typo)}'\")\n",
    "    cell4_runnable = \"Y\"\n",
    "    cell4_error = \"\"\n",
    "except Exception as e:\n",
    "    cell4_runnable = \"N\"\n",
    "    cell4_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell4_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f85247a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 5: Get top predictions function ===\n",
      "Clean prompt - 'The capital of France is':\n",
      "  ' the'          0.0601\n",
      "  ' a'            0.0484\n",
      "  ' now'          0.0265\n",
      "  ' Paris'        0.0244\n",
      "  ' in'           0.0233\n",
      "  ' not'          0.0201\n",
      "  ' one'          0.0170\n",
      "  ' France'       0.0165\n",
      "  ' known'        0.0160\n",
      "  ' home'         0.0151\n",
      "\n",
      "Typo prompt - 'The capital of Frnace is':\n",
      "  ' the'          0.1104\n",
      "  ' a'            0.0869\n",
      "  ' located'      0.0584\n",
      "  ' one'          0.0173\n",
      "  ' in'           0.0172\n",
      "  ' now'          0.0169\n",
      "  ' an'           0.0143\n",
      "  ' situated'     0.0134\n",
      "  ' St'           0.0090\n",
      "  ' known'        0.0090\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Get top predictions function\n",
    "print(\"=== Evaluating Cell 5: Get top predictions function ===\")\n",
    "\n",
    "try:\n",
    "    # Let's check top-k predictions for both cases\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    def get_top_predictions(logits, k=10):\n",
    "        \"\"\"Get top-k predicted tokens and their probabilities\"\"\"\n",
    "        probs = F.softmax(logits[0, -1], dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k)\n",
    "        return [(model.to_string(idx.item()), prob.item())\n",
    "                for idx, prob in zip(top_indices, top_probs)]\n",
    "\n",
    "    # Clean prompt\n",
    "    print(\"Clean prompt - 'The capital of France is':\")\n",
    "    clean_preds = get_top_predictions(logits)\n",
    "    for token, prob in clean_preds:\n",
    "        print(f\"  {repr(token):15} {prob:.4f}\")\n",
    "\n",
    "    # Typo prompt\n",
    "    print(\"\\nTypo prompt - 'The capital of Frnace is':\")\n",
    "    typo_preds = get_top_predictions(logits_typo)\n",
    "    for token, prob in typo_preds:\n",
    "        print(f\"  {repr(token):15} {prob:.4f}\")\n",
    "    cell5_runnable = \"Y\"\n",
    "    cell5_error = \"\"\n",
    "except Exception as e:\n",
    "    cell5_runnable = \"N\"\n",
    "    cell5_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell5_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69125e13",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 6: Test different types of typos ===\n",
      "Testing various typo types:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 'The cat sat on the' -> ' bed'\n",
      "Typo:  'The cat sat on teh' -> ' floor'\n",
      "Same prediction: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 'I want to eat an apple' -> '.'\n",
      "Typo:  'I want to eat an aple' -> 'a'\n",
      "Same prediction: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 'She went to the store' -> ' to'\n",
      "Typo:  'She went to teh store' -> ' and'\n",
      "Same prediction: False\n",
      "\n",
      "Clean: 'The weather is nice' -> ' and'\n",
      "Typo:  'The wether is nice' -> ' and'\n",
      "Same prediction: True\n",
      "\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test different types of typos\n",
    "print(\"=== Evaluating Cell 6: Test different types of typos ===\")\n",
    "\n",
    "try:\n",
    "    test_cases = [\n",
    "        # (clean, typoed, expected_completion)\n",
    "        (\"The cat sat on the\", \"The cat sat on teh\", \" mat\"),  # simple transposition\n",
    "        (\"I want to eat an apple\", \"I want to eat an aple\", \" and\"),  # deletion\n",
    "        (\"She went to the store\", \"She went to teh store\", \" and\"),  # transposition\n",
    "        (\"The weather is nice\", \"The wether is nice\", \" and\"),  # vowel deletion\n",
    "    ]\n",
    "\n",
    "    print(\"Testing various typo types:\\n\")\n",
    "    for clean, typo, expected in test_cases:\n",
    "        clean_tokens = model.to_tokens(clean)\n",
    "        typo_tokens = model.to_tokens(typo)\n",
    "        \n",
    "        clean_logits = model(clean_tokens)\n",
    "        typo_logits = model(typo_tokens)\n",
    "        \n",
    "        # Check if same top prediction\n",
    "        clean_pred = model.to_string(clean_logits[0, -1].argmax())\n",
    "        typo_pred = model.to_string(typo_logits[0, -1].argmax())\n",
    "        \n",
    "        print(f\"Clean: '{clean}' -> '{clean_pred}'\")\n",
    "        print(f\"Typo:  '{typo}' -> '{typo_pred}'\")\n",
    "        print(f\"Same prediction: {clean_pred == typo_pred}\")\n",
    "        print()\n",
    "    cell6_runnable = \"Y\"\n",
    "    cell6_error = \"\"\n",
    "except Exception as e:\n",
    "    cell6_runnable = \"N\"\n",
    "    cell6_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell6_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a98aeaf",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 7: Show tokenization function ===\n",
      "Tokenization comparison:\n",
      "\n",
      "weather         -> ['weather']\n",
      "wether          -> ['w', 'ether']\n",
      "\n",
      "beautiful       -> ['beaut', 'iful']\n",
      "beutiful        -> ['be', 'ut', 'iful']\n",
      "\n",
      "computer        -> ['computer']\n",
      "compter         -> ['comp', 'ter']\n",
      "\n",
      "afternoon       -> ['after', 'noon']\n",
      "afternon        -> ['after', 'non']\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Show tokenization function\n",
    "print(\"=== Evaluating Cell 7: Show tokenization function ===\")\n",
    "\n",
    "try:\n",
    "    import random\n",
    "\n",
    "    def show_tokenization(text):\n",
    "        \"\"\"Show how text is tokenized\"\"\"\n",
    "        tokens = model.to_tokens(text, prepend_bos=False)\n",
    "        token_strs = [model.to_string(t) for t in tokens[0]]\n",
    "        return token_strs\n",
    "\n",
    "    # Compare tokenizations\n",
    "    examples = [\n",
    "        (\"weather\", \"wether\"),\n",
    "        (\"beautiful\", \"beutiful\"),\n",
    "        (\"computer\", \"compter\"),\n",
    "        (\"afternoon\", \"afternon\")\n",
    "    ]\n",
    "\n",
    "    print(\"Tokenization comparison:\")\n",
    "    for clean, typo in examples:\n",
    "        clean_toks = show_tokenization(clean)\n",
    "        typo_toks = show_tokenization(typo)\n",
    "        print(f\"\\n{clean:15} -> {clean_toks}\")\n",
    "        print(f\"{typo:15} -> {typo_toks}\")\n",
    "    cell7_runnable = \"Y\"\n",
    "    cell7_error = \"\"\n",
    "except Exception as e:\n",
    "    cell7_runnable = \"N\"\n",
    "    cell7_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell7_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170f06b7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 8: Create systematic typo dataset ===\n",
      "Dataset created with 60 examples\n",
      "\n",
      "Sample entries:\n",
      "  Clean: The weather is very important\n",
      "  Typo:  The weathwr is very important\n",
      "  Type:  substitution\n",
      "\n",
      "  Clean: The weather is very important\n",
      "  Typo:  The ewather is very important\n",
      "  Type:  transposition\n",
      "\n",
      "  Clean: The weather is very important\n",
      "  Typo:  The weaher is very important\n",
      "  Type:  deletion\n",
      "\n",
      "\n",
      "Runnable: Y\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Create systematic typo dataset\n",
    "print(\"=== Evaluating Cell 8: Create systematic typo dataset ===\")\n",
    "\n",
    "try:\n",
    "    import random\n",
    "    from typing import List, Dict, Tuple\n",
    "\n",
    "    def create_typo_substitution(word: str, idx: int = None) -> str:\n",
    "        \"\"\"Create a typo by substituting a character\"\"\"\n",
    "        if len(word) < 2:\n",
    "            return word\n",
    "        keyboard_neighbors = {\n",
    "            'a': 'sq', 'b': 'vn', 'c': 'xv', 'd': 'sf', 'e': 'wr',\n",
    "            'f': 'dg', 'g': 'fh', 'h': 'gj', 'i': 'uo', 'j': 'hk',\n",
    "            'k': 'jl', 'l': 'ko', 'm': 'n', 'n': 'bm', 'o': 'ip',\n",
    "            'p': 'o', 'q': 'wa', 'r': 'et', 's': 'ad', 't': 'ry',\n",
    "            'u': 'yi', 'v': 'cb', 'w': 'qe', 'x': 'zc', 'y': 'tu',\n",
    "            'z': 'x'\n",
    "        }\n",
    "        if idx is None:\n",
    "            idx = random.randint(0, len(word) - 1)\n",
    "        char = word[idx].lower()\n",
    "        if char in keyboard_neighbors:\n",
    "            new_char = random.choice(keyboard_neighbors[char])\n",
    "            return word[:idx] + new_char + word[idx+1:]\n",
    "        return word\n",
    "\n",
    "    def create_typo_transposition(word: str, idx: int = None) -> str:\n",
    "        \"\"\"Create a typo by swapping adjacent characters\"\"\"\n",
    "        if len(word) < 2:\n",
    "            return word\n",
    "        if idx is None:\n",
    "            idx = random.randint(0, len(word) - 2)\n",
    "        return word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
    "\n",
    "    def create_typo_deletion(word: str, idx: int = None) -> str:\n",
    "        \"\"\"Create a typo by deleting a character\"\"\"\n",
    "        if len(word) < 3:\n",
    "            return word\n",
    "        if idx is None:\n",
    "            idx = random.randint(1, len(word) - 2)\n",
    "        return word[:idx] + word[idx+1:]\n",
    "\n",
    "    def create_typo_insertion(word: str, idx: int = None) -> str:\n",
    "        \"\"\"Create a typo by inserting a duplicate character\"\"\"\n",
    "        if len(word) < 2:\n",
    "            return word\n",
    "        if idx is None:\n",
    "            idx = random.randint(0, len(word) - 1)\n",
    "        return word[:idx] + word[idx] + word[idx:]\n",
    "\n",
    "    # Create dataset\n",
    "    template_sentences = [\n",
    "        (\"The {} is very important\", [\"weather\", \"problem\", \"answer\", \"meeting\", \"decision\"]),\n",
    "        (\"I need to {} this correctly\", [\"explain\", \"analyze\", \"complete\", \"understand\", \"evaluate\"]),\n",
    "        (\"She {} the document\", [\"received\", \"reviewed\", \"submitted\", \"approved\", \"rejected\"]),\n",
    "        (\"They will {} tomorrow\", [\"arrive\", \"depart\", \"return\", \"continue\", \"present\"]),\n",
    "    ]\n",
    "\n",
    "    random.seed(42)\n",
    "    dataset = []\n",
    "\n",
    "    for template, words in template_sentences:\n",
    "        for word in words:\n",
    "            # Original\n",
    "            clean_sentence = template.format(word)\n",
    "            \n",
    "            # Create different typo types\n",
    "            for typo_type, typo_func in [\n",
    "                (\"substitution\", create_typo_substitution),\n",
    "                (\"transposition\", create_typo_transposition),\n",
    "                (\"deletion\", create_typo_deletion),\n",
    "            ]:\n",
    "                typo_word = typo_func(word)\n",
    "                if typo_word != word:  # Only if typo was created\n",
    "                    typo_sentence = template.format(typo_word)\n",
    "                    dataset.append({\n",
    "                        \"clean\": clean_sentence,\n",
    "                        \"typo\": typo_sentence,\n",
    "                        \"clean_word\": word,\n",
    "                        \"typo_word\": typo_word,\n",
    "                        \"typo_type\": typo_type\n",
    "                    })\n",
    "\n",
    "    print(f\"Dataset created with {len(dataset)} examples\")\n",
    "    print(\"\\nSample entries:\")\n",
    "    for entry in dataset[:3]:\n",
    "        print(f\"  Clean: {entry['clean']}\")\n",
    "        print(f\"  Typo:  {entry['typo']}\")\n",
    "        print(f\"  Type:  {entry['typo_type']}\")\n",
    "        print()\n",
    "    cell8_runnable = \"Y\"\n",
    "    cell8_error = \"\"\n",
    "except Exception as e:\n",
    "    cell8_runnable = \"N\"\n",
    "    cell8_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell8_runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8257cd0d",
   "metadata": {
    "execution_status": "running"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Cell 9: Evaluate model performance on clean vs typo ===\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Evaluate model performance on clean vs typo\n",
    "print(\"=== Evaluating Cell 9: Evaluate model performance on clean vs typo ===\")\n",
    "\n",
    "try:\n",
    "    import torch.nn.functional as F\n",
    "    import numpy as np\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        clean_tokens = model.to_tokens(entry[\"clean\"])\n",
    "        typo_tokens = model.to_tokens(entry[\"typo\"])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            clean_logits = model(clean_tokens)\n",
    "            typo_logits = model(typo_tokens)\n",
    "        \n",
    "        # Get predictions\n",
    "        clean_pred = clean_logits[0, -1].argmax().item()\n",
    "        typo_pred = typo_logits[0, -1].argmax().item()\n",
    "        \n",
    "        # Get probabilities\n",
    "        clean_probs = F.softmax(clean_logits[0, -1], dim=-1)\n",
    "        typo_probs = F.softmax(typo_logits[0, -1], dim=-1)\n",
    "        \n",
    "        # Probability ratio for the clean prediction\n",
    "        clean_pred_prob_clean = clean_probs[clean_pred].item()\n",
    "        clean_pred_prob_typo = typo_probs[clean_pred].item()\n",
    "        \n",
    "        prob_ratio = clean_pred_prob_typo / clean_pred_prob_clean if clean_pred_prob_clean > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            **entry,\n",
    "            \"clean_pred\": model.to_string(clean_pred),\n",
    "            \"typo_pred\": model.to_string(typo_pred),\n",
    "            \"same_pred\": clean_pred == typo_pred,\n",
    "            \"clean_pred_prob_clean\": clean_pred_prob_clean,\n",
    "            \"clean_pred_prob_typo\": clean_pred_prob_typo,\n",
    "            \"prob_ratio\": prob_ratio\n",
    "        })\n",
    "\n",
    "    # Summary statistics\n",
    "    same_pred_rate = sum(r[\"same_pred\"] for r in results) / len(results)\n",
    "    avg_prob_ratio = np.mean([r[\"prob_ratio\"] for r in results])\n",
    "\n",
    "    print(f\"Total examples: {len(results)}\")\n",
    "    print(f\"Same prediction rate: {same_pred_rate:.1%}\")\n",
    "    print(f\"Average probability ratio: {avg_prob_ratio:.3f}\")\n",
    "    \n",
    "    # By typo type\n",
    "    print(\"\\nBy typo type:\")\n",
    "    for typo_type in [\"substitution\", \"transposition\", \"deletion\"]:\n",
    "        type_results = [r for r in results if r[\"typo_type\"] == typo_type]\n",
    "        type_same_rate = sum(r[\"same_pred\"] for r in type_results) / len(type_results)\n",
    "        print(f\"  {typo_type}: {type_same_rate:.1%} same prediction\")\n",
    "    cell9_runnable = \"Y\"\n",
    "    cell9_error = \"\"\n",
    "except Exception as e:\n",
    "    cell9_runnable = \"N\"\n",
    "    cell9_error = str(e)\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "print(f\"\\nRunnable: {cell9_runnable}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-15-12-55_CodeEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
