{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bcc77e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1b60f7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e015e7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo_claude_2026-01-05_00-38-13/\n",
      "  doc_only_evaluation/\n",
      "    code_critic_evaluation.ipynb\n",
      "    replication_evaluation.md\n",
      "    typo_clean_similarity_curve.png\n",
      "    divergence_by_layer.png\n",
      "    code_critic_summary.json\n",
      "    layer_position_patching.png\n",
      "    consistency_evaluation.json\n",
      "    similarity_weather.png\n",
      "    generalization_eval_summary.json\n",
      "    attention_head_patching.png\n",
      "    mlp_vs_attention.png\n",
      "    similarity_by_distance.png\n",
      "    generalization_eval.ipynb\n",
      "    final_summary.png\n",
      "    self_matching.ipynb\n",
      "    self_replication_evaluation.json\n",
      "    similarity_analysis.png\n",
      "  no_exe_evaluation/\n",
      "    generalization_eval_summary.json\n",
      "    generalization_eval.ipynb\n",
      "    code_critic_summary.json\n",
      "    code_critic_evaluation.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    replications/\n",
      "      no_exe_evaluation_replication.md\n",
      "      self_replication_evaluation.json\n",
      "  .git/\n",
      "    config\n",
      "    FETCH_HEAD\n",
      "    index\n",
      "    COMMIT_EDITMSG\n",
      "    description\n",
      "    HEAD\n",
      "    ORIG_HEAD\n",
      "    info/\n",
      "      exclude\n",
      "    refs/\n",
      "      tags/\n",
      "      heads/\n",
      "        eval2\n",
      "        new_c_cs_1\n",
      "        eval1\n",
      "        new_c_cs_2\n",
      "        eval3\n",
      "        main\n",
      "      remotes/\n",
      "        origin/\n",
      "          human_eval\n",
      "          eval3\n",
      "          HEAD\n",
      "          eval2\n",
      "          new_c_cs_2\n",
      "          eval1\n",
      "          new_c_cs_1\n",
      "          main\n",
      "    hooks/\n",
      "      prepare-commit-msg.sample\n",
      "      pre-rebase.sample\n",
      "      post-update.sample\n",
      "      commit-msg.sample\n",
      "      pre-receive.sample\n",
      "      pre-push.sample\n",
      "      update.sample\n",
      "      pre-applypatch.sample\n",
      "      sendemail-validate.sample\n",
      "      applypatch-msg.sample\n",
      "      pre-merge-commit.sample\n",
      "      push-to-checkout.sample\n",
      "      pre-commit.sample\n",
      "      fsmonitor-watchman.sample\n",
      "    logs/\n",
      "      HEAD\n",
      "      refs/\n",
      "        remotes/\n",
      "          origin/\n",
      "            main\n",
      "            eval2\n",
      "            new_c_cs_2\n",
      "            eval1\n",
      "            new_c_cs_1\n",
      "            human_eval\n",
      "            HEAD\n",
      "            eval3\n",
      "        heads/\n",
      "          eval3\n",
      "          main\n",
      "          eval1\n",
      "          new_c_cs_2\n",
      "          eval2\n",
      "          new_c_cs_1\n",
      "    objects/\n",
      "      3b/\n",
      "        826b5cf2b3d2635a929bf8928dfa60e044bf96\n",
      "        cd41a1c1fbc2f1fc6c388f5956c80f424123ad\n",
      "      19/\n",
      "        a9a85c09734f1c02627f567ac4fe931fcc7c74\n",
      "      c9/\n",
      "        8db8f1435e5f99ae395c2853b0b02bca17bf90\n",
      "      fe/\n",
      "        f03565882bd20403db94be5e36a2b177bb63d4\n",
      "      ab/\n",
      "        58566dc4704831f0b7918ef1e494632f0c2196\n",
      "        c93e0d1571da3dac11c27e43dc59cedf1c5e84\n",
      "      99/\n",
      "        a9b3368c0c0637115da487143582df3ddcb954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      a2/\n",
      "        464aeec407d820a0bafa72911fb238d3ab4340\n",
      "      f5/\n",
      "        29adef701bb8f918ff45ba3d578fb17eeafa57\n",
      "        c5c88b07e105ff5198f4f8012abb4e843e81b7\n",
      "      dd/\n",
      "        d96823b30825972fe441c8b81bc249e9d1851c\n",
      "        285a698c0b9ef1577650cf8966eeca1357d648\n",
      "      a8/\n",
      "        0bc6691f96adf62e97e8bddde7da3cc000f9d7\n",
      "        8a87c897ffc4e62d4a8753a4041ca0eb9e42f6\n",
      "      5f/\n",
      "        bed683b167c48c8349e7fa083b3841c759654e\n",
      "        3afebacf7efbc976228b06e78eeb384da906d9\n",
      "      2a/\n",
      "        9fcf50d833578f55790b8f31b619759a784fb5\n",
      "      93/\n",
      "        050e6b3a75c131d0842c3b0a8e9dbeedf1a2f7\n",
      "      b0/\n",
      "        5c0050f11686136d242c7e8d2792719366a4eb\n",
      "      45/\n",
      "        d63bf1d76efab4ae841e721702c56c10de914f\n",
      "      38/\n",
      "        71b2b117ef69068966481f0785096725da781f\n",
      "      1c/\n",
      "        e3d1d100571eef0a67bf2f88ecaaf94d5afb0e\n",
      "      pack/\n",
      "      b7/\n",
      "        606a3228d311b56a662f17561beeda0e54f3f1\n",
      "      e0/\n",
      "        5772c498d1d52a364258a6059513da0508c3b7\n",
      "      42/\n",
      "        5a4b6d0d253222f0f1b17aef64b3d385c046c9\n",
      "        2771ff925e777cdc94c3bc36f6809240f3c4e3\n",
      "      6c/\n",
      "        ed53847cddaf83b07e7e6369620e91a4db8072\n",
      "      1d/\n",
      "        0132023ea77e3dfa2773430dc08b02f0fe248f\n",
      "      70/\n",
      "        aad6a77fa030b74f72a5ec1338f10ab55cf1d9\n",
      "      f2/\n",
      "        79e856775a060e34e9c34c64d571882bd9ddd3\n",
      "      a5/\n",
      "        02d455e39c5295d7ed57a842735cbbcf131b02\n",
      "        dbb5545e54f68f6dc0217789e9552f7adf6776\n",
      "      dc/\n",
      "        46e0611bbbf1ea937d40515c96c60350fa7cae\n",
      "        65fbd6255abd66196f80fbdfc629ea3e257905\n",
      "      2f/\n",
      "        ec9923e63e218d4a42201fed27d52f9ef14097\n",
      "      5a/\n",
      "        7ebbd3edb263836015fc3d6c7c17fa635640a0\n",
      "        258d515b2acd6fe92e2da5bd74aa53c235ec46\n",
      "      94/\n",
      "        5aa8604efd9688f0c37ced389b9ee6f7b16829\n",
      "        f363fee79ceb7ef6936cf41967c8e6ee3ed483\n",
      "      d9/\n",
      "        2c2cfecfa0476c7efbb68d2a1ce1c33932a2bc\n",
      "      fb/\n",
      "        ae0155223d5acc502a04a5012f0f7f409aa66f\n",
      "        8df9a64f588c0889b267bacdf326ac4dc20f01\n",
      "      d3/\n",
      "        450c3bfcb0485dfb2e56385b486abdd2393cbe\n",
      "      3e/\n",
      "        1a0307054585f652f57fe79ab366b2f7738dc9\n",
      "        6d4efa3f6cc066daed7c723747d0db82d05bca\n",
      "      4b/\n",
      "        56d055455119cd4fe4acb67c2444d67b82f1b8\n",
      "      87/\n",
      "        63f46c3d8f9b53c1f1a2dbaa68e459d9c57a29\n",
      "      69/\n",
      "        1cad5d6e87c9ddc1b226dfd837e17aa4a014c3\n",
      "        c9b67aae8475d4202cddd7720fde6a1b52b85d\n",
      "        6a97da29653d4d28ddf236f27cae114db365fd\n",
      "      bc/\n",
      "        1133a2e9ba9343eaa9325921e72d6878cccd1c\n",
      "      83/\n",
      "        aca7ac97900cc78db6f0af5d801fc6c03e8c16\n",
      "      3a/\n",
      "        e8b0cc7491e4b136e86c8dc023e56e5b89718d\n",
      "        7627b23a25c714c4b10035c0403374ad8438c6\n",
      "      c0/\n",
      "        3d835a4163b3c16772145694ddca38146110e1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      28/\n",
      "        cc9c74f834b8328f5c2c94db48c9fbb3db43df\n",
      "        0a82675975dca8b918857cd3511aa6ded7377d\n",
      "      aa/\n",
      "        9820e796e606f5f8b9538bc63c83c39dc5e608\n",
      "      ff/\n",
      "        5051394819ea2644f4600bff6925b55bd16fbf\n",
      "        a22baba7c2c32432e71aca44fc2ed1f4556b3e\n",
      "      90/\n",
      "        c4d2091c51606a628a98f19034dae3a7cd7b0a\n",
      "        985453fbb3b73f648a1a18bac1b48e357101aa\n",
      "      a1/\n",
      "        ce6f7419b32f217bb4ceb049c4a248b273ff80\n",
      "      74/\n",
      "        e8e523cffc30a3207bc739e1093f89109e339b\n",
      "      8c/\n",
      "        1ae1950fe1810912f8a6077713bd8b3d6f54dd\n",
      "        1aaa2e36df01ac5c05f5b5367efa0d14585379\n",
      "      b3/\n",
      "        d5e1e8c76774253df47c631310563592f039b9\n",
      "      41/\n",
      "        89b126d8b84affbf3c2813b4ff2b738c348ab2\n",
      "      2e/\n",
      "        73a294e0603a01fee32544dfe4160ae7f162f1\n",
      "        d2ca498539272de2bccd199330bf61bc20685d\n",
      "      f1/\n",
      "        e9377995aecacea229ef9404e0ef2e61acce56\n",
      "      c7/\n",
      "        981326b220f7fb09b1f6d34ec230d0c9c24f77\n",
      "        f477e869f1e273c1ec1adc9275000295f7a35b\n",
      "      58/\n",
      "        afd3d27a2260d16da0ed7f0c0d002934a58a51\n",
      "      fa/\n",
      "        63f043e24ad526f2f0f0ac80384388858717bc\n",
      "        33054fdc99a44872a8ffc00b2902f54fe99ce3\n",
      "      af/\n",
      "        93c70bdb2e1604e6098a243bcfbc85167a09da\n",
      "      bd/\n",
      "        a593573b2de3f8d9178288a69dac7e172dcf8a\n",
      "        077e644a0e2813eaff76b2bfcbcec534d9fb98\n",
      "      ec/\n",
      "        27ce20878abe244da38c3c9df2333e792c8761\n",
      "      4a/\n",
      "        3a6b93685d0dd3de5e86e63647750d4063a15b\n",
      "      c5/\n",
      "        6eca960b52439d5fb7280c82c00ea3b28d0757\n",
      "        5d2ee9229dbaf33f9c1ac909385d9c7a9689b5\n",
      "      27/\n",
      "        b86495d1045b5921fbd447b206ae71d04cefed\n",
      "      9e/\n",
      "        2b402e74eeb134a2d718079a2a6ef4d9dc80d6\n",
      "      d8/\n",
      "        d0c3818d7f2b100fcc01aae547232934e421c6\n",
      "        f0b1987efe6ed3f63ec9229608827b1e170e56\n",
      "        86a83dcb971b36f8ef4891c8b42eda4a3a3b67\n",
      "      ad/\n",
      "        6b5f1630d69de6dd96623e79c4c35e3fa3ef5e\n",
      "        3fd2b6a95c6c14010538f4474c867d55c47ec5\n",
      "      4c/\n",
      "        651a1b345aecf8542d892952fdec55d1c1af37\n",
      "      68/\n",
      "        5c153e7369375dcafaa58bad4bd423967331ff\n",
      "      49/\n",
      "        a76e05095b59bd447ff282f35b81aabf0bffd9\n",
      "      1e/\n",
      "        3ac93add36854711c4453fc0d82538b7a097c5\n",
      "        12e3ddb5e6cf4a20e458aedaec863e64a8ee12\n",
      "      43/\n",
      "        028577b149afdf6594b661790722d718d4c9aa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8f/\n",
      "        0361165f42fa04a98da54da44ec9179aa7c0c2\n",
      "      e1/\n",
      "        0fa8c0d7dd47e83cb4401564005c392399133d\n",
      "        71b5d7121402f3e37ebe22b781657587b08963\n",
      "      b6/\n",
      "        eaaa573eb4532e7752a95d0101b3b95b403263\n",
      "      info/\n",
      "      95/\n",
      "        fd53e72a347acbfb24cfe4aef6020eaee2b4f7\n",
      "        4cbf2ac837c174a311fd506afcb35f5e563936\n",
      "      71/\n",
      "        9e997abcd41d50e44dcd77ddc3760ecba730b8\n",
      "      06/\n",
      "        e9becb601d6edbf8bd589353b3ddfcd051c47b\n",
      "      92/\n",
      "        1f57e35a6cc11573289bd4a352a4514ff969ec\n",
      "      de/\n",
      "        6adcb1102c84d56cdfeddab8296c9a5a1a2e12\n",
      "        7dede0b1fcec0b92fe32cac065cbcf099756c4\n",
      "      a9/\n",
      "        fe2ee411aaec957564dc85c0d528e1306900ad\n",
      "      a3/\n",
      "        ecb6ed9f68ce1fc084cc34a565218bd3045ecf\n",
      "      98/\n",
      "        af230624b2c15b7eecad8bfd150850e4199e52\n",
      "      01/\n",
      "        e763dbc41b004d0942f20b6298c1f49ea5eaed\n",
      "      76/\n",
      "        2b27357841a1833c47b3297968a95735319f58\n",
      "      6e/\n",
      "        27e877f7b0532132773dd18dbf8aa7ed9fb4ad\n",
      "      1b/\n",
      "        8bd177e2efef6bbdcbd488313f8be630903858\n",
      "        9540e51efdbde6703067ed56655723510a2d16\n",
      "        54beae054ba1b04d004ff1d5c9cb230269a8b9\n",
      "      b1/\n",
      "        068accef687b1e5187010c9ae8d72c26f4ffa9\n",
      "        258192b81e9d53c3c657d2ba042e5ece339f88\n",
      "      e6/\n",
      "        ec9ed3f8b5a957ee44a8fccdad3049d38c4f3e\n",
      "      12/\n",
      "        6af41011cb260d75e9a32ad1f2534e5e060dcc\n",
      "      65/\n",
      "        e048113f654c8e549ddda032863cf02768c7d1\n",
      "      ef/\n",
      "        7f726d64649bce2a8bf09549786f353ea068e2\n",
      "      18/\n",
      "        82548718d6800da2de6f174b6bacaba226866a\n",
      "      c2/\n",
      "        58b2e0f57e37877b7c48cabb4f916de871b2df\n",
      "        44e98ebda01374d0041f88795216789649f771\n",
      "      57/\n",
      "        71f8cb5f887ad9ea6e06c30fc19c9ec5c4fd77\n",
      "      20/\n",
      "        2b3b29b728f7771af1028ad916d74ca0eb2016\n",
      "      9b/\n",
      "        f7051ebf85b41a05a091709785250c9279e04c\n",
      "        0d2070a9c50019e83056ba9cd701bcee9dcf56\n",
      "        c38631daf82be3442aac0a94a8d396b0852a67\n",
      "      7f/\n",
      "        220ffff732dec85f6fa2aac3b9bc2713d3c82e\n",
      "      0a/\n",
      "        79842c04e85548abd61a558b1e7b152c3af185\n",
      "        eb65628ec5d89366f6153c5414d005e540ad10\n",
      "      c8/\n",
      "        ee904343daada9c7b0e6e7687769c81738e0a5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fd/\n",
      "        7c74cb3d79af0ebdb41c9f7f4421de4faeb131\n",
      "        10286e12e407759eca224a9259ccaa8f89f476\n",
      "      7b/\n",
      "        53f4aa9f483abd52e9aec31c17d9e08db536f6\n",
      "      9f/\n",
      "        7d341c26d648e4a7d3dabd572dca21308fbea5\n",
      "      85/\n",
      "        023728892334753b6d8718ec9bf0dd215b3e7b\n",
      "        ba71dca94005e22c5638be350c107ec2968ddc\n",
      "        f5dd6cf3c86171dc60f5865421f16f845ba6e8\n",
      "      eb/\n",
      "        bd560d8c6825c46f534f6a9240210c0c868bc3\n",
      "      16/\n",
      "        e67deb1eee624bdf522af1098b5236cd0fc1b6\n",
      "      61/\n",
      "        005d919c2606092837df096417767f4a11e01c\n",
      "      b5/\n",
      "        84eba706617c822066495aedc73bea3ee9f598\n",
      "      8e/\n",
      "        e26ce117dc10b3251027091bf9fda93cc0dc1e\n",
      "        5cfe4f8d519f534e39ecd37a215a9098197c69\n",
      "      37/\n",
      "        880a1300e88c670d1d5e0c012df8894a593968\n",
      "        838d62fdb1859f535e829a45ba23b0e678f832\n",
      "      6a/\n",
      "        230f85fef9623c958e912d20813188d0a03af1\n",
      "      f0/\n",
      "        82a11c663271d5f47707230ab0e6770f71fb1e\n",
      "      a7/\n",
      "        50c2ce6f8ce35200c2fc5adda2f01a7603d7d8\n",
      "      da/\n",
      "        55a0d87768d8d9bd91eda728cc556efc893dbf\n",
      "      cf/\n",
      "        e9c62d23a42d459f1d1f83eae1fb9c40f22721\n",
      "        1d7a45d11586c6f33424688ee0e31ec59b2279\n",
      "      96/\n",
      "        6ef8a5dc34e5fb0799d9197656f79b631e3319\n",
      "        92ad003c0d2eb234861d624a46180155cefd76\n",
      "      5c/\n",
      "        58a28d948b042538ac8bb8001dd59baa9c6680\n",
      "      75/\n",
      "        c5d4525e5b74f7fefbdd3955063352f32a039c\n",
      "        1389f87241de067799966c0f3fa4523f7947c1\n",
      "      ca/\n",
      "        5150fcbe7d49a9737a511e5261e0e07a0137d7\n",
      "      91/\n",
      "        b6bbc4f3665d002210b94c0cadf7abc2292c5b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5d/\n",
      "        c5d50bf631977d8ec95e8b6a2b1d8a3b1da13b\n",
      "        22eef058bb96c18048e796773a38824f596c2a\n",
      "        7b3f40b41d3d9b68c2df6d479001fe1e61aa53\n",
      "      2c/\n",
      "        240b56f970a3b9a05cfd38717395ecdbfbfd27\n",
      "      8b/\n",
      "        cc9c6f1d12bc24dc025979bb979fe91aab6f57\n",
      "      30/\n",
      "        8f01257d088c41e0deb4da160bd75d2a7f1d43\n",
      "      47/\n",
      "        3c1b636903d29d343385ddfc4378e265081ce7\n",
      "      1a/\n",
      "        dff7d687c8d3718a777f1ffee8e0d1214e1430\n",
      "        26d5cfeaef88699744861df0276694d9c2b6d1\n",
      "      b8/\n",
      "        e666ac1102bb278d2fbbca81f65f1b0fc3547b\n",
      "      ee/\n",
      "        ada2b432edb9eb8c1f5f068d4182f2387a9718\n",
      "      11/\n",
      "        d48e6f150f649b73b331cfd4314c76624705ec\n",
      "      88/\n",
      "        14de83c569566641a6910bb14231ae699edb21\n",
      "      0b/\n",
      "        f48d0686fa0cb0f1ddc76f70ebb57e35beb00e\n",
      "        dcbe2672dfe20602dcf2f0768b56c909429628\n",
      "        58b5f54d5bf24857408ab9cfc0c019499f11aa\n",
      "        21dcb786a36ca83b7aed254b44dab1a3ce9457\n",
      "      7e/\n",
      "        234b3ed93e384490fb1f4fbaae615eebb7df78\n",
      "      29/\n",
      "        9d800c1fe32a3fc8180ff425cd96d7f17ca36a\n",
      "      9a/\n",
      "        72df8c36a2fda75bc41342d22db1aed31d11aa\n",
      "      54/\n",
      "        8e5ef6ed0f34b7a7495c25c36a87ec07b10c7a\n",
      "  results/\n",
      "    real_circuits_1.json\n",
      "  notebooks/\n",
      "    2026-01-05-00-38_typo_circuit_analysis.ipynb\n",
      "  logs/\n",
      "    code_walk.md\n",
      "    open_typo_claude.log\n",
      "    plan_v2.md\n",
      "    plan_v1.md\n",
      "    documentation.md\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the repo\n",
    "repo_path = '/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13'\n",
    "for root, dirs, files in os.walk(repo_path):\n",
    "    level = root.replace(repo_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f'{subindent}{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738bc63c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells: 50\n",
      "Kernel: {'display_name': 'Scribe: 2026-01-05-00-38_typo_circuit_analysis', 'language': 'python', 'name': 'python3'}\n",
      "\n",
      "Cell 0 (Code): import os\n",
      "os.chdir('/home/smallyan/eval_agent')\n",
      "print(f\"Work... | Outputs: 1\n",
      "Cell 1 (Code): # Check GPU availability and set up environment\n",
      "import torch... | Outputs: 1\n",
      "Cell 2 (Code): # Create necessary directories\n",
      "import os\n",
      "\n",
      "os.makedirs('logs'... | Outputs: 1\n",
      "Cell 3 (Markdown): # Plan V1: Mechanistic Analysis of Typo Correction in Language Models\n",
      "\n",
      "## Initial Hypothesis\n",
      "\n",
      "**Hypo...\n",
      "Cell 4 (Code): # Load GPT-2 Medium via HookedTransformer\n",
      "from transformer_l... | Outputs: 5\n",
      "Cell 5 (Code): # Test that the model works with a simple completion\n",
      "test_pr... | Outputs: 1\n",
      "Cell 6 (Code): # Let's check top-k predictions for both cases\n",
      "import torch.... | Outputs: 1\n",
      "Cell 7 (Code): # Let's try different types of typos to understand the model... | Outputs: 3\n",
      "Cell 8 (Markdown): ## Observation: GPT-2 Shows Variable Typo Robustness\n",
      "\n",
      "Initial tests show that GPT-2 medium has **var...\n",
      "Cell 9 (Code): # Let's examine the tokenization of clean vs typoed words to... | Outputs: 1\n",
      "Cell 10 (Code): # Create a systematic typo dataset\n",
      "# We'll categorize by typ... | Outputs: 1\n",
      "Cell 11 (Code): # Evaluate model performance on clean vs typo sentences\n",
      "impo... | Outputs: 1\n",
      "Cell 12 (Code): # Let's look at specific cases: best and worst typo handling... | Outputs: 1\n",
      "Cell 13 (Markdown): ## Dataset Created and Behavioral Analysis Complete\n",
      "\n",
      "Key findings from behavioral analysis:\n",
      "1. **Typ...\n",
      "Cell 14 (Code): # Phase 1: Layer-wise representation similarity analysis\n",
      "# K... | Outputs: 1\n",
      "Cell 15 (Code): # The tokenization differs - clean has \" weather\" as one tok... | Outputs: 1\n",
      "Cell 16 (Code): # Interesting! Similarity DECREASES then increases again in ... | Outputs: 2\n",
      "Cell 17 (Code): # Let's analyze multiple examples to see if the pattern hold... | Outputs: 2\n",
      "Cell 18 (Code): # Key observation: Well-handled typos maintain HIGH similari... | Outputs: 2\n",
      "Cell 19 (Code): # The divergence is small! Both good and bad typos maintain ... | Outputs: 1\n",
      "Cell 20 (Code): # The position finding logic was wrong. Let's fix it properl... | Outputs: 1\n",
      "Cell 21 (Code): # Interesting pattern: \n",
      "# - Token RIGHT AFTER typo (\" is\") s... | Outputs: 3\n",
      "Cell 22 (Markdown): ## Key Finding: Typo Effect Diminishes with Distance\n",
      "\n",
      "The similarity analysis reveals:\n",
      "1. **Initial ...\n",
      "Cell 23 (Code): # Phase 2: Activation Patching\n",
      "# Key question: Which compone... | Outputs: 1\n",
      "Cell 24 (Code): # Let's first verify the baseline behavior and understand to... | Outputs: 1\n",
      "Cell 25 (Code): # Activation patching: Patch clean activations into the typo... | Outputs: 1\n",
      "Cell 26 (Code): # Use the correct TransformerLens hook API\n",
      "from functools im... | Outputs: 1\n",
      "Cell 27 (Code): # Plot recovery by layer\n",
      "recoveries = [r['recovery'] for r i... | Outputs: 2\n",
      "Cell 28 (Code): # Interesting! Early layers (0-4) give excellent recovery, l... | Outputs: 2\n",
      "Cell 29 (Code): # KEY FINDING: Position 4 (the typo position) is critical!\n",
      "#... | Outputs: 2\n",
      "Cell 30 (Code): # Visualize the layer x position heatmap\n",
      "import matplotlib.p... | Outputs: 2\n",
      "Cell 31 (Markdown): ## Critical Finding: Typo Position Drives the Damage\n",
      "\n",
      "The layer x position patching reveals:\n",
      "\n",
      "1. **P...\n",
      "Cell 32 (Code): # Now let's look at attention heads and MLPs specifically\n",
      "# ... | Outputs: 2\n",
      "Cell 33 (Code): # Visualize attention head importance\n",
      "fig, axes = plt.subplo... | Outputs: 2\n",
      "Cell 34 (Code): # Individual heads show very small effects! This suggests th... | Outputs: 4\n",
      "Cell 35 (Code): # KEY FINDING: MLP Layer 0 is responsible for ~58% of the re... | Outputs: 2\n",
      "Cell 36 (Markdown): ## Major Discovery: MLP Layer 0 is the Key Component\n",
      "\n",
      "**The first MLP layer (m0) accounts for ~58% o...\n",
      "Cell 37 (Code): # Let's verify this finding across multiple examples\n",
      "# Does ... | Outputs: 8\n",
      "Cell 38 (Code): # Average MLP0 recovery of 89% confirms this is a robust fin... | Outputs: 1\n",
      "Cell 39 (Code): # Train probes at each layer to predict the clean word from ... | Outputs: 1\n",
      "Cell 40 (Code): # Perfect accuracy at all layers! The probe is likely overfi... | Outputs: 1\n",
      "Cell 41 (Code): # Interesting pattern: similarity is HIGH at layer 0, DROPS ... | Outputs: 2\n",
      "Cell 42 (Markdown): ## U-Shaped Similarity Pattern\n",
      "\n",
      "The representation similarity between typoed and clean words follows...\n",
      "Cell 43 (Code): # Let's do one more analysis: look at what MLP0 neurons are ... | Outputs: 1\n",
      "Cell 44 (Code): # Interesting! Many neurons are more active for the TYPO tha... | Outputs: 1\n",
      "Cell 45 (Code): # Let's check if these top neurons are consistently importan... | Outputs: 2\n",
      "Cell 46 (Code): # Neurons 2527, 2066, and 3244 appear most frequently\n",
      "# Let'... | Outputs: 1\n",
      "Cell 47 (Markdown): # Summary of Typo Correction Mechanism Analysis\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "1. **MLP Layer 0 is the primary co...\n",
      "Cell 48 (Code): # Save final summary figure\n",
      "fig, axes = plt.subplots(2, 2, f... | Outputs: 2\n",
      "Cell 49 (Code): # List all files created\n",
      "import os\n",
      "\n",
      "print(\"Files created dur... | Outputs: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's read the notebook and extract key information\n",
    "import json\n",
    "\n",
    "notebook_path = '/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13/notebooks/2026-01-05-00-38_typo_circuit_analysis.ipynb'\n",
    "with open(notebook_path, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "print(f\"Number of cells: {len(nb['cells'])}\")\n",
    "print(f\"Kernel: {nb.get('metadata', {}).get('kernelspec', {})}\")\n",
    "print()\n",
    "\n",
    "# List markdown and code cell types and their content summary\n",
    "for i, cell in enumerate(nb['cells']):\n",
    "    cell_type = cell['cell_type']\n",
    "    if cell_type == 'markdown':\n",
    "        content = ''.join(cell['source'])[:200]\n",
    "        print(f\"Cell {i} (Markdown): {content[:100]}...\")\n",
    "    else:\n",
    "        source_preview = ''.join(cell['source'])[:100]\n",
    "        outputs = cell.get('outputs', [])\n",
    "        print(f\"Cell {i} (Code): {source_preview[:60]}... | Outputs: {len(outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e420f7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cell 35 (code):\n",
      "# KEY FINDING: MLP Layer 0 is responsible for ~58% of the recovery!\n",
      "# This is a major localized effect - the first MLP layer is critical for typo handling\n",
      "\n",
      "# Let's visualize MLP vs Attention contributions\n",
      "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
      "\n",
      "# MLP recovery\n",
      "ax1 = axes[0]\n",
      "ax1.bar(range(model.cfg.n_layers), mlp_recovery, alpha=0.7, color='coral', label='MLP')\n",
      "ax1.set_xlabel('Layer')\n",
      "ax1.set_ylabel('Recovery')\n",
      "ax1.set_title('MLP Patching Recovery at Position 4')\n",
      "ax1.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
      "ax1.legend()\n",
      "\n",
      "# Attention sum per layer\n",
      "ax2 = axes[1]\n",
      "attn_per_layer = np.sum(head_recovery, axis=1)\n",
      "ax2.bar(range(model.cfg.n_layers), attn_per_layer, alpha=0.7, color='steelblue', label='Attention (sum of heads)')\n",
      "ax2.set_xlabel('Layer')\n",
      "ax2.set_ylabel('Recovery')\n",
      "ax2.set_title('Attention Patching Recovery at Position 4\\n(Sum across all heads)')\n",
      "ax2.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
      "ax2.legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('notebooks/mlp_vs_attention.png', dpi=150)\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nSummary:\")\n",
      "print(f\"MLP Layer 0 recovery: {mlp_recovery[0]:.4f}\")\n",
      "print(f\"All other MLPs combined: {sum(mlp_recovery[1:]):.4f}\")\n",
      "print(f\"All attention heads combined: {np.sum(head_recovery):.4f}\")\n",
      "Output: ['<Figure size 1400x500 with 2 Axes>']\n",
      "Output: ['\\n', 'Summary:\\n', 'MLP Layer 0 recovery: 0.5804\\n', 'All other MLPs combined: 0.0775\\n', 'All attention heads combined: 0.0005\\n']\n",
      "================================================================================\n",
      "Cell 36 (markdown):\n",
      "## Major Discovery: MLP Layer 0 is the Key Component\n",
      "\n",
      "**The first MLP layer (m0) accounts for ~58% of typo recovery!**\n",
      "\n",
      "This is a striking finding:\n",
      "- Attention heads collectively contribute almost nothing (0.05%)\n",
      "- MLP Layer 0 alone provides 58% recovery\n",
      "- All other MLPs combined provide only 8% recovery\n",
      "\n",
      "This suggests that **MLP Layer 0 acts as a \"token normalization\" layer** that maps unusual subword token sequences to more standard representations. When the clean version's MLP0 output is patched in, it provides the model with a \"corrected\" representation that propagates forward.\n",
      "\n",
      "**Hypothesis update**: The model doesn't explicitly \"correct\" typos. Instead, MLP0 encodes a representation that either captures the intended meaning (for robust cases) or fails to do so (for non-robust cases). The difference in behavior comes from whether MLP0's output for the typoed tokens is similar enough to the clean version.\n",
      "================================================================================\n",
      "Cell 37 (code):\n",
      "# Let's verify this finding across multiple examples\n",
      "# Does MLP0 consistently show high importance for typo handling?\n",
      "\n",
      "print(\"Testing MLP0 importance across multiple examples...\\n\")\n",
      "\n",
      "mlp0_importance = []\n",
      "for entry in results[:15]:  # Test on 15 examples\n",
      "    clean_tokens_ex = model.to_tokens(entry['clean'])\n",
      "    typo_tokens_ex = model.to_tokens(entry['typo'])\n",
      "    \n",
      "    # Get clean cache\n",
      "    _, clean_cache_ex = model.run_with_cache(clean_tokens_ex)\n",
      "    \n",
      "    # Baseline\n",
      "    with torch.no_grad():\n",
      "        clean_logits_ex = model(clean_tokens_ex)\n",
      "        typo_logits_ex = model(typo_tokens_ex)\n",
      "    \n",
      "    expected_token_ex = model.to_tokens(entry['expected_next'])[0, 1]\n",
      "    clean_prob_ex = F.softmax(clean_logits_ex[0, -1], dim=-1)[expected_token_ex].item()\n",
      "    typo_prob_ex = F.softmax(typo_logits_ex[0, -1], dim=-1)[expected_token_ex].item()\n",
      "    \n",
      "    # Find the typo position (roughly where tokens differ)\n",
      "    # Simple heuristic: find first position where clean and typo differ\n",
      "    clean_str = [model.to_string(t) for t in clean_tokens_ex[0]]\n",
      "    typo_str = [model.to_string(t) for t in typo_tokens_ex[0]]\n",
      "    \n",
      "    typo_pos = None\n",
      "    for i in range(min(len(clean_str), len(typo_str))):\n",
      "        if clean_str[i] != typo_str[i]:\n",
      "            typo_pos = i\n",
      "            break\n",
      "    \n",
      "    if typo_pos is None:\n",
      "        continue\n",
      "    \n",
      "    # Patch MLP0 at typo position\n",
      "    hook_fn = partial(patch_mlp_output, clean_cache=clean_cache_ex, pos=typo_pos)\n",
      "    \n",
      "    patched_logits = model.run_with_hooks(\n",
      "        typo_tok\n",
      "Output: ['Testing MLP0 importance across multiple examples...\\n', '\\n', 'MLP0 recovery: -0.278 | Prob ratio: 1.072 | The beautiful is beautiful today...\\n']\n",
      "Output: ['MLP0 recovery: 1.022 | Prob ratio: 1.126 | The beautiful is beautiful today...\\n', 'MLP0 recovery: 3.559 | Prob ratio: 0.947 | The beautiful is beautiful today...\\n']\n",
      "================================================================================\n",
      "Cell 38 (code):\n",
      "# Average MLP0 recovery of 89% confirms this is a robust finding!\n",
      "# Now let's probe for \"clean word\" representations to understand what MLP0 is doing\n",
      "\n",
      "# Create a probing experiment:\n",
      "# Can we linearly decode the \"intended word\" from activations at different layers?\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "import numpy as np\n",
      "\n",
      "# Build a dataset: typo representations -> clean word identity\n",
      "# We'll use a subset of common words and their typo variants\n",
      "\n",
      "probe_words = ['beautiful', 'explain', 'quick', 'capital', 'France', 'love', 'apple', 'weather', 'read', 'dog']\n",
      "\n",
      "# Generate multiple typo variants for each word\n",
      "probe_data = []\n",
      "for word in probe_words:\n",
      "    # Get clean token representation\n",
      "    clean_tokens = model.to_tokens(\" \" + word, prepend_bos=True)\n",
      "    _, clean_cache = model.run_with_cache(clean_tokens)\n",
      "    \n",
      "    # Get activations at word position (position 1, after BOS)\n",
      "    clean_acts = {layer: clean_cache[f'blocks.{layer}.hook_resid_post'][0, 1].cpu().numpy() \n",
      "                  for layer in range(model.cfg.n_layers)}\n",
      "    \n",
      "    probe_data.append({\n",
      "        'word': word,\n",
      "        'type': 'clean',\n",
      "        'activations': clean_acts\n",
      "    })\n",
      "    \n",
      "    # Generate typo variants\n",
      "    for typo_fn_name, typo_fn in typo_functions.items():\n",
      "        typo_word = typo_fn(word)\n",
      "        if typo_word != word:\n",
      "            typo_tokens = model.to_tokens(\" \" + typo_word, prepend_bos=True)\n",
      "            _, typo_cache = model.run_with_cache(typo_to\n",
      "Output: ['Created probe dataset with 50 examples\\n', \"Words: ['beautiful', 'explain', 'quick', 'capital', 'France', 'love', 'apple', 'weather', 'read', 'dog']\\n\", 'Example entries:\\n', '  Word: beautiful, Type: clean\\n', '  Word: beautiful, Type: substitution\\n', '  Word: beautiful, Type: transposition\\n']\n",
      "================================================================================\n",
      "Cell 41 (code):\n",
      "# Interesting pattern: similarity is HIGH at layer 0, DROPS in middle layers, then RECOVERS at layer 23\n",
      "# This matches our earlier observation about distributed processing\n",
      "\n",
      "# Let's visualize this U-shaped pattern\n",
      "avg_sim_by_layer = []\n",
      "for layer in range(model.cfg.n_layers):\n",
      "    sims = []\n",
      "    for word in probe_words:\n",
      "        clean_entry = [e for e in probe_data if e['word'] == word and e['type'] == 'clean'][0]\n",
      "        typo_entries = [e for e in probe_data if e['word'] == word and e['type'] != 'clean']\n",
      "        \n",
      "        clean_act = torch.tensor(clean_entry['activations'][layer])\n",
      "        \n",
      "        for te in typo_entries:\n",
      "            typo_act = torch.tensor(te['activations'][layer])\n",
      "            sim = F.cosine_similarity(clean_act.unsqueeze(0), typo_act.unsqueeze(0)).item()\n",
      "            sims.append(sim)\n",
      "    \n",
      "    avg_sim_by_layer.append(np.mean(sims))\n",
      "\n",
      "plt.figure(figsize=(12, 5))\n",
      "plt.plot(range(model.cfg.n_layers), avg_sim_by_layer, 'b-o', markersize=5)\n",
      "plt.xlabel('Layer')\n",
      "plt.ylabel('Average Cosine Similarity')\n",
      "plt.title('Typo vs Clean Word Representation Similarity by Layer\\n(Isolated words without context)')\n",
      "plt.grid(True, alpha=0.3)\n",
      "plt.axhline(avg_sim_by_layer[0], color='gray', linestyle='--', alpha=0.5, label=f'Layer 0: {avg_sim_by_layer[0]:.3f}')\n",
      "plt.axhline(avg_sim_by_layer[-1], color='green', linestyle='--', alpha=0.5, label=f'Layer 23: {avg_sim_by_layer[-1]:.3f}')\n",
      "min_layer = np.argmin(avg_sim_by_layer)\n",
      "plt.axhline(avg_sim_by_layer[min_layer], color='red', linestyle='--', al\n",
      "Output: ['<Figure size 1200x500 with 1 Axes>']\n",
      "Output: ['\\n', 'Pattern: Layer 0: 0.899 -> Layer 16: 0.758 -> Layer 23: 0.923\\n']\n",
      "================================================================================\n",
      "Cell 42 (markdown):\n",
      "## U-Shaped Similarity Pattern\n",
      "\n",
      "The representation similarity between typoed and clean words follows a U-shaped curve:\n",
      "- **Layer 0**: High similarity (0.90) - initial embedding + MLP0 creates similar representations\n",
      "- **Middle layers (16)**: Lowest similarity (0.76) - divergent processing\n",
      "- **Layer 23**: High similarity again (0.92) - convergence for output\n",
      "\n",
      "This pattern suggests a **three-phase process**:\n",
      "1. **Early normalization** (Layers 0-3): MLP0 creates representations that partially preserve word identity\n",
      "2. **Divergent processing** (Layers 4-18): Different subword tokens lead to different intermediate computations\n",
      "3. **Late convergence** (Layers 19-23): Representations converge toward similar output distributions\n",
      "\n",
      "The fact that similarity RECOVERS in late layers explains why the model can often produce correct outputs despite typos - the output representations are more similar than the intermediate representations.\n",
      "================================================================================\n",
      "Cell 47 (markdown):\n",
      "# Summary of Typo Correction Mechanism Analysis\n",
      "\n",
      "## Key Findings\n",
      "\n",
      "1. **MLP Layer 0 is the primary component** for typo robustness (~58% recovery)\n",
      "2. **Attention heads contribute minimally** (<1% combined)\n",
      "3. **U-shaped similarity pattern**: High similarity at input, low in middle, high at output\n",
      "4. **Position-specific effect**: Only patching the typo position helps\n",
      "\n",
      "## Mechanism\n",
      "\n",
      "The model uses a **two-stage process**:\n",
      "1. **MLP0 Normalization**: Maps unusual subword sequences to overlapping semantic representations\n",
      "2. **Distributed Compensation**: Context from surrounding tokens helps recover in later layers\n",
      "\n",
      "## Circuit\n",
      "\n",
      "Minimal circuit: `{input, m0}` at the typo position\n"
     ]
    }
   ],
   "source": [
    "# Read specific key cells to understand the results\n",
    "def read_cell(nb, idx):\n",
    "    cell = nb['cells'][idx]\n",
    "    return {\n",
    "        'type': cell['cell_type'],\n",
    "        'source': ''.join(cell['source']),\n",
    "        'outputs': cell.get('outputs', [])\n",
    "    }\n",
    "\n",
    "# Let's read the key finding cells: Cell 35 (MLP0 finding), Cell 36-37 (verification)\n",
    "key_cells = [35, 36, 37, 38, 41, 42, 47]\n",
    "\n",
    "for idx in key_cells:\n",
    "    cell = read_cell(nb, idx)\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Cell {idx} ({cell['type']}):\")\n",
    "    print(cell['source'][:1500])\n",
    "    if cell['outputs']:\n",
    "        for out in cell['outputs'][:2]:\n",
    "            if 'text' in out:\n",
    "                print(f\"Output: {out['text'][:500]}\")\n",
    "            elif 'data' in out:\n",
    "                for k, v in out['data'].items():\n",
    "                    if k == 'text/plain':\n",
    "                        print(f\"Output: {v[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66fdebf",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cell 27 (code):\n",
      "# Plot recovery by layer\n",
      "recoveries = [r['recovery'] for r in patch_results]\n",
      "\n",
      "plt.figure(figsize=(12, 5))\n",
      "plt.bar(layers, recoveries, alpha=0.7, color='steelblue')\n",
      "plt.axhline(0, color='r', linestyle='--', label='Typo baseline')\n",
      "plt.axhline(1, color='g', linestyle='--', label='Clean baseline')\n",
      "plt.xlabel('Layer')\n",
      "plt.ylabel('Recovery (0=typo, 1=clean)')\n",
      "plt.title('Recovery of Clean Behavior by Patching Residual Stream at Each Layer\\n(Patching all positions)')\n",
      "plt.legend()\n",
      "plt.grid(True, alpha=0.3)\n",
      "plt.ylim(-0.2, 1.5)\n",
      "plt.savefig('notebooks/patching_by_layer.png', dpi=150)\n",
      "plt.show()\n",
      "\n",
      "# Find best layer\n",
      "best_layer = max(patch_results, key=lambda x: x['recovery'])\n",
      "worst_layer = min(patch_results, key=lambda x: x['recovery'])\n",
      "print(f\"\\nBest layer for recovery: Layer {best_layer['layer']}\")\n",
      "print(f\"  Recovery: {best_layer['recovery']:.4f}\")\n",
      "\n",
      "print(f\"\\nWorst layer for recovery: Layer {worst_layer['layer']}\")\n",
      "print(f\"  Recovery: {worst_layer['recovery']:.4f}\")\n",
      "Output: ['<Figure size 1200x500 with 1 Axes>']\n",
      "Output: ['\\n', 'Best layer for recovery: Layer 2\\n', '  Recovery: 1.0815\\n', '\\n', 'Worst layer for recovery: Layer 20\\n', '  Recovery: -0.0029\\n']\n",
      "================================================================================\n",
      "Cell 28 (code):\n",
      "# Interesting! Early layers (0-4) give excellent recovery, late layers don't help\n",
      "# This suggests typo damage happens early and propagates forward\n",
      "\n",
      "# Let's do position-specific patching to understand WHERE the corruption matters\n",
      "# Focus on the typo position vs other positions\n",
      "\n",
      "# Token positions:\n",
      "# Clean: ['<|endoftext|>', 'I', ' need', ' to', ' explain', ' this', ' problem']\n",
      "# Typo:  ['<|endoftext|>', 'I', ' need', ' to', ' expl', 'ani', ' this', ' problem']\n",
      "\n",
      "# The key difference is at positions 4-5 (clean: ' explain', typo: ' expl', 'ani')\n",
      "# Let's patch at the LAST position only (where prediction happens)\n",
      "\n",
      "print(\"Position-specific patching at Layer 2 (best layer):\\n\")\n",
      "\n",
      "# Patch at each position individually\n",
      "position_patch_results = []\n",
      "\n",
      "for pos in range(min(clean_tokens.shape[1], typo_tokens.shape[1])):\n",
      "    hook_name = 'blocks.2.hook_resid_post'\n",
      "    hook_fn = partial(patch_residual_hook, clean_cache=clean_cache, pos=pos)\n",
      "    \n",
      "    patched_logits = model.run_with_hooks(\n",
      "        typo_tokens,\n",
      "        fwd_hooks=[(hook_name, hook_fn)]\n",
      "    )\n",
      "    \n",
      "    patched_prob_to = F.softmax(patched_logits[0, -1], dim=-1)[to_token].item()\n",
      "    recovery = (patched_prob_to - typo_prob_to) / (clean_prob_to - typo_prob_to + 1e-10)\n",
      "    \n",
      "    token_str = model.to_string(clean_tokens[0, pos]) if pos < clean_tokens.shape[1] else \"N/A\"\n",
      "    position_patch_results.append({\n",
      "        'pos': pos,\n",
      "        'token': token_str,\n",
      "        'patched_prob': patched_prob_to,\n",
      "        'recovery': recovery\n",
      "    })\n",
      "    \n",
      "    print\n",
      "Output: ['Position-specific patching at Layer 2 (best layer):\\n', '\\n', \"Pos 0 ('<|endoftext|>'): prob=0.0206, recovery=0.0000\\n\", \"Pos 1 ('I'): prob=0.0206, recovery=0.0000\\n\", \"Pos 2 (' need'): prob=0.0206, recovery=-0.0000\\n\", \"Pos 3 (' to'): prob=0.0206, recovery=0.0000\\n\"]\n",
      "Output: [\"Pos 4 (' explain'): prob=0.1160, recovery=0.6310\\n\", \"Pos 5 (' this'): prob=0.0216, recovery=0.0068\\n\", \"Pos 6 (' problem'): prob=0.0240, recovery=0.0228\\n\", '\\n', '\\n', 'Patching typo region (positions 4-5 in typo) at Layer 2:\\n', '  Recovery: 0.6310\\n']\n",
      "================================================================================\n",
      "Cell 29 (code):\n",
      "# KEY FINDING: Position 4 (the typo position) is critical!\n",
      "# Patching just position 4 gives 63% recovery, while other positions give near 0%\n",
      "\n",
      "# Let's visualize this as a heatmap: layer x position\n",
      "print(\"Building layer x position patching heatmap...\")\n",
      "\n",
      "layer_pos_recovery = np.zeros((model.cfg.n_layers, min(clean_tokens.shape[1], typo_tokens.shape[1])))\n",
      "\n",
      "for layer in range(model.cfg.n_layers):\n",
      "    for pos in range(min(clean_tokens.shape[1], typo_tokens.shape[1])):\n",
      "        hook_name = f'blocks.{layer}.hook_resid_post'\n",
      "        hook_fn = partial(patch_residual_hook, clean_cache=clean_cache, pos=pos)\n",
      "        \n",
      "        patched_logits = model.run_with_hooks(\n",
      "            typo_tokens,\n",
      "            fwd_hooks=[(hook_name, hook_fn)]\n",
      "        )\n",
      "        \n",
      "        patched_prob_to = F.softmax(patched_logits[0, -1], dim=-1)[to_token].item()\n",
      "        recovery = (patched_prob_to - typo_prob_to) / (clean_prob_to - typo_prob_to + 1e-10)\n",
      "        layer_pos_recovery[layer, pos] = recovery\n",
      "\n",
      "print(\"Done!\")\n",
      "Output: ['Building layer x position patching heatmap...\\n']\n",
      "Output: ['Done!\\n']\n",
      "================================================================================\n",
      "Cell 30 (code):\n",
      "# Visualize the layer x position heatmap\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "\n",
      "# Get token labels\n",
      "token_labels = [model.to_string(clean_tokens[0, i]) for i in range(clean_tokens.shape[1])]\n",
      "\n",
      "im = ax.imshow(layer_pos_recovery, aspect='auto', cmap='RdYlGn', vmin=-0.2, vmax=1.0)\n",
      "ax.set_xlabel('Position (Token)')\n",
      "ax.set_ylabel('Layer')\n",
      "ax.set_title('Recovery from Patching Clean Activation into Typo Run\\n(Higher = More Recovery)')\n",
      "\n",
      "# Set x-tick labels\n",
      "ax.set_xticks(range(len(token_labels)))\n",
      "ax.set_xticklabels([f\"{i}:{t}\" for i, t in enumerate(token_labels)], rotation=45, ha='right')\n",
      "\n",
      "# Add colorbar\n",
      "cbar = plt.colorbar(im, ax=ax)\n",
      "cbar.set_label('Recovery (0=typo, 1=clean)')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('notebooks/layer_position_patching.png', dpi=150)\n",
      "plt.show()\n",
      "\n",
      "# Find the best (layer, position) combinations\n",
      "print(\"\\nTop 5 (layer, position) combinations for recovery:\")\n",
      "flat_indices = np.argsort(layer_pos_recovery.flatten())[::-1][:5]\n",
      "for idx in flat_indices:\n",
      "    layer = idx // layer_pos_recovery.shape[1]\n",
      "    pos = idx % layer_pos_recovery.shape[1]\n",
      "    token = model.to_string(clean_tokens[0, pos])\n",
      "    print(f\"  Layer {layer}, Pos {pos} ('{token}'): {layer_pos_recovery[layer, pos]:.4f}\")\n",
      "Output: ['<Figure size 1000x1000 with 2 Axes>']\n",
      "Output: ['\\n', 'Top 5 (layer, position) combinations for recovery:\\n', \"  Layer 3, Pos 4 (' explain'): 0.7066\\n\", \"  Layer 4, Pos 4 (' explain'): 0.6948\\n\", \"  Layer 2, Pos 4 (' explain'): 0.6310\\n\", \"  Layer 6, Pos 4 (' explain'): 0.5872\\n\", \"  Layer 0, Pos 4 (' explain'): 0.5845\\n\"]\n",
      "================================================================================\n",
      "Cell 32 (code):\n",
      "# Now let's look at attention heads and MLPs specifically\n",
      "# Which components at the critical position (4) are most important?\n",
      "\n",
      "# Attention head patching at the typo position\n",
      "print(\"Attention head patching at position 4 (typo word)...\\n\")\n",
      "\n",
      "head_recovery = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
      "\n",
      "def patch_attention_output(activation, hook, clean_cache, pos, head):\n",
      "    \"\"\"Patch a specific attention head's output at a position\"\"\"\n",
      "    # activation shape: [batch, seq, n_heads, d_head]\n",
      "    if pos < activation.shape[1] and pos < clean_cache[hook.name].shape[1]:\n",
      "        activation[:, pos, head, :] = clean_cache[hook.name][:, pos, head, :]\n",
      "    return activation\n",
      "\n",
      "for layer in range(model.cfg.n_layers):\n",
      "    for head in range(model.cfg.n_heads):\n",
      "        hook_name = f'blocks.{layer}.attn.hook_z'  # Attention output before projection\n",
      "        hook_fn = partial(patch_attention_output, clean_cache=clean_cache, pos=4, head=head)\n",
      "        \n",
      "        patched_logits = model.run_with_hooks(\n",
      "            typo_tokens,\n",
      "            fwd_hooks=[(hook_name, hook_fn)]\n",
      "        )\n",
      "        \n",
      "        patched_prob_to = F.softmax(patched_logits[0, -1], dim=-1)[to_token].item()\n",
      "        recovery = (patched_prob_to - typo_prob_to) / (clean_prob_to - typo_prob_to + 1e-10)\n",
      "        head_recovery[layer, head] = recovery\n",
      "\n",
      "print(\"Done! Finding top heads...\")\n",
      "Output: ['Attention head patching at position 4 (typo word)...\\n', '\\n']\n",
      "Output: ['Done! Finding top heads...\\n']\n",
      "================================================================================\n",
      "Cell 33 (code):\n",
      "# Visualize attention head importance\n",
      "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
      "\n",
      "# Heatmap\n",
      "ax1 = axes[0]\n",
      "im = ax1.imshow(head_recovery, aspect='auto', cmap='RdYlGn', vmin=-0.1, vmax=0.3)\n",
      "ax1.set_xlabel('Head')\n",
      "ax1.set_ylabel('Layer')\n",
      "ax1.set_title('Recovery from Patching Individual Attention Heads\\n(Position 4)')\n",
      "cbar = plt.colorbar(im, ax=ax1)\n",
      "cbar.set_label('Recovery')\n",
      "\n",
      "# Top heads bar chart\n",
      "ax2 = axes[1]\n",
      "flat_indices = np.argsort(head_recovery.flatten())[::-1][:15]\n",
      "top_heads = []\n",
      "for idx in flat_indices:\n",
      "    layer = idx // model.cfg.n_heads\n",
      "    head = idx % model.cfg.n_heads\n",
      "    top_heads.append((layer, head, head_recovery[layer, head]))\n",
      "\n",
      "labels = [f'L{l}H{h}' for l, h, _ in top_heads]\n",
      "values = [v for _, _, v in top_heads]\n",
      "\n",
      "ax2.barh(range(len(labels)), values, color='steelblue')\n",
      "ax2.set_yticks(range(len(labels)))\n",
      "ax2.set_yticklabels(labels)\n",
      "ax2.set_xlabel('Recovery')\n",
      "ax2.set_title('Top 15 Attention Heads for Typo Recovery')\n",
      "ax2.invert_yaxis()\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('notebooks/attention_head_patching.png', dpi=150)\n",
      "plt.show()\n",
      "\n",
      "print(\"\\nTop 10 attention heads:\")\n",
      "for l, h, r in top_heads[:10]:\n",
      "    print(f\"  a{l}.h{h}: {r:.4f}\")\n",
      "Output: ['<Figure size 1600x600 with 3 Axes>']\n",
      "Output: ['\\n', 'Top 10 attention heads:\\n', '  a0.h1: 0.0035\\n', '  a0.h10: 0.0029\\n', '  a1.h9: 0.0026\\n', '  a0.h6: 0.0023\\n', '  a2.h3: 0.0016\\n', '  a4.h3: 0.0015\\n', '  a0.h9: 0.0014\\n', '  a2.h8: 0.0013\\n', '  a0.h0: 0.0013\\n', '  a5.h13: 0.0012\\n']\n",
      "================================================================================\n",
      "Cell 34 (code):\n",
      "# Individual heads show very small effects! This suggests the effect is distributed\n",
      "# Let's check MLP layers instead\n",
      "\n",
      "print(\"MLP patching at position 4 (typo word)...\\n\")\n",
      "\n",
      "mlp_recovery = []\n",
      "\n",
      "def patch_mlp_output(activation, hook, clean_cache, pos):\n",
      "    \"\"\"Patch MLP output at a specific position\"\"\"\n",
      "    if pos < activation.shape[1] and pos < clean_cache[hook.name].shape[1]:\n",
      "        activation[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
      "    return activation\n",
      "\n",
      "for layer in range(model.cfg.n_layers):\n",
      "    hook_name = f'blocks.{layer}.hook_mlp_out'\n",
      "    hook_fn = partial(patch_mlp_output, clean_cache=clean_cache, pos=4)\n",
      "    \n",
      "    patched_logits = model.run_with_hooks(\n",
      "        typo_tokens,\n",
      "        fwd_hooks=[(hook_name, hook_fn)]\n",
      "    )\n",
      "    \n",
      "    patched_prob_to = F.softmax(patched_logits[0, -1], dim=-1)[to_token].item()\n",
      "    recovery = (patched_prob_to - typo_prob_to) / (clean_prob_to - typo_prob_to + 1e-10)\n",
      "    mlp_recovery.append(recovery)\n",
      "    \n",
      "    if layer % 4 == 0:\n",
      "        print(f\"Layer {layer}: {recovery:.4f}\")\n",
      "\n",
      "print(\"\\nMLP recovery vs Residual stream recovery comparison:\")\n",
      "print(f\"Sum of individual attention heads: {np.sum(head_recovery):.4f}\")\n",
      "print(f\"Sum of MLP recoveries: {np.sum(mlp_recovery):.4f}\")\n",
      "print(f\"Best residual stream (layer 3, pos 4): {layer_pos_recovery[3, 4]:.4f}\")\n",
      "Output: ['MLP patching at position 4 (typo word)...\\n', '\\n', 'Layer 0: 0.5804\\n']\n",
      "Output: ['Layer 4: 0.0201\\n', 'Layer 8: 0.0032\\n']\n"
     ]
    }
   ],
   "source": [
    "# Read more results cells\n",
    "key_cells2 = [27, 28, 29, 30, 32, 33, 34]\n",
    "\n",
    "for idx in key_cells2:\n",
    "    cell = read_cell(nb, idx)\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Cell {idx} ({cell['type']}):\")\n",
    "    print(cell['source'][:1500])\n",
    "    if cell['outputs']:\n",
    "        for out in cell['outputs'][:2]:\n",
    "            if 'text' in out:\n",
    "                print(f\"Output: {out['text'][:500]}\")\n",
    "            elif 'data' in out:\n",
    "                for k, v in out['data'].items():\n",
    "                    if k == 'text/plain':\n",
    "                        print(f\"Output: {v[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7763ea2d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results JSON keys: ['task', 'model', 'description', 'prompt_example', 'clean_prompt_example', 'nodes', 'node_details', 'additional_components', 'findings_summary', 'interpretation']\n",
      "\n",
      "{\n",
      "  \"task\": \"typo_correction\",\n",
      "  \"model\": \"gpt2-medium\",\n",
      "  \"description\": \"Circuit components responsible for typo robustness - mapping typoed tokens to intended meaning representations\",\n",
      "  \"prompt_example\": \"I need to explani this problem\",\n",
      "  \"clean_prompt_example\": \"I need to explain this problem\",\n",
      "  \"nodes\": [\n",
      "    \"input\",\n",
      "    \"m0\"\n",
      "  ],\n",
      "  \"node_details\": {\n",
      "    \"input\": {\n",
      "      \"description\": \"Input embeddings of typoed tokens\",\n",
      "      \"role\": \"Provides initial subword token representations\"\n",
      "    },\n",
      "    \"m0\": {\n",
      "      \"description\": \"MLP Layer 0\",\n",
      "      \"role\": \"Primary component for typo robustness. Acts as token normalizer mapping unusual subword sequences to representations that overlap with clean versions.\",\n",
      "      \"recovery_contribution\": 0.58,\n",
      "      \"key_neurons\": [\n",
      "        2527,\n",
      "        2066,\n",
      "        3244,\n",
      "        3495,\n",
      "        3897\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"additional_components\": {\n",
      "    \"description\": \"Distributed processing across layers 1-23 provides additional ~40% recovery\",\n",
      "    \"mechanism\": \"Context integration from surrounding tokens and late-layer convergence toward similar output distributions\"\n",
      "  },\n",
      "  \"findings_summary\": {\n",
      "    \"mlp0_recovery\": 0.58,\n",
      "    \"attention_recovery\": 0.01,\n",
      "    \"distributed_recovery\": 0.4,\n",
      "    \"u_shaped_similarity\": {\n",
      "      \"layer_0\": 0.899,\n",
      "      \"layer_16_min\": 0.758,\n",
      "      \"layer_23\": 0.923\n",
      "    }\n",
      "  },\n",
      "  \"interpretation\": \"The model does not explicitly correct typos. Instead, MLP0 creates overlapping representations for similar-looking tokens, and late layers converge toward similar outputs despite middle-layer divergence. This is a robustness mechanism, not an explicit correction circuit.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's check the results file and also read more cells for verification\n",
    "results_path = '/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13/results/real_circuits_1.json'\n",
    "with open(results_path, 'r') as f:\n",
    "    results_json = json.load(f)\n",
    "    \n",
    "print(\"Results JSON keys:\", list(results_json.keys()))\n",
    "print()\n",
    "print(json.dumps(results_json, indent=2)[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84bff131",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cell 10 (code):\n",
      "# Create a systematic typo dataset\n",
      "# We'll categorize by typo type and tokenization impact\n",
      "\n",
      "import random\n",
      "from typing import List, Dict, Tuple\n",
      "\n",
      "def create_typo_substitution(word: str, idx: int = None) -> str:\n",
      "    \"\"\"Create a typo by substituting a character\"\"\"\n",
      "    if len(word) < 2:\n",
      "        return word\n",
      "    if idx is None:\n",
      "        idx = random.randint(0, len(word) - 1)\n",
      "    # Simple substitution with nearby keyboard letters\n",
      "    keyboard_neighbors = {\n",
      "        'a': 'sq', 'b': 'vn', 'c': 'xv', 'd': 'sf', 'e': 'wr',\n",
      "        'f': 'dg', 'g': 'fh', 'h': 'gj', 'i': 'uo', 'j': 'hk',\n",
      "        'k': 'jl', 'l': 'ko', 'm': 'n', 'n': 'bm', 'o': 'ip',\n",
      "        'p': 'ol', 'q': 'wa', 'r': 'et', 's': 'ad', 't': 'ry',\n",
      "        'u': 'yi', 'v': 'cb', 'w': 'qe', 'x': 'zc', 'y': 'tu', 'z': 'x'\n",
      "    }\n",
      "    char = word[idx].lower()\n",
      "    if char in keyboard_neighbors:\n",
      "        new_char = random.choice(keyboard_neighbors[char])\n",
      "        if word[idx].isupper():\n",
      "            new_char = new_char.upper()\n",
      "        return word[:idx] + new_char + word[idx+1:]\n",
      "    return word\n",
      "\n",
      "def create_typo_transposition(word: str, idx: int = None) -> str:\n",
      "    \"\"\"Create a typo by swapping adjacent characters\"\"\"\n",
      "    if len(word) < 2:\n",
      "        return word\n",
      "    if idx is None:\n",
      "        idx = random.randint(0, len(word) - 2)\n",
      "    return word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
      "\n",
      "def create_typo_deletion(word: str, idx: int = None) -> str:\n",
      "    \"\"\"Create a typo by deleting a character\"\"\"\n",
      "    if len(word) < 3:\n",
      "        return word\n",
      "    if id\n",
      "Output: ['Created dataset with 39 examples\\n', '\\n', 'Sample entries:\\n', '\\n', '1. Type: substitution\\n', '   Clean: The beautiful is beautiful today\\n', '   Typo:  The bwautiful is beautiful today\\n', \"   Word: 'beautiful' -> 'bwautiful'\\n\", '\\n', '2. Type: transposition\\n', '   Clean: The beautiful is beautiful today\\n', '   Typo:  The beauitful is beautiful today\\n', \"   Word: 'beautiful' -> 'beauitful'\\n\", '\\n', '3. Type: deletion\\n', '   Clean: The beautiful is beautiful today\\n', '   Typo:  The beutiful is beautiful today\\n', \"   Word: 'beautiful' -> 'beutiful'\\n\", '\\n', '4. Type: insertion\\n', '   Clean: The beautiful is beautiful today\\n', '   Typo:  The beauutiful is beautiful today\\n', \"   Word: 'beautiful' -> 'beauutiful'\\n\", '\\n', '5. Type: substitution\\n', '   Clean: I need to explain this problem\\n', '   Typo:  I need to ezplain this problem\\n', \"   Word: 'explain' -> 'ezplain'\\n\", '\\n', '6. Type: transposition\\n', '   Clean: I need to explain this problem\\n', '   Typo:  I need to explani this problem\\n', \"   Word: 'explain' -> 'explani'\\n\", '\\n', '7. Type: deletion\\n', '   Clean: I need to explain this problem\\n', '   Typo:  I need to explan this problem\\n', \"   Word: 'explain' -> 'explan'\\n\", '\\n', '8. Type: insertion\\n', '   Clean: I need to explain this problem\\n', '   Typo:  I need to eexplain this problem\\n', \"   Word: 'explain' -> 'eexplain'\\n\"]\n",
      "================================================================================\n",
      "Cell 11 (code):\n",
      "# Evaluate model performance on clean vs typo sentences\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "results = []\n",
      "\n",
      "for entry in dataset:\n",
      "    clean_tokens = model.to_tokens(entry[\"clean\"])\n",
      "    typo_tokens = model.to_tokens(entry[\"typo\"])\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        clean_logits = model(clean_tokens)\n",
      "        typo_logits = model(typo_tokens)\n",
      "    \n",
      "    # Get the expected token\n",
      "    expected_token = model.to_tokens(entry[\"expected_next\"])[0, 1]\n",
      "    \n",
      "    # Get probabilities\n",
      "    clean_probs = F.softmax(clean_logits[0, -1], dim=-1)\n",
      "    typo_probs = F.softmax(typo_logits[0, -1], dim=-1)\n",
      "    \n",
      "    clean_prob = clean_probs[expected_token].item()\n",
      "    typo_prob = typo_probs[expected_token].item()\n",
      "    \n",
      "    # Get top prediction\n",
      "    clean_top = model.to_string(clean_logits[0, -1].argmax())\n",
      "    typo_top = model.to_string(typo_logits[0, -1].argmax())\n",
      "    \n",
      "    # Check tokenization difference\n",
      "    clean_word_tokens = model.to_tokens(entry[\"target_word\"], prepend_bos=False)\n",
      "    typo_word_tokens = model.to_tokens(entry[\"typo_word\"], prepend_bos=False)\n",
      "    token_count_diff = typo_word_tokens.shape[1] - clean_word_tokens.shape[1]\n",
      "    \n",
      "    results.append({\n",
      "        **entry,\n",
      "        \"clean_prob\": clean_prob,\n",
      "        \"typo_prob\": typo_prob,\n",
      "        \"prob_ratio\": typo_prob / max(clean_prob, 1e-8),\n",
      "        \"clean_top\": clean_top,\n",
      "        \"typo_top\": typo_top,\n",
      "        \"same_top\": clean_top == typo_top,\n",
      "        \"token_count_diff\": token_count_diff,\n",
      "        \"clean_token_count\": clean_word_token\n",
      "Output: ['Performance by Typo Type:\\n', '\\n', 'Substitution:\\n', '  Avg prob ratio (typo/clean): 0.868\\n', '  Same top prediction: 60.0%\\n', '  Avg token count diff: 1.30\\n', '\\n', 'Transposition:\\n', '  Avg prob ratio (typo/clean): 0.789\\n', '  Same top prediction: 55.6%\\n', '  Avg token count diff: 1.22\\n', '\\n', 'Deletion:\\n', '  Avg prob ratio (typo/clean): 0.900\\n', '  Same top prediction: 60.0%\\n', '  Avg token count diff: 0.80\\n', '\\n', 'Insertion:\\n', '  Avg prob ratio (typo/clean): 0.902\\n', '  Same top prediction: 70.0%\\n', '  Avg token count diff: 1.30\\n', '\\n']\n",
      "================================================================================\n",
      "Cell 12 (code):\n",
      "# Let's look at specific cases: best and worst typo handling\n",
      "print(\"Cases with BEST typo handling (highest prob ratio):\\n\")\n",
      "sorted_results = sorted(results, key=lambda x: x[\"prob_ratio\"], reverse=True)\n",
      "for r in sorted_results[:5]:\n",
      "    print(f\"Ratio: {r['prob_ratio']:.3f}\")\n",
      "    print(f\"  Clean: {r['clean']}\")\n",
      "    print(f\"  Typo:  {r['typo']}\")\n",
      "    print(f\"  Type: {r['typo_type']}, Token diff: {r['token_count_diff']}\")\n",
      "    print(f\"  Clean pred: '{r['clean_top']}', Typo pred: '{r['typo_top']}'\")\n",
      "    print()\n",
      "\n",
      "print(\"\\n\" + \"=\"*60)\n",
      "print(\"\\nCases with WORST typo handling (lowest prob ratio):\\n\")\n",
      "for r in sorted_results[-5:]:\n",
      "    print(f\"Ratio: {r['prob_ratio']:.3f}\")\n",
      "    print(f\"  Clean: {r['clean']}\")\n",
      "    print(f\"  Typo:  {r['typo']}\")\n",
      "    print(f\"  Type: {r['typo_type']}, Token diff: {r['token_count_diff']}\")\n",
      "    print(f\"  Clean pred: '{r['clean_top']}', Typo pred: '{r['typo_top']}'\")\n",
      "    print()\n",
      "Output: ['Cases with BEST typo handling (highest prob ratio):\\n', '\\n', 'Ratio: 2.243\\n', '  Clean: The dog jumped over the fence\\n', '  Typo:  The dg jumped over the fence\\n', '  Type: deletion, Token diff: 1\\n', \"  Clean pred: ' and', Typo pred: ' and'\\n\", '\\n', 'Ratio: 1.605\\n', '  Clean: The dog jumped over the fence\\n', '  Typo:  The dgo jumped over the fence\\n', '  Type: transposition, Token diff: 1\\n', \"  Clean pred: ' and', Typo pred: ' and'\\n\", '\\n', 'Ratio: 1.585\\n', '  Clean: The dog jumped over the fence\\n', '  Typo:  The dig jumped over the fence\\n', '  Type: substitution, Token diff: 0\\n', \"  Clean pred: ' and', Typo pred: ' and'\\n\", '\\n', 'Ratio: 1.259\\n', '  Clean: Please read the instructions carefully\\n', '  Typo:  Please rread the instructions carefully\\n', '  Type: insertion, Token diff: 1\\n', \"  Clean pred: ' before', Typo pred: '.'\\n\", '\\n', 'Ratio: 1.173\\n', '  Clean: The beautiful is beautiful today\\n', '  Typo:  The beauutiful is beautiful today\\n', '  Type: insertion, Token diff: 2\\n', \"  Clean pred: '.', Typo pred: ','\\n\", '\\n', '\\n', '============================================================\\n', '\\n', 'Cases with WORST typo handling (lowest prob ratio):\\n', '\\n', 'Ratio: 0.481\\n', '  Clean: The weather was perfect for hiking\\n', '  Typo:  The weaather was perfect for hiking\\n', '  Type: insertion, Token diff: 2\\n', \"  Clean pred: '.', Typo pred: ' in'\\n\", '\\n', 'Ratio: 0.371\\n', '  Clean: I need to explain this problem\\n', '  Typo:  I need to ezplain this problem\\n', '  Type: substitution, Token diff: 0\\n', \"  Clean pred: '.', Typo pred: '.'\\n\", '\\n', 'Ratio: 0.365\\n', '  Clean: The capital of the country is Paris\\n', '  Typo:  The capiatl of the country is Paris\\n', '  Type: transposition, Token diff: 2\\n', \"  Clean pred: ',', Typo pred: ','\\n\", '\\n', 'Ratio: 0.360\\n', '  Clean: I need to explain this problem\\n', '  Typo:  I need to explan this problem\\n', '  Type: deletion, Token diff: 0\\n', \"  Clean pred: '.', Typo pred: '.'\\n\", '\\n', 'Ratio: 0.120\\n', '  Clean: I need to explain this problem\\n', '  Typo:  I need to explani this problem\\n', '  Type: transposition, Token diff: 0\\n', \"  Clean pred: '.', Typo pred: '.'\\n\", '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Read more cells to understand the MLP0 verification across examples\n",
    "# Cell 37 was truncated, let's check more outputs and behavioral results\n",
    "\n",
    "# Check behavioral results (Cell 11 and 12)\n",
    "behavioral_cells = [10, 11, 12]\n",
    "\n",
    "for idx in behavioral_cells:\n",
    "    cell = read_cell(nb, idx)\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Cell {idx} ({cell['type']}):\")\n",
    "    print(cell['source'][:1500])\n",
    "    if cell['outputs']:\n",
    "        for out in cell['outputs'][:3]:\n",
    "            if 'text' in out:\n",
    "                print(f\"Output: {out['text'][:1000]}\")\n",
    "            elif 'data' in out:\n",
    "                for k, v in out['data'].items():\n",
    "                    if k == 'text/plain':\n",
    "                        print(f\"Output: {v[:1000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c4baaa4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Let's verify this finding across multiple examples\n",
      "# Does MLP0 consistently show high importance for typo handling?\n",
      "\n",
      "print(\"Testing MLP0 importance across multiple examples...\\n\")\n",
      "\n",
      "mlp0_importance = []\n",
      "for entry in results[:15]:  # Test on 15 examples\n",
      "    clean_tokens_ex = model.to_tokens(entry['clean'])\n",
      "    typo_tokens_ex = model.to_tokens(entry['typo'])\n",
      "    \n",
      "    # Get clean cache\n",
      "    _, clean_cache_ex = model.run_with_cache(clean_tokens_ex)\n",
      "    \n",
      "    # Baseline\n",
      "    with torch.no_grad():\n",
      "        clean_logits_ex = model(clean_tokens_ex)\n",
      "        typo_logits_ex = model(typo_tokens_ex)\n",
      "    \n",
      "    expected_token_ex = model.to_tokens(entry['expected_next'])[0, 1]\n",
      "    clean_prob_ex = F.softmax(clean_logits_ex[0, -1], dim=-1)[expected_token_ex].item()\n",
      "    typo_prob_ex = F.softmax(typo_logits_ex[0, -1], dim=-1)[expected_token_ex].item()\n",
      "    \n",
      "    # Find the typo position (roughly where tokens differ)\n",
      "    # Simple heuristic: find first position where clean and typo differ\n",
      "    clean_str = [model.to_string(t) for t in clean_tokens_ex[0]]\n",
      "    typo_str = [model.to_string(t) for t in typo_tokens_ex[0]]\n",
      "    \n",
      "    typo_pos = None\n",
      "    for i in range(min(len(clean_str), len(typo_str))):\n",
      "        if clean_str[i] != typo_str[i]:\n",
      "            typo_pos = i\n",
      "            break\n",
      "    \n",
      "    if typo_pos is None:\n",
      "        continue\n",
      "    \n",
      "    # Patch MLP0 at typo position\n",
      "    hook_fn = partial(patch_mlp_output, clean_cache=clean_cache_ex, pos=typo_pos)\n",
      "    \n",
      "    patched_logits = model.run_with_hooks(\n",
      "        typo_tokens_ex,\n",
      "        fwd_hooks=[('blocks.0.hook_mlp_out', hook_fn)]\n",
      "    )\n",
      "    \n",
      "    patched_prob = F.softmax(patched_logits[0, -1], dim=-1)[expected_token_ex].item()\n",
      "    \n",
      "    if abs(clean_prob_ex - typo_prob_ex) > 0.001:  # Avoid division by tiny numbers\n",
      "        recovery = (patched_prob - typo_prob_ex) / (clean_prob_ex - typo_prob_ex)\n",
      "    else:\n",
      "        recovery = 0\n",
      "    \n",
      "    mlp0_importance.append({\n",
      "        'clean': entry['clean'][:40],\n",
      "        'prob_ratio': entry['prob_ratio'],\n",
      "        'mlp0_recovery': recovery\n",
      "    })\n",
      "    \n",
      "    print(f\"MLP0 recovery: {recovery:.3f} | Prob ratio: {entry['prob_ratio']:.3f} | {entry['clean'][:35]}...\")\n",
      "\n",
      "print(f\"\\n\\nAverage MLP0 recovery: {np.mean([x['mlp0_recovery'] for x in mlp0_importance]):.3f}\")\n",
      "\n",
      "Outputs:\n",
      "Testing MLP0 importance across multiple examples...\n",
      "\n",
      "MLP0 recovery: -0.278 | Prob ratio: 1.072 | The beautiful is beautiful today...\n",
      "\n",
      "MLP0 recovery: 1.022 | Prob ratio: 1.126 | The beautiful is beautiful today...\n",
      "MLP0 recovery: 3.559 | Prob ratio: 0.947 | The beautiful is beautiful today...\n",
      "\n",
      "MLP0 recovery: 0.254 | Prob ratio: 1.173 | The beautiful is beautiful today...\n",
      "MLP0 recovery: 0.536 | Prob ratio: 0.371 | I need to explain this problem...\n",
      "\n",
      "MLP0 recovery: 0.580 | Prob ratio: 0.120 | I need to explain this problem...\n",
      "MLP0 recovery: 0.979 | Prob ratio: 0.360 | I need to explain this problem...\n",
      "\n",
      "MLP0 recovery: -0.604 | Prob ratio: 0.734 | I need to explain this problem...\n",
      "MLP0 recovery: 1.172 | Prob ratio: 0.820 | She is very quick at her job...\n",
      "\n",
      "MLP0 recovery: 0.775 | Prob ratio: 0.758 | She is very quick at her job...\n",
      "MLP0 recovery: 1.288 | Prob ratio: 0.869 | She is very quick at her job...\n",
      "\n",
      "MLP0 recovery: 2.354 | Prob ratio: 0.960 | She is very quick at her job...\n",
      "MLP0 recovery: 0.735 | Prob ratio: 0.517 | The capital of the country is Paris...\n",
      "\n",
      "MLP0 recovery: 0.394 | Prob ratio: 0.365 | The capital of the country is Paris...\n",
      "MLP0 recovery: 0.619 | Prob ratio: 0.505 | The capital of the country is Paris...\n",
      "\n",
      "\n",
      "Average MLP0 recovery: 0.892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Cell 37's full output for verification across examples\n",
    "cell37 = read_cell(nb, 37)\n",
    "print(cell37['source'])\n",
    "print(\"\\nOutputs:\")\n",
    "for out in cell37['outputs']:\n",
    "    if 'text' in out:\n",
    "        print(''.join(out['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fa2a4",
   "metadata": {},
   "source": [
    "# Consistency Evaluation of Typo Correction Mechanism Analysis\n",
    "\n",
    "## Repository Path\n",
    "`/net/scratch2/smallyan/open_question/typo_claude_2026-01-05_00-38-13`\n",
    "\n",
    "## Overview\n",
    "This evaluation assesses the consistency of a mechanistic interpretability research project investigating how GPT-2 Medium processes typos. The project aims to identify the circuit/mechanism responsible for typo robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e944468",
   "metadata": {},
   "source": [
    "## Key Files Analyzed\n",
    "\n",
    "1. **Plan Files**: \n",
    "   - `logs/plan_v1.md` - Initial experimental plan\n",
    "   - `logs/plan_v2.md` - Refined plan with updated hypothesis\n",
    "\n",
    "2. **Implementation**: \n",
    "   - `notebooks/2026-01-05-00-38_typo_circuit_analysis.ipynb` - Main experimental notebook\n",
    "\n",
    "3. **Documentation**: \n",
    "   - `logs/documentation.md` - Final research documentation\n",
    "\n",
    "4. **Results**: \n",
    "   - `results/real_circuits_1.json` - Circuit specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72afeee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CS1: Conclusion vs Original Results\n",
    "\n",
    "### Summary of Key Claims in Documentation vs Implementation Results\n",
    "\n",
    "#### Documentation Claims:\n",
    "1. **MLP0 recovery**: ~58%\n",
    "2. **Attention recovery**: <1% combined\n",
    "3. **U-shaped similarity**: Layer 0 (0.899)  Layer 16 (0.758)  Layer 23 (0.923)\n",
    "4. **Behavioral findings**: Same top prediction rate 55-70%\n",
    "\n",
    "#### Implementation Results (from notebook):\n",
    "1. **MLP0 recovery**: 0.5804 (Cell 35)\n",
    "2. **Attention recovery**: 0.0005 (0.05%) combined (Cell 35)\n",
    "3. **U-shaped similarity**: Layer 0 (0.899)  Layer 16 (0.758)  Layer 23 (0.923) (Cell 41)\n",
    "4. **Behavioral findings**: Substitution 60%, Transposition 55.6%, Deletion 60%, Insertion 70% (Cell 11)\n",
    "5. **Average MLP0 recovery across examples**: 0.892 (Cell 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30802b4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CS1: Verifying Documentation vs Implementation Results ===\n",
      "\n",
      "1. MLP Layer 0 Recovery:\n",
      "   Documentation states: ~58%\n",
      "   Implementation result: 0.5804 (58.04%)\n",
      "   MATCH: \n",
      "\n",
      "2. Attention Recovery:\n",
      "   Documentation states: <1% combined\n",
      "   Implementation result: 0.0005 (0.05%)\n",
      "   MATCH: \n",
      "\n",
      "3. U-Shaped Similarity Pattern:\n",
      "   Documentation states: Layer 0 (0.899)  Layer 16 (0.758)  Layer 23 (0.923)\n",
      "   Implementation result: Layer 0 (0.899)  Layer 16 (0.758)  Layer 23 (0.923)\n",
      "   MATCH: \n",
      "\n",
      "4. Behavioral Results (Same top prediction):\n",
      "   Documentation states: 55-70% across typo types\n",
      "   Implementation shows:\n",
      "     - Substitution: 60.0%\n",
      "     - Transposition: 55.6%\n",
      "     - Deletion: 60.0%\n",
      "     - Insertion: 70.0%\n",
      "   MATCH: \n",
      "\n",
      "5. Position-Specific Patching Recovery:\n",
      "   Documentation states: 63-71% at typo position\n",
      "   Implementation shows: Layers 2-4 at Position 4 give 63-71% recovery\n",
      "   MATCH: \n",
      "\n",
      "CS1 ASSESSMENT: PASS - All evaluable conclusions match original results\n"
     ]
    }
   ],
   "source": [
    "# Verify the key numeric results from the notebook directly\n",
    "# Parse the outputs for the main numeric claims\n",
    "\n",
    "# 1. MLP0 recovery\n",
    "print(\"=== CS1: Verifying Documentation vs Implementation Results ===\\n\")\n",
    "\n",
    "print(\"1. MLP Layer 0 Recovery:\")\n",
    "print(\"   Documentation states: ~58%\")\n",
    "print(\"   Implementation result: 0.5804 (58.04%)\")\n",
    "print(\"   MATCH: \")\n",
    "print()\n",
    "\n",
    "print(\"2. Attention Recovery:\")\n",
    "print(\"   Documentation states: <1% combined\")\n",
    "print(\"   Implementation result: 0.0005 (0.05%)\")\n",
    "print(\"   MATCH: \")\n",
    "print()\n",
    "\n",
    "print(\"3. U-Shaped Similarity Pattern:\")\n",
    "print(\"   Documentation states: Layer 0 (0.899)  Layer 16 (0.758)  Layer 23 (0.923)\")\n",
    "print(\"   Implementation result: Layer 0 (0.899)  Layer 16 (0.758)  Layer 23 (0.923)\")\n",
    "print(\"   MATCH: \")\n",
    "print()\n",
    "\n",
    "print(\"4. Behavioral Results (Same top prediction):\")\n",
    "print(\"   Documentation states: 55-70% across typo types\")\n",
    "print(\"   Implementation shows:\")\n",
    "print(\"     - Substitution: 60.0%\")\n",
    "print(\"     - Transposition: 55.6%\")\n",
    "print(\"     - Deletion: 60.0%\")\n",
    "print(\"     - Insertion: 70.0%\")\n",
    "print(\"   MATCH: \")\n",
    "print()\n",
    "\n",
    "print(\"5. Position-Specific Patching Recovery:\")\n",
    "print(\"   Documentation states: 63-71% at typo position\")\n",
    "print(\"   Implementation shows: Layers 2-4 at Position 4 give 63-71% recovery\")\n",
    "print(\"   MATCH: \")\n",
    "print()\n",
    "\n",
    "print(\"CS1 ASSESSMENT: PASS - All evaluable conclusions match original results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1544a4",
   "metadata": {},
   "source": [
    "### CS1 Verification Summary\n",
    "\n",
    "| Claim | Documentation | Implementation | Match |\n",
    "|-------|--------------|----------------|-------|\n",
    "| MLP0 recovery | ~58% | 0.5804 (58.04%) |  |\n",
    "| Attention recovery | <1% | 0.0005 (0.05%) |  |\n",
    "| U-shaped Layer 0 | 0.899 | 0.899 |  |\n",
    "| U-shaped Layer 16 (min) | 0.758 | 0.758 |  |\n",
    "| U-shaped Layer 23 | 0.923 | 0.923 |  |\n",
    "| Same prediction rate | 55-70% | 55.6-70% |  |\n",
    "\n",
    "### CS1 Result: **PASS**\n",
    "\n",
    "All evaluable conclusions in the documentation precisely match the results recorded in the implementation notebook. The numerical values are consistent with no contradictions found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6389d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CS2: Implementation Follows the Plan\n",
    "\n",
    "### Plan V2 Steps (Final Plan) vs Implementation\n",
    "\n",
    "The final plan (plan_v2.md) includes \"Next Steps\" section with 4 items:\n",
    "1. Verify MLP0 importance across more examples\n",
    "2. Investigate what MLP0 neurons are doing specifically\n",
    "3. Test whether the U-shaped pattern holds for different models\n",
    "4. Create final documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a3c6df",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CS2: Evaluating Plan vs Implementation ===\n",
      "\n",
      "Plan V2 Next Steps:\n",
      "1. 'Verify MLP0 importance across more examples'\n",
      "   Implementation: Cell 37 tests MLP0 across 15 examples\n",
      "   Status: IMPLEMENTED \n",
      "\n",
      "2. 'Investigate what MLP0 neurons are doing specifically'\n",
      "   Implementation: Cells 43-46 analyze MLP0 neurons\n",
      "   Identified neurons 2527, 2066, 3244 as important\n",
      "   Status: IMPLEMENTED \n",
      "\n",
      "3. 'Test whether the U-shaped pattern holds for different models'\n",
      "   Implementation: NOT FOUND in notebook\n",
      "   Only GPT-2 Medium tested\n",
      "   Status: NOT IMPLEMENTED \n",
      "\n",
      "4. 'Create final documentation'\n",
      "   Implementation: logs/documentation.md exists\n",
      "   Status: IMPLEMENTED \n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Plan V1 Experimental Phases vs Implementation:\n",
      "\n",
      "Phase 1: Behavioral Baseline\n",
      "   - Create typo dataset: IMPLEMENTED (Cell 10, 39 examples)\n",
      "   - Measure next-token prediction: IMPLEMENTED (Cells 11-12)\n",
      "   Status: IMPLEMENTED \n",
      "\n",
      "Phase 2: Representation Analysis\n",
      "   - Layer-wise similarity: IMPLEMENTED (Cells 14-21)\n",
      "   - Activation patching: IMPLEMENTED (Cells 23-30)\n",
      "   - Probing for clean representations: IMPLEMENTED (Cells 38-40)\n",
      "   Status: IMPLEMENTED \n",
      "\n",
      "Phase 3: Causal Intervention\n",
      "   - Identify candidate heads/MLPs: IMPLEMENTED (Cells 32-35)\n",
      "   - Perform ablations: IMPLEMENTED via patching\n",
      "   - Test sufficiency: IMPLEMENTED\n",
      "   Status: IMPLEMENTED \n",
      "\n",
      "CS2 ASSESSMENT: FAIL\n",
      "Reason: Plan V2 'Next Step #3' - testing U-shaped pattern on different models - is not implemented\n"
     ]
    }
   ],
   "source": [
    "# CS2: Check if plan steps are reflected in implementation\n",
    "print(\"=== CS2: Evaluating Plan vs Implementation ===\\n\")\n",
    "\n",
    "print(\"Plan V2 Next Steps:\")\n",
    "print(\"1. 'Verify MLP0 importance across more examples'\")\n",
    "print(\"   Implementation: Cell 37 tests MLP0 across 15 examples\")\n",
    "print(\"   Status: IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "print(\"2. 'Investigate what MLP0 neurons are doing specifically'\")\n",
    "print(\"   Implementation: Cells 43-46 analyze MLP0 neurons\")\n",
    "print(\"   Identified neurons 2527, 2066, 3244 as important\")\n",
    "print(\"   Status: IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "print(\"3. 'Test whether the U-shaped pattern holds for different models'\")\n",
    "print(\"   Implementation: NOT FOUND in notebook\")\n",
    "print(\"   Only GPT-2 Medium tested\")\n",
    "print(\"   Status: NOT IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "print(\"4. 'Create final documentation'\")\n",
    "print(\"   Implementation: logs/documentation.md exists\")\n",
    "print(\"   Status: IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "# Also check the original Plan V1 phases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nPlan V1 Experimental Phases vs Implementation:\")\n",
    "print()\n",
    "print(\"Phase 1: Behavioral Baseline\")\n",
    "print(\"   - Create typo dataset: IMPLEMENTED (Cell 10, 39 examples)\")\n",
    "print(\"   - Measure next-token prediction: IMPLEMENTED (Cells 11-12)\")\n",
    "print(\"   Status: IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "print(\"Phase 2: Representation Analysis\")\n",
    "print(\"   - Layer-wise similarity: IMPLEMENTED (Cells 14-21)\")\n",
    "print(\"   - Activation patching: IMPLEMENTED (Cells 23-30)\")\n",
    "print(\"   - Probing for clean representations: IMPLEMENTED (Cells 38-40)\")\n",
    "print(\"   Status: IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "print(\"Phase 3: Causal Intervention\")\n",
    "print(\"   - Identify candidate heads/MLPs: IMPLEMENTED (Cells 32-35)\")\n",
    "print(\"   - Perform ablations: IMPLEMENTED via patching\")\n",
    "print(\"   - Test sufficiency: IMPLEMENTED\")\n",
    "print(\"   Status: IMPLEMENTED \")\n",
    "print()\n",
    "\n",
    "print(\"CS2 ASSESSMENT: FAIL\")\n",
    "print(\"Reason: Plan V2 'Next Step #3' - testing U-shaped pattern on different models - is not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d81a6",
   "metadata": {},
   "source": [
    "### CS2 Plan vs Implementation Analysis\n",
    "\n",
    "#### Plan V2 \"Next Steps\" Section:\n",
    "\n",
    "| Step | Description | Implementation Status |\n",
    "|------|-------------|----------------------|\n",
    "| 1 | Verify MLP0 importance across more examples |  Implemented (Cell 37, 15 examples) |\n",
    "| 2 | Investigate what MLP0 neurons are doing |  Implemented (Cells 43-46) |\n",
    "| 3 | Test U-shaped pattern for different models |  NOT Implemented |\n",
    "| 4 | Create final documentation |  Implemented |\n",
    "\n",
    "#### Plan V1 Experimental Phases:\n",
    "\n",
    "| Phase | Description | Implementation Status |\n",
    "|-------|-------------|----------------------|\n",
    "| Phase 1 | Behavioral Baseline |  Fully Implemented |\n",
    "| Phase 2 | Representation Analysis |  Fully Implemented |\n",
    "| Phase 3 | Causal Intervention |  Fully Implemented |\n",
    "\n",
    "### CS2 Result: **FAIL**\n",
    "\n",
    "The final plan (plan_v2.md) specifies \"Test whether the U-shaped pattern holds for different models\" as Next Step #3, but this step is not reflected in the implementation. Only GPT-2 Medium was tested. While the documentation acknowledges this as a limitation, the plan explicitly includes it as a step to be done, making this a missing implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a9b17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CS3: Effect Size\n",
    "\n",
    "Evaluating whether reported effects have non-trivial magnitude relative to baseline behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6addc943",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CS3: Effect Size Evaluation ===\n",
      "\n",
      "1. MLP Layer 0 Recovery Effect:\n",
      "   Effect size: 58% recovery\n",
      "   Baseline: Attention heads provide ~0.05% recovery\n",
      "   This is >1000x larger than attention contribution\n",
      "   Assessment: CLEARLY NON-TRIVIAL \n",
      "\n",
      "2. Position-Specific Effect:\n",
      "   Typo position (Pos 4): 63-71% recovery\n",
      "   Other positions: <3% recovery\n",
      "   This is ~20x difference\n",
      "   Assessment: CLEARLY NON-TRIVIAL \n",
      "\n",
      "3. U-Shaped Similarity Pattern:\n",
      "   Layer 0 similarity: 0.899\n",
      "   Layer 16 (min): 0.758\n",
      "   Layer 23: 0.923\n",
      "   Drop: 0.899  0.758 = 14% drop (substantial)\n",
      "   Recovery: 0.758  0.923 = 16.5% increase (substantial)\n",
      "   Assessment: CLEARLY NON-TRIVIAL \n",
      "\n",
      "4. Behavioral Effect:\n",
      "   Same top prediction: 55-70%\n",
      "   Probability ratio (typo/clean): 0.79-0.90\n",
      "   These are moderate effects showing variable robustness\n",
      "   Assessment: NON-TRIVIAL (shows real variability)\n",
      "\n",
      "5. MLP0 vs Other Components Comparison:\n",
      "   MLP0 alone: 58%\n",
      "   All attention heads (384 total): 0.05%\n",
      "   All other MLPs (layers 1-23): 8%\n",
      "   MLP0 is ~7x more important than all other MLPs combined\n",
      "   Assessment: CLEARLY NON-TRIVIAL \n",
      "\n",
      "CS3 ASSESSMENT: PASS\n",
      "Reason: All main effects show substantial magnitudes relative to baselines and control conditions\n"
     ]
    }
   ],
   "source": [
    "# CS3: Effect Size Evaluation\n",
    "print(\"=== CS3: Effect Size Evaluation ===\\n\")\n",
    "\n",
    "print(\"1. MLP Layer 0 Recovery Effect:\")\n",
    "print(\"   Effect size: 58% recovery\")\n",
    "print(\"   Baseline: Attention heads provide ~0.05% recovery\")\n",
    "print(\"   This is >1000x larger than attention contribution\")\n",
    "print(\"   Assessment: CLEARLY NON-TRIVIAL \")\n",
    "print()\n",
    "\n",
    "print(\"2. Position-Specific Effect:\")\n",
    "print(\"   Typo position (Pos 4): 63-71% recovery\")\n",
    "print(\"   Other positions: <3% recovery\")\n",
    "print(\"   This is ~20x difference\")\n",
    "print(\"   Assessment: CLEARLY NON-TRIVIAL \")\n",
    "print()\n",
    "\n",
    "print(\"3. U-Shaped Similarity Pattern:\")\n",
    "print(\"   Layer 0 similarity: 0.899\")\n",
    "print(\"   Layer 16 (min): 0.758\")\n",
    "print(\"   Layer 23: 0.923\")\n",
    "print(\"   Drop: 0.899  0.758 = 14% drop (substantial)\")\n",
    "print(\"   Recovery: 0.758  0.923 = 16.5% increase (substantial)\")\n",
    "print(\"   Assessment: CLEARLY NON-TRIVIAL \")\n",
    "print()\n",
    "\n",
    "print(\"4. Behavioral Effect:\")\n",
    "print(\"   Same top prediction: 55-70%\")\n",
    "print(\"   Probability ratio (typo/clean): 0.79-0.90\")\n",
    "print(\"   These are moderate effects showing variable robustness\")\n",
    "print(\"   Assessment: NON-TRIVIAL (shows real variability)\")\n",
    "print()\n",
    "\n",
    "print(\"5. MLP0 vs Other Components Comparison:\")\n",
    "print(\"   MLP0 alone: 58%\")\n",
    "print(\"   All attention heads (384 total): 0.05%\")\n",
    "print(\"   All other MLPs (layers 1-23): 8%\")\n",
    "print(\"   MLP0 is ~7x more important than all other MLPs combined\")\n",
    "print(\"   Assessment: CLEARLY NON-TRIVIAL \")\n",
    "print()\n",
    "\n",
    "print(\"CS3 ASSESSMENT: PASS\")\n",
    "print(\"Reason: All main effects show substantial magnitudes relative to baselines and control conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da289c0",
   "metadata": {},
   "source": [
    "### CS3 Effect Size Analysis\n",
    "\n",
    "| Finding | Effect Size | Baseline/Control | Ratio | Assessment |\n",
    "|---------|-------------|------------------|-------|------------|\n",
    "| MLP0 Recovery | 58% | 0.05% (attn) | >1000x | Non-trivial |\n",
    "| Position-specific (typo) | 63-71% | <3% (other pos) | ~20x | Non-trivial |\n",
    "| U-shaped drop | 0.8990.758 | N/A | 14% drop | Substantial |\n",
    "| U-shaped recovery | 0.7580.923 | N/A | 16.5% increase | Substantial |\n",
    "| MLP0 vs other MLPs | 58% | 8% combined | 7x | Non-trivial |\n",
    "\n",
    "### CS3 Result: **PASS**\n",
    "\n",
    "All reported effects show clearly non-trivial magnitudes:\n",
    "- MLP0 recovery (58%) is orders of magnitude larger than attention contributions\n",
    "- Position specificity shows ~20x difference between typo position and others\n",
    "- U-shaped similarity shows substantial 14% drop and 16.5% recovery\n",
    "- Conclusions do not rely on marginal or negligible changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-15-12-55_CircuitConsistencyEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
