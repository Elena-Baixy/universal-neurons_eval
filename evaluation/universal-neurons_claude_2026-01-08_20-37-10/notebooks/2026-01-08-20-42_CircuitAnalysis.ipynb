{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd616ed",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2a484",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for Universal Neurons\n",
    "\n",
    "This notebook evaluates whether the findings in the `universal-neurons_eval` repository generalize beyond the original experimental setting.\n",
    "\n",
    "## Evaluation Checklist\n",
    "- **GT1**: Generalization to a New Model\n",
    "- **GT2**: Generalization to New Data  \n",
    "- **GT3**: Method / Specificity Generalizability\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7134c7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python files in the repository:\n",
      "/net/scratch2/smallyan/universal-neurons_eval/summary.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/entropy_intervention.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/activations.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/make_dataset.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/attention_deactivation_qpos.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/correlations_parallel.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/summary_viewer.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/weights.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/attention_deactivation.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/correlations_fast.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/explain.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/utils.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/correlations.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/weights.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/vocab_df.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/entropy_neurons.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/correlations.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/activations.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/prediction_neurons.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/heuristic_explanation.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/sequence_features.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/plots.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/neuron_df.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/analysis/__init__.py\n",
      "/net/scratch2/smallyan/universal-neurons_eval/intervention.py\n",
      "\n",
      "\n",
      "JSON files in the repository:\n",
      "/net/scratch2/smallyan/universal-neurons_eval/evaluation/consistency_evaluation.json\n",
      "\n",
      "\n",
      "Markdown files in the repository:\n",
      "/net/scratch2/smallyan/universal-neurons_eval/CodeWalkthrough.md\n",
      "/net/scratch2/smallyan/universal-neurons_eval/plan.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the repository structure\n",
    "import subprocess\n",
    "\n",
    "repo_path = '/net/scratch2/smallyan/universal-neurons_eval'\n",
    "result = subprocess.run(['find', repo_path, '-type', 'f', '-name', '*.py'], capture_output=True, text=True)\n",
    "print(\"Python files in the repository:\")\n",
    "print(result.stdout)\n",
    "\n",
    "result2 = subprocess.run(['find', repo_path, '-type', 'f', '-name', '*.json'], capture_output=True, text=True)\n",
    "print(\"\\nJSON files in the repository:\")\n",
    "print(result2.stdout)\n",
    "\n",
    "result3 = subprocess.run(['find', repo_path, '-type', 'f', '-name', '*.md'], capture_output=True, text=True)\n",
    "print(\"\\nMarkdown files in the repository:\")\n",
    "print(result3.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9cbbd32",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Plan\n",
      "## Objective\n",
      "Study the universality of individual neurons across GPT2 language models trained from different random seeds to identify interpretable neurons and understand whether neural mechanisms are universal across models.\n",
      "\n",
      "## Hypothesis\n",
      "1. Universal neurons (those that consistently activate on the same inputs across different models) are more likely to be monosemantic and interpretable than non-universal neurons.\n",
      "2. Neurons with high activation correlation across models will have clear interpretations and can be taxonomized into a small number of neuron families.\n",
      "3. Universal neurons exhibit specific statistical properties in their weights and activations that distinguish them from non-universal neurons, including large negative input bias, high pre-activation skew and kurtosis, and large weight norm.\n",
      "\n",
      "## Methodology\n",
      "1. Compute pairwise Pearson correlations of neuron activations over 100 million tokens from the Pile test set for every neuron pair across five GPT2 models trained from different random seeds to identify universal neurons with excess correlation above baseline.\n",
      "2. Analyze statistical properties of universal neurons (excess correlation > 0.5) including activation statistics (mean, skew, kurtosis, sparsity) and weight statistics (input bias, cosine similarity between input and output weights, weight decay penalty).\n",
      "3. Develop automated tests using algorithmically generated labels from vocabulary elements and NLP tools (spaCy) to classify neurons into families by computing reduction in activation variance when conditioned on binary test explanations.\n",
      "4. Study neuron functional roles through weight analysis using logit attribution (WU*wout) to identify prediction, suppression, and partition neurons, and analyze moment statistics (kurtosis, skew, variance) of vocabulary effects.\n",
      "5. Perform causal interventions by fixing neuron activations to specific values and measuring effects on layer norm scale, next token entropy, attention head output norms, and BOS attention patterns through path ablation.\n",
      "\n",
      "## Experiments\n",
      "### Neuron correlation analysis across random seeds\n",
      "- What varied: Neuron pairs across five GPT2 models (GPT2-small, GPT2-medium, Pythia-160m) trained from different random initializations\n",
      "- Metric: Pairwise Pearson correlation of neuron activations over 100 million tokens; excess correlation (difference from random baseline)\n",
      "- Main result: Only 1-5% of neurons are universal (excess correlation > 0.5): GPT2-medium 1.23%, Pythia-160M 1.26%, GPT2-small 4.16%. Universal neurons show depth specialization, with most correlated neuron pairs occurring in similar layers.\n",
      "\n",
      "### Statistical properties of universal neurons\n",
      "- What varied: Universal (ϱ>0.5) vs non-universal neurons across GPT2-medium-a, GPT2-small-a, and Pythia-160m\n",
      "- Metric: Activation statistics (mean, skew, kurtosis, sparsity), weight statistics (bias, cosine similarity, weight norm, WU kurtosis), reported as percentiles within layer\n",
      "- Main result: Universal neurons have large weight norm, large negative input bias, high pre-activation skew and kurtosis (monosemantic signature), and lower activation frequency compared to non-universal neurons which show Gaussian-like distributions.\n",
      "\n",
      "### Taxonomization of universal neuron families\n",
      "- What varied: Universal neurons (ϱ>0.5) classified by automated tests using vocabulary properties and NLP labels\n",
      "- Metric: Reduction in activation variance when conditioned on binary test explanations (token properties, syntax, semantics, position)\n",
      "- Main result: Universal neurons cluster into families: unigram neurons (activate for specific tokens, concentrated in layers 0-1), alphabet neurons (18/26 letters), previous token neurons (layers 4-6), position neurons (layers 0-2), syntax neurons (linguistic features), and semantic/context neurons (topics, languages, domains).\n",
      "\n",
      "### Prediction neuron analysis via logit attribution\n",
      "- What varied: Neurons across layers analyzed by moments (kurtosis, skew, variance) of WU*wout distribution\n",
      "- Metric: Kurtosis and skew of vocabulary logit effects; layer-wise percentiles across GPT2-medium models and Pythia models (410M-6.9B)\n",
      "- Main result: After network midpoint, prediction neurons (high kurtosis, positive skew) become prevalent, peaking before final layers where suppression neurons (high kurtosis, negative skew) dominate. Pattern consistent across different seeds and model sizes. Suppression neurons activate more when next token is from the suppressed set.\n",
      "\n",
      "### Entropy modulation neurons via causal intervention\n",
      "- What varied: Fixed activation values (0-6) for entropy neuron L23.945 and anti-entropy neuron L22.2882 vs 20 random neurons from final two layers\n",
      "- Metric: Layer norm scale, next token prediction entropy, reciprocal rank of true token, cross-entropy loss\n",
      "- Main result: Entropy neuron (L23.945) has high weight norm and low logit variance. Increasing activation dramatically increases layer norm scale and prediction entropy while preserving token rank\n"
     ]
    }
   ],
   "source": [
    "# Read the plan file to understand the research\n",
    "with open(f'{repo_path}/plan.md', 'r') as f:\n",
    "    plan_content = f.read()\n",
    "print(plan_content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bed0cd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing. Anti-entropy neuron (L22.2882) has opposite effect with cos similarity -0.886 to entropy neuron.\n",
      "\n",
      "### Attention head deactivation neurons via path ablation\n",
      "- What varied: Path ablation of neuron L4.3594 on attention head L5.H0; analyzed all neuron-head pairs using heuristic score hn=WTout*WTQ*kBOS\n",
      "- Metric: Change in BOS attention and head output norm when neuron contribution is ablated; heuristic score distribution vs random baseline\n",
      "- Main result: Heuristic identifies neurons controlling BOS attention. Neuron L4.3594 increases BOS attention and decreases head L5.H0 output norm when activated (deactivation neuron). Median head has WO*vBOS norm 19.4x smaller than other tokens, enabling heads to turn off by attending to BOS.\n"
     ]
    }
   ],
   "source": [
    "# Continue reading the plan\n",
    "print(plan_content[5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec6c695",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Universal Neurons\n",
      "All supporting data and code for Universal Neurons in GPT2 Language Models by Gurnee et al. (2024).\n",
      "\n",
      "## Contents\n",
      "* `dataframes/neuron_dfs` contains dataframes with neuron statistics for all neurons for the main models studies.\n",
      "* `paper_notebooks` contains much of the plotting code to generate the figures in the paper.\n",
      "* `correlations_fast.py` contains the script to compute neuron correlations.\n",
      "* `summary.py` and `weights.py` contain scripts to compute neuron activation and weight statistic summaries for use of our summary viewer (contained in `summary_viewer.py`). See next section for more information on the data generated.\n",
      "* `activations.py` contains scripts to cache neuron activations.\n",
      "* `explain.py` contains script to compute our reduction in variance explanations.\n",
      "* `attention_deactivation.py`, `entropy_intervention.py`, and `intervention.py` contain scripts for our functional neuron experiments.\n",
      "* The `analysis` directory contains further plotting and analysis code.\n",
      "* The `slurm` directory contains the shell scripts used to run the experiments on the cluster. These are not necessary to run the code, but may be useful to reference if replicating our experiments in a different environment.\n",
      "\n",
      "\n",
      "## Summary Viewer\n",
      "For this project, we leveraged precomputed activation data to explore neurons with our neuron viewer.\n",
      "\n",
      "This data can either be recomputed using `summary.py` and `weights.py` or by downloading the data from our [box](TODO:add link) link. Add this to the top level of the directory. It is organized as follows:\n",
      "\n",
      "```python\n",
      "# Summary data for neuron weights in each model\n",
      "summary_data/model_name/weights/data_file\n",
      "\n",
      "# Summary data for activations of each model within different datasets\n",
      "summary_data/model_name/activations/dataset_name/data_file\n",
      "\n",
      "# This data can be loaded with via the following functions\n",
      "from summary_viewer import load_all_summaries, load_weights_summary\n",
      "dataset_summaries = load_all_summaries(model_name)\n",
      "weight_summaries = load_weights_summary(model_name)\n",
      "```\n",
      "\n",
      "A common pattern in the summary data is \"compressing\" a distribution by binning it while saving the tails. In particular, we compute the following:\n",
      "- `bin_counts`: a histogram of the distribution (where there is either a corresponding `bin_edges` or some standard bin size, look at the code in `weights.py` for details)\n",
      "- `max_ix`: the indices of the top k elements of the distribution\n",
      "- `max_vals`: the values of the top k elements of the distribution\n",
      "- `min_ix`: the indices of the bottom k elements of the distribution\n",
      "- `min_vals`: the values of the bottom k elements of the distribution\n",
      "Though note the naming convention is not always consistent.\n",
      "\n",
      "In particular, within the `weights` directory there is\n",
      "- `neuron_comps.pt` which is a dictionary with keys, `in_in`, `in_out`, `out_in`, `out_out` with values also being a dictionary with 'top_neuron_value', 'top_neuron_ix', 'bottom_neuron_value', 'bottom_neuron_ix', 'comp_hist' corresponding to the summary format described above. The distribution here is the cosine similarity between the {input, output} and {input, output} weight vectors of every pair of neurons in the model. Hence each of these will be a `n_layers x d_mlp x {k or n_bins}` tensor.\n",
      "- `vocab_comps.pt` which is a dict with keys 'E_in', 'U_in', 'E_out', 'U_out' corresponding to the cosine similarities betweein the {Embedding, Unembedding} and the neuron {input, output} weight vectors. Similar to the above, the values of these keys are also a dictionary with a summary data structures above, named 'top_vocab_value', 'top_vocab_ix', 'bottom_vocab_value', 'bottom_vocab_ix', 'comp_hist' again with shape  `n_layers x d_mlp x {k or n_bins}` tensor.\n",
      "- `{k, q, v, o}_comps.pt`: each of these are tensors with shape `n_layers x d_mlp x n_layers x n_heads_per_layer`. They give the composition scores for each combination of neuron and attention head for the {key, query, value, output} vectors. For example, ||W_QK @ n_out|| / (||W_QK|| ||n_out||) for k_comp.\n",
      "\n",
      "Activations are similar, with binned activations histograms for each distribution, as well as mean and max vocab activations for each neuron, and max activating dataset examples.\n",
      "\n",
      "\n",
      "## Cite us\n",
      "```\n",
      "@article{gurnee2024universal,\n",
      "  title={Universal neurons in gpt2 language models},\n",
      "  author={Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},\n",
      "  journal={arXiv preprint arXiv:2401.12181},\n",
      "  year={2024}\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CodeWalkthrough to understand the implementation\n",
    "with open(f'{repo_path}/CodeWalkthrough.md', 'r') as f:\n",
    "    walkthrough = f.read()\n",
    "print(walkthrough[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f3a7d8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataframes/ exists:\n",
      "  - interpretable_neurons\n",
      "  - neuron_dfs\n",
      "  - vocab_dfs\n",
      "\n",
      "paper_notebooks/ exists:\n",
      "  - previous_token_neurons.ipynb\n",
      "  - bos_signal_neurons.ipynb\n",
      "  - properties_of_universal_neurons.ipynb\n",
      "  - alphabet_neurons.ipynb\n",
      "  - unigram_neurons.ipynb\n",
      "  - prediction_neurons.ipynb\n",
      "  - syntax_neurons.ipynb\n",
      "  - position_neurons.ipynb\n",
      "  - entropy_neurons.ipynb\n",
      "  - mysteries.ipynb\n",
      "  - topic_neurons.ipynb\n",
      "  - family_count.ipynb\n",
      "\n",
      "summary_data/ does not exist\n",
      "\n",
      "evaluation/ exists:\n",
      "  - replications\n",
      "  - self_matching.ipynb\n",
      "  - consistency_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "# Check available data in the repository\n",
    "import os\n",
    "\n",
    "# Check directories\n",
    "dirs_to_check = ['dataframes', 'paper_notebooks', 'summary_data', 'evaluation']\n",
    "for d in dirs_to_check:\n",
    "    full_path = os.path.join(repo_path, d)\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"\\n{d}/ exists:\")\n",
    "        try:\n",
    "            contents = os.listdir(full_path)[:20]\n",
    "            for c in contents:\n",
    "                print(f\"  - {c}\")\n",
    "        except:\n",
    "            print(\"  Could not list contents\")\n",
    "    else:\n",
    "        print(f\"\\n{d}/ does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7ded81",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron dataframes available:\n",
      "  - stanford-gpt2-small-a.csv\n",
      "  - pythia-160m.csv\n",
      "  - stanford-gpt2-medium-a.csv\n"
     ]
    }
   ],
   "source": [
    "# Check the neuron dataframes to understand what neurons were identified\n",
    "import pandas as pd\n",
    "\n",
    "neuron_df_path = os.path.join(repo_path, 'dataframes', 'neuron_dfs')\n",
    "contents = os.listdir(neuron_df_path)\n",
    "print(\"Neuron dataframes available:\")\n",
    "for c in contents:\n",
    "    print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18250107",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2-medium-a neuron dataframe:\n",
      "Shape: (98304, 22)\n",
      "Columns: ['layer', 'neuron', 'max_corr', 'mean_corr', 'min_corr', 'max_baseline', 'min_baseline', 'mean_baseline', 'w_in_norm', 'input_bias', 'w_out_norm', 'in_out_sim', 'l2_penalty', 'mean', 'var', 'skew', 'kurt', 'vocab_mean', 'vocab_var', 'vocab_skew', 'vocab_kurt', 'sparsity']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>neuron</th>\n",
       "      <th>max_corr</th>\n",
       "      <th>mean_corr</th>\n",
       "      <th>min_corr</th>\n",
       "      <th>max_baseline</th>\n",
       "      <th>min_baseline</th>\n",
       "      <th>mean_baseline</th>\n",
       "      <th>w_in_norm</th>\n",
       "      <th>input_bias</th>\n",
       "      <th>...</th>\n",
       "      <th>l2_penalty</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>vocab_mean</th>\n",
       "      <th>vocab_var</th>\n",
       "      <th>vocab_skew</th>\n",
       "      <th>vocab_kurt</th>\n",
       "      <th>sparsity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4646</td>\n",
       "      <td>0.439825</td>\n",
       "      <td>0.4092</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>0.342250</td>\n",
       "      <td>0.370499</td>\n",
       "      <td>-0.388558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271325</td>\n",
       "      <td>-1.077630</td>\n",
       "      <td>0.410425</td>\n",
       "      <td>0.063839</td>\n",
       "      <td>2.780662</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>2.978626</td>\n",
       "      <td>0.057869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.228850</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.368054</td>\n",
       "      <td>-0.367035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251356</td>\n",
       "      <td>-0.619509</td>\n",
       "      <td>0.104088</td>\n",
       "      <td>0.417814</td>\n",
       "      <td>4.443205</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>2.983132</td>\n",
       "      <td>0.057327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.518625</td>\n",
       "      <td>0.4575</td>\n",
       "      <td>0.1963</td>\n",
       "      <td>0.1887</td>\n",
       "      <td>0.190875</td>\n",
       "      <td>0.395692</td>\n",
       "      <td>-0.539252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288211</td>\n",
       "      <td>-1.267382</td>\n",
       "      <td>0.312051</td>\n",
       "      <td>0.506417</td>\n",
       "      <td>3.808384</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>3.005584</td>\n",
       "      <td>0.028301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.340350</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>0.2527</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.240050</td>\n",
       "      <td>0.392904</td>\n",
       "      <td>-0.490780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280817</td>\n",
       "      <td>-1.150307</td>\n",
       "      <td>0.335263</td>\n",
       "      <td>0.261270</td>\n",
       "      <td>3.022481</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>2.992840</td>\n",
       "      <td>0.037805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2878</td>\n",
       "      <td>0.263300</td>\n",
       "      <td>0.2396</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.150250</td>\n",
       "      <td>0.385215</td>\n",
       "      <td>-0.417577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262087</td>\n",
       "      <td>-0.711964</td>\n",
       "      <td>0.165003</td>\n",
       "      <td>0.494364</td>\n",
       "      <td>4.288840</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>-0.004821</td>\n",
       "      <td>3.000447</td>\n",
       "      <td>0.061776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer  neuron  max_corr  mean_corr  min_corr  max_baseline  min_baseline  \\\n",
       "0      0       0    0.4646   0.439825    0.4092        0.3500        0.3330   \n",
       "1      0       1    0.2600   0.228850    0.2119        0.1885        0.1608   \n",
       "2      0       2    0.5570   0.518625    0.4575        0.1963        0.1887   \n",
       "3      0       3    0.3880   0.340350    0.3157        0.2527        0.2285   \n",
       "4      0       4    0.2878   0.263300    0.2396        0.1550        0.1460   \n",
       "\n",
       "   mean_baseline  w_in_norm  input_bias  ...  l2_penalty      mean       var  \\\n",
       "0       0.342250   0.370499   -0.388558  ...    0.271325 -1.077630  0.410425   \n",
       "1       0.168700   0.368054   -0.367035  ...    0.251356 -0.619509  0.104088   \n",
       "2       0.190875   0.395692   -0.539252  ...    0.288211 -1.267382  0.312051   \n",
       "3       0.240050   0.392904   -0.490780  ...    0.280817 -1.150307  0.335263   \n",
       "4       0.150250   0.385215   -0.417577  ...    0.262087 -0.711964  0.165003   \n",
       "\n",
       "       skew      kurt  vocab_mean  vocab_var  vocab_skew  vocab_kurt  sparsity  \n",
       "0  0.063839  2.780662    0.000301   0.000995    0.007442    2.978626  0.057869  \n",
       "1  0.417814  4.443205   -0.000187   0.000939    0.017759    2.983132  0.057327  \n",
       "2  0.506417  3.808384   -0.000063   0.000957   -0.001548    3.005584  0.028301  \n",
       "3  0.261270  3.022481   -0.000015   0.000930    0.014691    2.992840  0.037805  \n",
       "4  0.494364  4.288840    0.000254   0.000920   -0.004821    3.000447  0.061776  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and examine the neuron dataframes to understand the findings\n",
    "df_gpt2_medium = pd.read_csv(os.path.join(neuron_df_path, 'stanford-gpt2-medium-a.csv'))\n",
    "print(\"GPT2-medium-a neuron dataframe:\")\n",
    "print(f\"Shape: {df_gpt2_medium.shape}\")\n",
    "print(f\"Columns: {df_gpt2_medium.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_gpt2_medium.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc195ab",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of universal neurons in GPT2-medium-a: 2010 / 98304 (2.04%)\n",
      "\n",
      "Top 20 universal neurons by excess correlation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>neuron</th>\n",
       "      <th>max_corr</th>\n",
       "      <th>max_baseline</th>\n",
       "      <th>excess_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>1</td>\n",
       "      <td>657</td>\n",
       "      <td>0.9360</td>\n",
       "      <td>0.12520</td>\n",
       "      <td>0.81080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>1</td>\n",
       "      <td>1031</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.14950</td>\n",
       "      <td>0.80550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.80290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0</td>\n",
       "      <td>553</td>\n",
       "      <td>0.9805</td>\n",
       "      <td>0.17770</td>\n",
       "      <td>0.80280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5489</th>\n",
       "      <td>1</td>\n",
       "      <td>1393</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.15670</td>\n",
       "      <td>0.78830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93509</th>\n",
       "      <td>22</td>\n",
       "      <td>3397</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>0.13950</td>\n",
       "      <td>0.78820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13640</th>\n",
       "      <td>3</td>\n",
       "      <td>1352</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.78560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>1</td>\n",
       "      <td>1566</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.16050</td>\n",
       "      <td>0.78450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6877</th>\n",
       "      <td>1</td>\n",
       "      <td>2781</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.18840</td>\n",
       "      <td>0.78260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.18540</td>\n",
       "      <td>0.78190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>0</td>\n",
       "      <td>3043</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>0.16870</td>\n",
       "      <td>0.78130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52486</th>\n",
       "      <td>12</td>\n",
       "      <td>3334</td>\n",
       "      <td>0.9280</td>\n",
       "      <td>0.14880</td>\n",
       "      <td>0.77920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87147</th>\n",
       "      <td>21</td>\n",
       "      <td>1131</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.09820</td>\n",
       "      <td>0.77880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18918</th>\n",
       "      <td>4</td>\n",
       "      <td>2534</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.15920</td>\n",
       "      <td>0.77830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>0</td>\n",
       "      <td>2088</td>\n",
       "      <td>0.9507</td>\n",
       "      <td>0.17460</td>\n",
       "      <td>0.77610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62987</th>\n",
       "      <td>15</td>\n",
       "      <td>1547</td>\n",
       "      <td>0.9297</td>\n",
       "      <td>0.15500</td>\n",
       "      <td>0.77470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23555</th>\n",
       "      <td>5</td>\n",
       "      <td>3075</td>\n",
       "      <td>0.8823</td>\n",
       "      <td>0.10834</td>\n",
       "      <td>0.77396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>2</td>\n",
       "      <td>2130</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.77280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50312</th>\n",
       "      <td>12</td>\n",
       "      <td>1160</td>\n",
       "      <td>0.9116</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.77180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0</td>\n",
       "      <td>771</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.17700</td>\n",
       "      <td>0.77000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       layer  neuron  max_corr  max_baseline  excess_corr\n",
       "4753       1     657    0.9360       0.12520      0.81080\n",
       "5127       1    1031    0.9550       0.14950      0.80550\n",
       "185        0     185    0.9670       0.16410      0.80290\n",
       "553        0     553    0.9805       0.17770      0.80280\n",
       "5489       1    1393    0.9450       0.15670      0.78830\n",
       "93509     22    3397    0.9277       0.13950      0.78820\n",
       "13640      3    1352    0.9653       0.17970      0.78560\n",
       "5662       1    1566    0.9450       0.16050      0.78450\n",
       "6877       1    2781    0.9710       0.18840      0.78260\n",
       "345        0     345    0.9673       0.18540      0.78190\n",
       "3043       0    3043    0.9500       0.16870      0.78130\n",
       "52486     12    3334    0.9280       0.14880      0.77920\n",
       "87147     21    1131    0.8770       0.09820      0.77880\n",
       "18918      4    2534    0.9375       0.15920      0.77830\n",
       "2088       0    2088    0.9507       0.17460      0.77610\n",
       "62987     15    1547    0.9297       0.15500      0.77470\n",
       "23555      5    3075    0.8823       0.10834      0.77396\n",
       "10322      2    2130    0.9510       0.17820      0.77280\n",
       "50312     12    1160    0.9116       0.13980      0.77180\n",
       "771        0     771    0.9470       0.17700      0.77000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the universal neurons (excess correlation > 0.5)\n",
    "# Excess correlation = max_corr - max_baseline\n",
    "df_gpt2_medium['excess_corr'] = df_gpt2_medium['max_corr'] - df_gpt2_medium['max_baseline']\n",
    "universal_neurons = df_gpt2_medium[df_gpt2_medium['excess_corr'] > 0.5]\n",
    "print(f\"Number of universal neurons in GPT2-medium-a: {len(universal_neurons)} / {len(df_gpt2_medium)} ({100*len(universal_neurons)/len(df_gpt2_medium):.2f}%)\")\n",
    "print(\"\\nTop 20 universal neurons by excess correlation:\")\n",
    "universal_neurons.sort_values('excess_corr', ascending=False).head(20)[['layer', 'neuron', 'max_corr', 'max_baseline', 'excess_corr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f7b791",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretable neurons files:\n",
      "  - stanford-gpt2-medium-a\n",
      "  - stanford-gpt2-small-a\n",
      "  - pythia-160m\n"
     ]
    }
   ],
   "source": [
    "# Check the interpretable neurons folder\n",
    "interp_path = os.path.join(repo_path, 'dataframes', 'interpretable_neurons')\n",
    "if os.path.exists(interp_path):\n",
    "    print(\"Interpretable neurons files:\")\n",
    "    for f in os.listdir(interp_path):\n",
    "        print(f\"  - {f}\")\n",
    "        full_file = os.path.join(interp_path, f)\n",
    "        if f.endswith('.csv'):\n",
    "            df = pd.read_csv(full_file)\n",
    "            print(f\"    Shape: {df.shape}\")\n",
    "            print(f\"    Columns: {df.columns.tolist()[:10]}\")\n",
    "else:\n",
    "    print(\"No interpretable_neurons directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174e2016",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of stanford-gpt2-medium-a:\n",
      "  - universal.csv\n",
      "  - prediction_neurons.csv\n"
     ]
    }
   ],
   "source": [
    "# Check the contents of one of the interpretable neurons folders\n",
    "interp_gpt2_medium = os.path.join(interp_path, 'stanford-gpt2-medium-a')\n",
    "if os.path.exists(interp_gpt2_medium):\n",
    "    print(\"Contents of stanford-gpt2-medium-a:\")\n",
    "    for f in os.listdir(interp_gpt2_medium):\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9986bf2a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal neurons CSV shape: (1211, 23)\n",
      "Columns: ['layer', 'neuron', 'max_corr', 'mean_corr', 'min_corr', 'max_baseline', 'min_baseline', 'mean_baseline', 'w_in_norm', 'input_bias', 'w_out_norm', 'in_out_sim', 'l2_penalty', 'mean', 'var', 'skew', 'kurt', 'vocab_mean', 'vocab_var', 'vocab_skew', 'vocab_kurt', 'sparsity', 'excess_corr']\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>neuron</th>\n",
       "      <th>max_corr</th>\n",
       "      <th>mean_corr</th>\n",
       "      <th>min_corr</th>\n",
       "      <th>max_baseline</th>\n",
       "      <th>min_baseline</th>\n",
       "      <th>mean_baseline</th>\n",
       "      <th>w_in_norm</th>\n",
       "      <th>input_bias</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>vocab_mean</th>\n",
       "      <th>vocab_var</th>\n",
       "      <th>vocab_skew</th>\n",
       "      <th>vocab_kurt</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>excess_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.702625</td>\n",
       "      <td>0.6772</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.185625</td>\n",
       "      <td>0.387450</td>\n",
       "      <td>-0.447526</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.103980</td>\n",
       "      <td>0.316467</td>\n",
       "      <td>0.510973</td>\n",
       "      <td>3.621278</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.016318</td>\n",
       "      <td>2.996915</td>\n",
       "      <td>0.041241</td>\n",
       "      <td>0.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0.7637</td>\n",
       "      <td>0.732850</td>\n",
       "      <td>0.6943</td>\n",
       "      <td>0.2158</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.198325</td>\n",
       "      <td>0.376307</td>\n",
       "      <td>-0.409769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.912744</td>\n",
       "      <td>0.254701</td>\n",
       "      <td>0.718533</td>\n",
       "      <td>4.532629</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>-0.001359</td>\n",
       "      <td>3.019827</td>\n",
       "      <td>0.056931</td>\n",
       "      <td>0.534525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.7886</td>\n",
       "      <td>0.734850</td>\n",
       "      <td>0.6973</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.169125</td>\n",
       "      <td>0.356303</td>\n",
       "      <td>-0.392182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660341</td>\n",
       "      <td>0.165108</td>\n",
       "      <td>1.585033</td>\n",
       "      <td>9.291728</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>2.990849</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.565725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>0.9624</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.3694</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.313525</td>\n",
       "      <td>0.348459</td>\n",
       "      <td>-0.345287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.698817</td>\n",
       "      <td>0.225584</td>\n",
       "      <td>2.267953</td>\n",
       "      <td>12.477121</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>-0.055805</td>\n",
       "      <td>2.993432</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.643575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.937175</td>\n",
       "      <td>0.9194</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.405053</td>\n",
       "      <td>-0.610551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.987586</td>\n",
       "      <td>0.267893</td>\n",
       "      <td>1.985424</td>\n",
       "      <td>21.138607</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.100476</td>\n",
       "      <td>3.083210</td>\n",
       "      <td>0.020191</td>\n",
       "      <td>0.781975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0.7680</td>\n",
       "      <td>0.717825</td>\n",
       "      <td>0.6826</td>\n",
       "      <td>0.2108</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.338234</td>\n",
       "      <td>-0.318337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500606</td>\n",
       "      <td>0.183664</td>\n",
       "      <td>2.847586</td>\n",
       "      <td>16.874023</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.074644</td>\n",
       "      <td>3.125614</td>\n",
       "      <td>0.083778</td>\n",
       "      <td>0.517925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.943300</td>\n",
       "      <td>0.9414</td>\n",
       "      <td>0.3223</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.294575</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>-0.361166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514652</td>\n",
       "      <td>0.233543</td>\n",
       "      <td>2.905099</td>\n",
       "      <td>17.004550</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.021488</td>\n",
       "      <td>3.009044</td>\n",
       "      <td>0.101384</td>\n",
       "      <td>0.648725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.864850</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.169450</td>\n",
       "      <td>0.391586</td>\n",
       "      <td>-0.405299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.771277</td>\n",
       "      <td>0.219925</td>\n",
       "      <td>1.266032</td>\n",
       "      <td>11.711295</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.027749</td>\n",
       "      <td>3.075940</td>\n",
       "      <td>0.057483</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.898925</td>\n",
       "      <td>0.7954</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.377022</td>\n",
       "      <td>-0.703568</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.405397</td>\n",
       "      <td>0.556662</td>\n",
       "      <td>0.493350</td>\n",
       "      <td>5.038054</td>\n",
       "      <td>-0.001262</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.286076</td>\n",
       "      <td>3.305293</td>\n",
       "      <td>0.029009</td>\n",
       "      <td>0.730025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>0.826350</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.1803</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.175575</td>\n",
       "      <td>0.373433</td>\n",
       "      <td>-0.349906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791489</td>\n",
       "      <td>0.253866</td>\n",
       "      <td>1.257878</td>\n",
       "      <td>11.766571</td>\n",
       "      <td>-0.000491</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.018181</td>\n",
       "      <td>3.050753</td>\n",
       "      <td>0.067014</td>\n",
       "      <td>0.650775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer  neuron  max_corr  mean_corr  min_corr  max_baseline  min_baseline  \\\n",
       "0      0      37    0.7236   0.702625    0.6772        0.1907        0.1800   \n",
       "1      0      45    0.7637   0.732850    0.6943        0.2158        0.1890   \n",
       "2      0      50    0.7886   0.734850    0.6973        0.1810        0.1556   \n",
       "3      0     135    0.9624   0.957100    0.9510        0.3694        0.2732   \n",
       "4      0     185    0.9670   0.937175    0.9194        0.1641        0.1492   \n",
       "5      0     196    0.7680   0.717825    0.6826        0.2108        0.1847   \n",
       "6      0     197    0.9450   0.943300    0.9414        0.3223        0.2686   \n",
       "7      0     205    0.8850   0.864850    0.8390        0.1833        0.1592   \n",
       "8      0     345    0.9673   0.898925    0.7954        0.1854        0.1544   \n",
       "9      0     348    0.9087   0.826350    0.6920        0.1803        0.1713   \n",
       "\n",
       "   mean_baseline  w_in_norm  input_bias  ...      mean       var      skew  \\\n",
       "0       0.185625   0.387450   -0.447526  ... -1.103980  0.316467  0.510973   \n",
       "1       0.198325   0.376307   -0.409769  ... -0.912744  0.254701  0.718533   \n",
       "2       0.169125   0.356303   -0.392182  ... -0.660341  0.165108  1.585033   \n",
       "3       0.313525   0.348459   -0.345287  ... -0.698817  0.225584  2.267953   \n",
       "4       0.155200   0.405053   -0.610551  ... -0.987586  0.267893  1.985424   \n",
       "5       0.199900   0.338234   -0.318337  ... -0.500606  0.183664  2.847586   \n",
       "6       0.294575   0.349206   -0.361166  ... -0.514652  0.233543  2.905099   \n",
       "7       0.169450   0.391586   -0.405299  ... -0.771277  0.219925  1.266032   \n",
       "8       0.168900   0.377022   -0.703568  ... -1.405397  0.556662  0.493350   \n",
       "9       0.175575   0.373433   -0.349906  ... -0.791489  0.253866  1.257878   \n",
       "\n",
       "        kurt  vocab_mean  vocab_var  vocab_skew  vocab_kurt  sparsity  \\\n",
       "0   3.621278    0.000199   0.000977   -0.016318    2.996915  0.041241   \n",
       "1   4.532629   -0.000166   0.000980   -0.001359    3.019827  0.056931   \n",
       "2   9.291728    0.000309   0.000947   -0.010772    2.990849  0.076345   \n",
       "3  12.477121    0.000074   0.000952   -0.055805    2.993432  0.070599   \n",
       "4  21.138607    0.000645   0.000988    0.100476    3.083210  0.020191   \n",
       "5  16.874023    0.000486   0.001069    0.074644    3.125614  0.083778   \n",
       "6  17.004550    0.000196   0.000956    0.021488    3.009044  0.101384   \n",
       "7  11.711295   -0.000276   0.000948    0.027749    3.075940  0.057483   \n",
       "8   5.038054   -0.001262   0.001196    0.286076    3.305293  0.029009   \n",
       "9  11.766571   -0.000491   0.000906    0.018181    3.050753  0.067014   \n",
       "\n",
       "   excess_corr  \n",
       "0     0.517000  \n",
       "1     0.534525  \n",
       "2     0.565725  \n",
       "3     0.643575  \n",
       "4     0.781975  \n",
       "5     0.517925  \n",
       "6     0.648725  \n",
       "7     0.695400  \n",
       "8     0.730025  \n",
       "9     0.650775  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the universal neurons file\n",
    "universal_neurons_df = pd.read_csv(os.path.join(interp_gpt2_medium, 'universal.csv'))\n",
    "print(f\"Universal neurons CSV shape: {universal_neurons_df.shape}\")\n",
    "print(f\"Columns: {universal_neurons_df.columns.tolist()}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "universal_neurons_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "517525e5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction neurons CSV shape: (136, 4)\n",
      "Columns: ['layer', 'neuron', 'feature', 'is_surpress']\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>neuron</th>\n",
       "      <th>feature</th>\n",
       "      <th>is_surpress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>3440</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>2148</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>591</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>1121</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>2336</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>84</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>2559</td>\n",
       "      <td>all_caps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>1585</td>\n",
       "      <td>end_w_ing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>3534</td>\n",
       "      <td>end_w_ing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer  neuron    feature  is_surpress\n",
       "0     20      13   all_caps        False\n",
       "1     23    3440   all_caps         True\n",
       "2     21    2148   all_caps         True\n",
       "3     15     591   all_caps         True\n",
       "4     19    1121   all_caps        False\n",
       "5     18    2336   all_caps        False\n",
       "6     15      84   all_caps        False\n",
       "7     17    2559   all_caps        False\n",
       "8     22    1585  end_w_ing        False\n",
       "9     22    3534  end_w_ing        False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check prediction neurons\n",
    "prediction_neurons_df = pd.read_csv(os.path.join(interp_gpt2_medium, 'prediction_neurons.csv'))\n",
    "print(f\"Prediction neurons CSV shape: {prediction_neurons_df.shape}\")\n",
    "print(f\"Columns: {prediction_neurons_df.columns.tolist()}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "prediction_neurons_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf39047",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique features in prediction neurons:\n",
      "['all_caps' 'end_w_ing' 'is_year' 'is_intensive_pronoun'\n",
      " 'is_second_person_pronoun' 'is_female_pronoun' 'is_neutral_pronoun'\n",
      " 'is_male_pronoun' 'contains_question' 'is_one_digit'\n",
      " 'contains_exclamation' 'is_relative_pronoun' 'start_w_no_space_and_digit'\n",
      " 'contains_close_paren' 'contains_quotation' 'is_interrogative_pronoun'\n",
      " 'is_month' 'contains_semicolon' 'contains_open_bracket'\n",
      " 'contains_open_paren' 'is_demonstrative_pronoun' 'contains_digit'\n",
      " 'contains_colon' 'is_state' 'is_possessive_pronoun']\n",
      "\n",
      "Feature counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "end_w_ing                     15\n",
       "is_neutral_pronoun            10\n",
       "is_second_person_pronoun       9\n",
       "all_caps                       8\n",
       "contains_colon                 8\n",
       "is_demonstrative_pronoun       8\n",
       "contains_open_paren            8\n",
       "is_year                        5\n",
       "contains_digit                 5\n",
       "is_male_pronoun                5\n",
       "contains_question              5\n",
       "is_relative_pronoun            5\n",
       "contains_close_paren           5\n",
       "contains_open_bracket          5\n",
       "is_interrogative_pronoun       5\n",
       "is_month                       4\n",
       "start_w_no_space_and_digit     4\n",
       "contains_quotation             4\n",
       "is_possessive_pronoun          4\n",
       "contains_semicolon             3\n",
       "is_female_pronoun              3\n",
       "is_state                       3\n",
       "contains_exclamation           2\n",
       "is_one_digit                   2\n",
       "is_intensive_pronoun           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what features are tracked\n",
    "print(\"Unique features in prediction neurons:\")\n",
    "print(prediction_neurons_df['feature'].unique())\n",
    "print(\"\\nFeature counts:\")\n",
    "prediction_neurons_df['feature'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4469ff0d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replications contents:\n",
      "  - universal_neurons_properties.png\n",
      "  - layer_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# Look at the replications folder to understand what has been replicated\n",
    "replications_path = os.path.join(repo_path, 'evaluation', 'replications')\n",
    "if os.path.exists(replications_path):\n",
    "    print(\"Replications contents:\")\n",
    "    for f in os.listdir(replications_path):\n",
    "        print(f\"  - {f}\")\n",
    "        subpath = os.path.join(replications_path, f)\n",
    "        if os.path.isdir(subpath):\n",
    "            for sf in os.listdir(subpath)[:10]:\n",
    "                print(f\"      - {sf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7138d590",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency evaluation:\n",
      "{\n",
      "  \"Checklist\": {\n",
      "    \"CS1_Results_vs_Conclusion\": \"PASS\",\n",
      "    \"CS2_Plan_vs_Implementation\": \"PASS\"\n",
      "  },\n",
      "  \"Rationale\": {\n",
      "    \"CS1_Results_vs_Conclusion\": \"All evaluable conclusions in the plan match the recorded results in the implementation notebooks: (1) Universal neuron percentages match exactly - GPT2-medium 1.23%, Pythia-160M 1.26%, GPT2-small 4.16% as claimed in plan and verified in family_count.ipynb/neuron dataframes; (2) Statistical properties of universal neurons match - they have more negative input bias (-0.486 vs -0.248), higher activation skew (1.100 vs 0.024), and higher kurtosis (8.111 vs 3.735) as shown in properties_of_universal_neurons.ipynb; (3) Prediction neuron layer distribution matches - high vocab_kurt neurons are concentrated in later layers (17-23) with 1136 neurons after midpoint vs 125 before, confirming the plan claim that prediction neurons become prevalent after network midpoint.\",\n",
      "    \"CS2_Plan_vs_Implementation\": \"A plan.md file exists and all 6 methodology steps from the plan are fully implemented: (1) Pearson correlations implemented in correlations_fast.py, correlations_parallel.py, correlations.py; (2) Statistical property analysis in summary.py, weights.py, and properties_of_universal_neurons.ipynb; (3) Automated tests using vocab/NLP labels in explain.py and analysis/heuristic_explanation.py; (4) Weight analysis/logit attribution in prediction_neurons.ipynb and analysis/prediction_neurons.py; (5) Entropy neuron interventions in entropy_intervention.py and entropy_neurons.ipynb; (6) Path ablation for attention deactivation in attention_deactivation.py and bos_signal_neurons.ipynb.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read the consistency evaluation file\n",
    "import json\n",
    "consistency_path = os.path.join(repo_path, 'evaluation', 'consistency_evaluation.json')\n",
    "with open(consistency_path, 'r') as f:\n",
    "    consistency_eval = json.load(f)\n",
    "print(\"Consistency evaluation:\")\n",
    "print(json.dumps(consistency_eval, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c87d2419",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n",
      "GPU Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Check available GPU and load necessary libraries\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa036771",
   "metadata": {},
   "source": [
    "## Summary of Repository Findings\n",
    "\n",
    "Based on the exploration:\n",
    "\n",
    "1. **Main Finding**: The paper identifies \"universal neurons\" - neurons that activate on the same inputs across different models trained from different random seeds. Universal neurons have excess correlation > 0.5 and comprise only 1-5% of neurons.\n",
    "\n",
    "2. **Key Properties of Universal Neurons**:\n",
    "   - Large negative input bias\n",
    "   - High activation skew and kurtosis (monosemantic signature)\n",
    "   - High weight norm\n",
    "   - Lower activation frequency\n",
    "\n",
    "3. **Neuron Families Identified**:\n",
    "   - Unigram neurons (specific tokens)\n",
    "   - Alphabet neurons\n",
    "   - Previous token neurons\n",
    "   - Position neurons\n",
    "   - Syntax neurons\n",
    "   - Semantic/context neurons\n",
    "   - Prediction neurons (high vocab_kurt)\n",
    "   - Entropy neurons\n",
    "\n",
    "4. **Models Used in Original Work**:\n",
    "   - GPT2-small (stanford-gpt2-small-a)\n",
    "   - GPT2-medium (stanford-gpt2-medium-a)\n",
    "   - Pythia-160m\n",
    "\n",
    "5. **Dataset Used**: Pile test set (100 million tokens)\n",
    "\n",
    "---\n",
    "\n",
    "## GT1: Generalization to a New Model\n",
    "\n",
    "We will test whether the universal neuron findings generalize to a model NOT used in the original work.\n",
    "\n",
    "**New model to test**: GPT2-large or a different Pythia model (e.g., Pythia-410m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b0cfe0a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set OPENAI_API_KEY=****\n",
      "Set HF_TOKEN=****\n",
      "Set NDIF_API_KEY=****\n",
      "Set OPENROUTER_API_KEY=****\n",
      "\n",
      "Checking for HF token...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables for API access\n",
    "import subprocess\n",
    "result = subprocess.run(['bash', '-c', 'source /home/smallyan/.bashrc && env'], capture_output=True, text=True)\n",
    "env_vars = {}\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if '=' in line:\n",
    "        key, _, value = line.partition('=')\n",
    "        env_vars[key] = value\n",
    "        if 'TOKEN' in key or 'API' in key or 'KEY' in key:\n",
    "            os.environ[key] = value\n",
    "            print(f\"Set {key}=****\")\n",
    "            \n",
    "# Also try sourcing directly\n",
    "import os\n",
    "hf_token_path = os.path.expanduser('~/.bashrc')\n",
    "print(\"\\nChecking for HF token...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c77ff9b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Install required packages and load models\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets torch numpy scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbcbcacc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading GPT2-medium...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "PermissionError at /net/projects2/chai-lab/shared_models/hub/.locks/models--gpt2-medium/fc5a354a19255ad494f3d71549390baca1ccf61d1d822b9408971705c687c9cd.lock when downloading gpt2-medium. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/file_download.py:1167\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWeakFileLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/huggingface_hub/utils/_fixes.py:109\u001b[0m, in \u001b[0;36mWeakFileLock\u001b[0;34m(lock_file, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melapsed_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Timeout:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/filelock/_api.py:332\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[0;34m(self, timeout, poll_interval, poll_intervall, blocking)\u001b[0m\n\u001b[1;32m    331\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to acquire lock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/filelock/_unix.py:42\u001b[0m, in \u001b[0;36mUnixFileLock._acquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     open_flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mO_CREAT\n\u001b[0;32m---> 42\u001b[0m fd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopen_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mPermissionError\u001b[39;00m):  \u001b[38;5;66;03m# This locked is not owned by this UID\u001b[39;00m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/net/projects2/chai-lab/shared_models/hub/.locks/models--gpt2-medium/fc5a354a19255ad494f3d71549390baca1ccf61d1d822b9408971705c687c9cd.lock'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load GPT2-medium (from original study)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GPT2-medium...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m gpt2_medium \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt2-medium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2-medium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load GPT2-large (NOT used in original study - for GT1 test)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/modeling_utils.py:4900\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4891\u001b[0m     gguf_file\n\u001b[1;32m   4892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4894\u001b[0m ):\n\u001b[1;32m   4895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4898\u001b[0m     )\n\u001b[0;32m-> 4900\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4907\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4920\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4921\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/modeling_utils.py:1037\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   1036\u001b[0m     }\n\u001b[0;32m-> 1037\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:524\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mPermissionError\u001b[39;00m):\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    531\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    532\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    534\u001b[0m ]\n",
      "\u001b[0;31mOSError\u001b[0m: PermissionError at /net/projects2/chai-lab/shared_models/hub/.locks/models--gpt2-medium/fc5a354a19255ad494f3d71549390baca1ccf61d1d822b9408971705c687c9cd.lock when downloading gpt2-medium. Check cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); 2) a previous download was canceled and the lock file needs manual removal."
     ]
    }
   ],
   "source": [
    "# Load the original model (GPT2-medium) and a new model (GPT2-large - not used in original paper)\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load GPT2-medium (from original study)\n",
    "print(\"Loading GPT2-medium...\")\n",
    "gpt2_medium = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Load GPT2-large (NOT used in original study - for GT1 test)\n",
    "print(\"Loading GPT2-large...\")\n",
    "gpt2_large = GPT2LMHeadModel.from_pretrained('gpt2-large').to(device)\n",
    "tokenizer_large = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c18d33c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading GPT2-medium...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c8ba16fdc049b180b954e679da984d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set a custom cache directory to avoid permission issues\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/home/smallyan/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/smallyan/.cache/huggingface/transformers'\n",
    "os.makedirs('/home/smallyan/.cache/huggingface/transformers', exist_ok=True)\n",
    "\n",
    "# Reload transformers\n",
    "import importlib\n",
    "import transformers\n",
    "importlib.reload(transformers)\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load GPT2-medium (from original study)\n",
    "print(\"Loading GPT2-medium...\")\n",
    "gpt2_medium = GPT2LMHeadModel.from_pretrained('gpt2-medium', cache_dir='/home/smallyan/.cache/huggingface/transformers').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', cache_dir='/home/smallyan/.cache/huggingface/transformers')\n",
    "\n",
    "print(\"GPT2-medium loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52a309dd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Check if model was loaded\n",
    "print(f\"GPT2-medium loaded: {gpt2_medium is not None}\")\n",
    "print(f\"Model config: {gpt2_medium.config.n_layer} layers, {gpt2_medium.config.n_head} heads, {gpt2_medium.config.n_embd} embd dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94d0245a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Wait and retry to check model\n",
    "import time\n",
    "time.sleep(5)\n",
    "try:\n",
    "    print(f\"GPT2-medium loaded: {gpt2_medium is not None}\")\n",
    "    print(f\"Model config: {gpt2_medium.config.n_layer} layers, {gpt2_medium.config.n_head} heads, {gpt2_medium.config.n_embd} embd dim\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0545a12",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# The model download might be running. Let me check if it completed\n",
    "print(\"Checking model status...\")\n",
    "print(type(gpt2_medium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a65effc1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Simple print test\n",
    "print(\"Test output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-08-20-42_CircuitAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
