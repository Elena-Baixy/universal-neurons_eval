{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51b2012",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['HF_HOME'] = '/home/smallyan/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/smallyan/.cache/huggingface/transformers'\n",
    "os.makedirs('/home/smallyan/.cache/huggingface/transformers', exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "import subprocess\n",
    "result = subprocess.run(['bash', '-c', 'source /home/smallyan/.bashrc && env'], capture_output=True, text=True)\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if '=' in line:\n",
    "        key, _, value = line.partition('=')\n",
    "        if 'TOKEN' in key or 'API' in key or 'KEY' in key:\n",
    "            os.environ[key] = value\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92881cc5",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for Universal Neurons\n",
    "\n",
    "This notebook evaluates whether the findings in the `universal-neurons_eval` repository generalize beyond the original experimental setting.\n",
    "\n",
    "## Evaluation Checklist\n",
    "- **GT1**: Generalization to a New Model\n",
    "- **GT2**: Generalization to New Data  \n",
    "- **GT3**: Method / Specificity Generalizability\n",
    "\n",
    "## Key Findings from Original Work\n",
    "\n",
    "1. **Universal neurons** are neurons that activate on the same inputs across different models trained from different random seeds\n",
    "2. Universal neurons have **excess correlation > 0.5** and comprise only 1-5% of neurons\n",
    "3. They have **statistical signatures**: large negative input bias, high activation skew/kurtosis, high weight norm\n",
    "4. **Models used**: GPT2-small, GPT2-medium, Pythia-160m\n",
    "5. **Dataset**: Pile test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d286f9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894351c4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT2-large (new model not used in original study)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d1a7bd8aed4b28a08810feacb16849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a74996577c482caedace351e8b36ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GPT2-large (NOT used in original study - for GT1 test)\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "cache_dir = '/home/smallyan/.cache/huggingface/transformers'\n",
    "\n",
    "print(\"Loading GPT2-large (new model not used in original study)...\")\n",
    "gpt2_large = GPT2LMHeadModel.from_pretrained('gpt2-large', cache_dir=cache_dir).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large', cache_dir=cache_dir)\n",
    "print(f\"GPT2-large loaded: {gpt2_large.config.n_layer} layers, {gpt2_large.config.n_head} heads, {gpt2_large.config.n_embd} embd dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb53497",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Check if model loaded successfully\n",
    "print(f\"GPT2-large model type: {type(gpt2_large)}\")\n",
    "print(f\"Config: {gpt2_large.config.n_layer} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9df7893",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# The download might still be in progress. Let's wait and retry\n",
    "import time\n",
    "time.sleep(10)\n",
    "print(\"Checking model status...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-08-21-10_GeneralizationEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
