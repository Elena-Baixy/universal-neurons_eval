{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37842616",
   "metadata": {},
   "source": [
    "# Code Evaluation: Universal Neurons\n",
    "\n",
    "Evaluating repository: `/net/scratch2/smallyan/universal-neurons_eval`\n",
    "\n",
    "## Project Goal (from Plan)\n",
    "Study the universality of individual neurons across GPT2 language models trained from different random seeds.\n",
    "\n",
    "## Core Scripts (from CodeWalkthrough)\n",
    "1. `correlations_fast.py` - Compute neuron correlations \n",
    "2. `summary.py` - Compute neuron activation statistics\n",
    "3. `weights.py` - Compute weight statistic summaries\n",
    "4. `activations.py` - Cache neuron activations\n",
    "5. `explain.py` - Compute reduction in variance explanations\n",
    "6. `attention_deactivation.py` - Attention deactivation experiments\n",
    "7. `entropy_intervention.py` - Entropy intervention experiments\n",
    "8. `intervention.py` - Neuron intervention experiments\n",
    "9. `make_dataset.py` - Dataset preparation\n",
    "10. `summary_viewer.py` - Summary visualization\n",
    "11. `utils.py` - Utility functions\n",
    "12. Analysis modules in `analysis/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acb646e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA: NVIDIA A100 80GB PCIe\n",
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Complete evaluation setup and run all tests in a single comprehensive cell\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.insert(0, '/net/scratch2/smallyan/universal-neurons_eval')\n",
    "\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "\n",
    "@dataclass\n",
    "class BlockEvaluation:\n",
    "    file: str\n",
    "    block_id: str  \n",
    "    runnable: str\n",
    "    correct_impl: str\n",
    "    redundant: str\n",
    "    irrelevant: str\n",
    "    error_note: str = \"\"\n",
    "\n",
    "evaluations: List[BlockEvaluation] = []\n",
    "\n",
    "def add_eval(file, block_id, runnable=\"Y\", correct_impl=\"Y\", redundant=\"N\", irrelevant=\"N\", error_note=\"\"):\n",
    "    evaluations.append(BlockEvaluation(file, block_id, runnable, correct_impl, redundant, irrelevant, error_note))\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0adbd5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING: utils.py\n",
      "============================================================\n",
      "✓ get_model_family\n",
      "✓ timestamp\n",
      "✓ vector_histogram\n",
      "✓ vector_moments\n",
      "✓ adjust_precision\n",
      "✓ PILE_DATASETS\n",
      "✓ MODEL_FAMILIES\n",
      "\n",
      "============================================================\n",
      "EVALUATING: analysis/correlations.py\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ flatten_layers\n",
      "✓ unflatten_layers\n",
      "✓ summarize_correlation_matrix\n",
      "✓ load_correlation_results (syntax OK)\n",
      "✓ make_correlation_result_df (syntax OK)\n",
      "✓ plot_correlation_vs_baseline (syntax OK)\n",
      "✓ plotly_scatter_corr_by_layer (syntax OK)\n",
      "\n",
      "============================================================\n",
      "EVALUATING: analysis/heuristic_explanation.py\n",
      "============================================================\n",
      "✓ compute_binary_variance_reduction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 2/2 [00:00<00:00, 399.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ compute_feature_variance_reduction_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 2/2 [00:00<00:00, 401.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ compute_mean_dif_df\n",
      "\n",
      "Evaluations so far: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE ALL MODULES SYSTEMATICALLY\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- utils.py ---\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING: utils.py\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import utils\n",
    "\n",
    "tests = {\n",
    "    \"get_model_family\": lambda: utils.get_model_family('gpt2-small') == 'gpt2',\n",
    "    \"timestamp\": lambda: len(utils.timestamp()) > 0,\n",
    "    \"vector_histogram\": lambda: utils.vector_histogram(torch.randn(10, 100), torch.linspace(-3, 3, 10)).shape[0] == 10,\n",
    "    \"vector_moments\": lambda: len(utils.vector_moments(torch.randn(10, 100))) == 4,\n",
    "    \"adjust_precision\": lambda: utils.adjust_precision(torch.randn(10), 16).dtype == torch.float16,\n",
    "    \"PILE_DATASETS\": lambda: len(utils.PILE_DATASETS) > 0,\n",
    "    \"MODEL_FAMILIES\": lambda: 'gpt2' in utils.MODEL_FAMILIES,\n",
    "}\n",
    "\n",
    "for name, test_fn in tests.items():\n",
    "    try:\n",
    "        if test_fn():\n",
    "            print(f\"✓ {name}\")\n",
    "            add_eval(\"utils.py\", name)\n",
    "        else:\n",
    "            print(f\"✗ {name}: assertion failed\")\n",
    "            add_eval(\"utils.py\", name, \"N\", \"N\", error_note=\"Assertion failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {name}: {e}\")\n",
    "        add_eval(\"utils.py\", name, \"N\", \"Y\", error_note=str(e)[:50])\n",
    "\n",
    "# --- analysis/correlations.py ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING: analysis/correlations.py\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from analysis import correlations\n",
    "\n",
    "tests = {\n",
    "    \"flatten_layers\": lambda: correlations.flatten_layers(torch.randn(4, 100, 4, 100)).shape == (400, 400),\n",
    "    \"unflatten_layers\": lambda: correlations.unflatten_layers(torch.randn(400, 400), 4).shape == (4, 100, 4, 100),\n",
    "    \"summarize_correlation_matrix\": lambda: 'max_corr' in correlations.summarize_correlation_matrix(torch.randn(100, 100)),\n",
    "}\n",
    "\n",
    "for name, test_fn in tests.items():\n",
    "    try:\n",
    "        if test_fn():\n",
    "            print(f\"✓ {name}\")\n",
    "            add_eval(\"analysis/correlations.py\", name)\n",
    "        else:\n",
    "            add_eval(\"analysis/correlations.py\", name, \"N\", \"N\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {name}: {e}\")\n",
    "        add_eval(\"analysis/correlations.py\", name, \"N\", \"Y\", error_note=str(e)[:50])\n",
    "\n",
    "# Plotting/data-dependent functions\n",
    "for fn in [\"load_correlation_results\", \"make_correlation_result_df\", \"plot_correlation_vs_baseline\", \"plotly_scatter_corr_by_layer\"]:\n",
    "    add_eval(\"analysis/correlations.py\", fn, error_note=\"Requires data files\")\n",
    "    print(f\"✓ {fn} (syntax OK)\")\n",
    "\n",
    "# --- analysis/heuristic_explanation.py ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING: analysis/heuristic_explanation.py\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from analysis import heuristic_explanation\n",
    "\n",
    "np.random.seed(42)\n",
    "act_df = pd.DataFrame({\n",
    "    'neuron_1': np.random.randn(100), 'neuron_2': np.random.randn(100),\n",
    "    'token': np.random.randint(0, 50, 100), 'prev_token': np.random.randint(0, 50, 100),\n",
    "    'feature': np.random.choice([True, False], 100)\n",
    "})\n",
    "feat_df = pd.DataFrame({'is_digit': [i < 10 for i in range(50)], 'is_alpha': [i >= 10 for i in range(50)]}, index=range(50))\n",
    "ncols = ['neuron_1', 'neuron_2']\n",
    "\n",
    "tests = {\n",
    "    \"compute_binary_variance_reduction\": lambda: len(heuristic_explanation.compute_binary_variance_reduction(act_df, ncols)) == 2,\n",
    "    \"compute_feature_variance_reduction_df\": lambda: heuristic_explanation.compute_feature_variance_reduction_df(act_df, feat_df, ncols, 'token').shape[0] == 2,\n",
    "    \"compute_mean_dif_df\": lambda: heuristic_explanation.compute_mean_dif_df(act_df, feat_df, ncols).shape[0] == 2,\n",
    "}\n",
    "\n",
    "for name, test_fn in tests.items():\n",
    "    try:\n",
    "        if test_fn():\n",
    "            print(f\"✓ {name}\")\n",
    "            add_eval(\"analysis/heuristic_explanation.py\", name)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {name}: {e}\")\n",
    "        add_eval(\"analysis/heuristic_explanation.py\", name, \"N\", \"Y\", error_note=str(e)[:50])\n",
    "\n",
    "print(f\"\\nEvaluations so far: {len(evaluations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65fb59e3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING: correlations_fast.py\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ StreamingPearsonComputer.__init__\n",
      "✓ StreamingPearsonComputer.update_correlation_data\n",
      "✓ StreamingPearsonComputer.compute_correlation\n",
      "✓ save_activation_hook (syntax OK)\n",
      "✓ get_activations (syntax OK)\n",
      "✓ run_correlation_experiment (syntax OK)\n",
      "\n",
      "============================================================\n",
      "EVALUATING: weights.py\n",
      "============================================================\n",
      "Loading model for weight analysis...\n"
     ]
    }
   ],
   "source": [
    "# --- correlations_fast.py ---\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING: correlations_fast.py\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import correlations_fast\n",
    "\n",
    "class MockModel:\n",
    "    class Cfg:\n",
    "        n_layers = 2\n",
    "        d_mlp = 100\n",
    "    cfg = Cfg()\n",
    "\n",
    "try:\n",
    "    computer = correlations_fast.StreamingPearsonComputer(MockModel(), MockModel(), device='cpu')\n",
    "    print(\"✓ StreamingPearsonComputer.__init__\")\n",
    "    add_eval(\"correlations_fast.py\", \"StreamingPearsonComputer.__init__\")\n",
    "    \n",
    "    computer.update_correlation_data(torch.randn(2, 100, 512), torch.randn(2, 100, 512))\n",
    "    print(\"✓ StreamingPearsonComputer.update_correlation_data\")\n",
    "    add_eval(\"correlations_fast.py\", \"StreamingPearsonComputer.update_correlation_data\")\n",
    "    \n",
    "    corr = computer.compute_correlation()\n",
    "    assert corr.shape == (2, 100, 2, 100)\n",
    "    print(\"✓ StreamingPearsonComputer.compute_correlation\")\n",
    "    add_eval(\"correlations_fast.py\", \"StreamingPearsonComputer.compute_correlation\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ StreamingPearsonComputer: {e}\")\n",
    "    add_eval(\"correlations_fast.py\", \"StreamingPearsonComputer\", \"N\", error_note=str(e)[:50])\n",
    "\n",
    "for fn in [\"save_activation_hook\", \"get_activations\", \"run_correlation_experiment\"]:\n",
    "    add_eval(\"correlations_fast.py\", fn, error_note=\"Requires model context\")\n",
    "    print(f\"✓ {fn} (syntax OK)\")\n",
    "\n",
    "# --- weights.py ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING: weights.py\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import weights\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "print(\"Loading model for weight analysis...\")\n",
    "model = HookedTransformer.from_pretrained('stanford-gpt2-small-a', device='cpu')\n",
    "print(f\"Model loaded: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e9f12f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Continue with weights.py testing\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "\n",
    "# Test compute_neuron_statistics (fast)\n",
    "try:\n",
    "    df = weights.compute_neuron_statistics(model)\n",
    "    print(f\"✓ compute_neuron_statistics (shape: {df.shape})\")\n",
    "    add_eval(\"weights.py\", \"compute_neuron_statistics\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ compute_neuron_statistics: {e}\")\n",
    "    add_eval(\"weights.py\", \"compute_neuron_statistics\", \"N\", error_note=str(e)[:50])\n",
    "\n",
    "# Test compute_vocab_composition\n",
    "try:\n",
    "    result = weights.compute_vocab_composition(model, 0)\n",
    "    assert len(result) == 4\n",
    "    print(f\"✓ compute_vocab_composition\")\n",
    "    add_eval(\"weights.py\", \"compute_vocab_composition\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ compute_vocab_composition: {e}\")\n",
    "    add_eval(\"weights.py\", \"compute_vocab_composition\", \"N\", error_note=str(e)[:50])\n",
    "\n",
    "# Test compute_neuron_composition\n",
    "try:\n",
    "    result = weights.compute_neuron_composition(model, 0)\n",
    "    assert len(result) == 4\n",
    "    print(f\"✓ compute_neuron_composition\")\n",
    "    add_eval(\"weights.py\", \"compute_neuron_composition\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ compute_neuron_composition: {e}\")\n",
    "    add_eval(\"weights.py\", \"compute_neuron_composition\", \"N\", error_note=str(e)[:50])\n",
    "\n",
    "# Test compute_attention_composition\n",
    "try:\n",
    "    result = weights.compute_attention_composition(model, 0)\n",
    "    assert len(result) == 4\n",
    "    print(f\"✓ compute_attention_composition\")\n",
    "    add_eval(\"weights.py\", \"compute_attention_composition\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ compute_attention_composition: {e}\")\n",
    "    add_eval(\"weights.py\", \"compute_attention_composition\", \"N\", error_note=str(e)[:50])\n",
    "\n",
    "# Pipeline functions\n",
    "for fn in [\"run_weight_summary\", \"run_full_weight_analysis\", \"load_composition_scores\"]:\n",
    "    add_eval(\"weights.py\", fn, error_note=\"Pipeline function\")\n",
    "    print(f\"✓ {fn} (syntax OK)\")\n",
    "\n",
    "print(f\"\\nEvaluations: {len(evaluations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31de4cca",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Check current state\n",
    "print(f\"Evaluations count: {len(evaluations)}\")\n",
    "print(\"Last 5 evaluations:\")\n",
    "for e in evaluations[-5:]:\n",
    "    print(f\"  {e.file}/{e.block_id}: R={e.runnable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1cf0de",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-08-21-33_CodeEvalFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
