{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1161b33c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Repo path: /net/scratch2/smallyan/universal-neurons_eval\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['HF_HOME'] = '/home/smallyan/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/smallyan/.cache/huggingface/transformers'\n",
    "os.makedirs('/home/smallyan/.cache/huggingface/transformers', exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "import subprocess\n",
    "result = subprocess.run(['bash', '-c', 'source /home/smallyan/.bashrc && env'], capture_output=True, text=True)\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if '=' in line:\n",
    "        key, _, value = line.partition('=')\n",
    "        if 'TOKEN' in key or 'API' in key or 'KEY' in key:\n",
    "            os.environ[key] = value\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "repo_path = '/net/scratch2/smallyan/universal-neurons_eval'\n",
    "print(f\"Repo path: {repo_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8785940",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for Universal Neurons\n",
    "\n",
    "This notebook evaluates whether the findings in the `universal-neurons_eval` repository generalize beyond the original experimental setting.\n",
    "\n",
    "## Evaluation Checklist\n",
    "- **GT1**: Generalization to a New Model\n",
    "- **GT2**: Generalization to New Data  \n",
    "- **GT3**: Method / Specificity Generalizability\n",
    "\n",
    "## Key Findings from Original Work\n",
    "\n",
    "1. **Universal neurons** are neurons that activate on the same inputs across different models trained from different random seeds\n",
    "2. Universal neurons have **excess correlation > 0.5** and comprise only 1-5% of neurons\n",
    "3. They have **statistical signatures**: large negative input bias, high activation skew/kurtosis, high weight norm\n",
    "4. **Models used**: GPT2-small, GPT2-medium, Pythia-160m\n",
    "5. **Dataset**: Pile test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bc0392",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal neurons: 1211\n",
      "Prediction neurons: 136\n",
      "\n",
      "Top universal neurons (by excess correlation):\n",
      "      layer  neuron  excess_corr  input_bias      skew       kurt\n",
      "156       1     657     0.808985   -0.747676  2.851113  26.548020\n",
      "16        0     553     0.794025   -0.365732  2.678056  18.010220\n",
      "327       3    1352     0.791950   -2.007722  1.144166   6.914615\n",
      "288       2    2130     0.787500   -0.563056  2.605676  15.836667\n",
      "4         0     185     0.781975   -0.610551  1.985424  21.138607\n",
      "173       1    1031     0.777950   -0.973660  2.880477  24.714020\n",
      "182       1    1393     0.776575   -0.910241  3.044531  32.796566\n",
      "127       0    3954     0.772575   -0.492060  4.159611  37.739380\n",
      "1168     23    2517     0.770200   -0.295914  2.378946  13.762036\n",
      "380       4    2534     0.768275   -1.329550  1.391916   8.143710\n"
     ]
    }
   ],
   "source": [
    "# Load the identified universal neurons from the original study\n",
    "interp_path = os.path.join(repo_path, 'dataframes', 'interpretable_neurons', 'stanford-gpt2-medium-a')\n",
    "universal_neurons_df = pd.read_csv(os.path.join(interp_path, 'universal.csv'))\n",
    "prediction_neurons_df = pd.read_csv(os.path.join(interp_path, 'prediction_neurons.csv'))\n",
    "\n",
    "print(f\"Universal neurons: {len(universal_neurons_df)}\")\n",
    "print(f\"Prediction neurons: {len(prediction_neurons_df)}\")\n",
    "\n",
    "# Look at some high-correlation universal neurons\n",
    "print(\"\\nTop universal neurons (by excess correlation):\")\n",
    "top_universal = universal_neurons_df.nlargest(10, 'excess_corr')[['layer', 'neuron', 'excess_corr', 'input_bias', 'skew', 'kurt']]\n",
    "print(top_universal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26f18fa",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT2-small...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aa7882d2874a889358d6f6a6f41fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fbaf240cc84519906afc6a0484aa90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GPT2-small first (faster) to verify setup works\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cache_dir = '/home/smallyan/.cache/huggingface/transformers'\n",
    "\n",
    "print(\"Loading GPT2-small...\")\n",
    "gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir=cache_dir).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"GPT2-small loaded: {gpt2_small.config.n_layer} layers, {gpt2_small.config.n_embd} embd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091c6378",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "print(f\"GPT2-small type: {type(gpt2_small)}\")\n",
    "print(f\"Layers: {gpt2_small.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9010d9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "x = 1 + 1\n",
    "print(f\"x = {x}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-08-21-25_GenEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
