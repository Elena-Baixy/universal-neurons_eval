{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3564fa54",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/adapter_config.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 122] Disk quota exceeded: '/net/projects/chai-lab/shared_models/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/adapter_config.json'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec94574f15434fe980b2f56072469aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Complete setup\n",
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "repo_path = '/net/scratch2/smallyan/belief_tracking_eval'\n",
    "sys.path.insert(0, repo_path)\n",
    "sys.path.insert(0, os.path.join(repo_path, 'notebooks', 'causalToM_novis'))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "from src.dataset import Sample, Dataset\n",
    "from utils import error_detection, get_answer_lookback_payload\n",
    "\n",
    "# Load entities\n",
    "data_path = os.path.join(repo_path, 'data', 'synthetic_entities')\n",
    "with open(os.path.join(data_path, 'characters.json'), 'r') as f:\n",
    "    all_characters = json.load(f)\n",
    "with open(os.path.join(data_path, 'bottles.json'), 'r') as f:\n",
    "    all_objects = json.load(f)\n",
    "with open(os.path.join(data_path, 'drinks.json'), 'r') as f:\n",
    "    all_states = json.load(f)\n",
    "\n",
    "# Load model\n",
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.float16, dispatch=True)\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edeea85",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GT2: Testing generalization to new data instances\n",
      "============================================================\n",
      "Created 20 new samples with novel entity combinations\n",
      "\n",
      "Example story:\n",
      "  Characters: ['Sara', 'Jim']\n",
      "  Objects: ['mug', 'drum']\n",
      "  States: ['port', 'bourbon']\n"
     ]
    }
   ],
   "source": [
    "# GT2: Test with NEW data instances - use a different random seed to get \n",
    "# entity combinations that weren't used in original experiments\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GT2: Testing generalization to new data instances\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use a new seed to get different entity combinations\n",
    "random.seed(999)  # Different from seed used in original experiments\n",
    "\n",
    "n_samples = 20\n",
    "new_dataset = get_answer_lookback_payload(all_characters, all_objects, all_states, n_samples)\n",
    "new_dataloader = DataLoader(new_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Show example\n",
    "print(f\"Created {len(new_dataset)} new samples with novel entity combinations\")\n",
    "print(f\"\\nExample story:\")\n",
    "print(f\"  Characters: {new_dataset[0]['clean_characters']}\")\n",
    "print(f\"  Objects: {new_dataset[0]['clean_objects']}\")\n",
    "print(f\"  States: {new_dataset[0]['clean_states']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75a7443",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding valid samples on new data combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:01<00:23,  1.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:02<00:18,  1.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:03<00:17,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:04<00:15,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:04<00:14,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:05<00:13,  1.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:06<00:12,  1.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:07<00:11,  1.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:08<00:10,  1.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:09<00:09,  1.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:10<00:08,  1.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:11<00:07,  1.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:12<00:06,  1.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:13<00:05,  1.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:14<00:04,  1.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:14<00:03,  1.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:16<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:16<00:01,  1.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:17<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [00:18<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [00:18<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 valid samples out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Find valid samples\n",
    "print(\"Finding valid samples on new data combinations...\")\n",
    "_, new_errors = error_detection(model, new_dataloader, is_remote=False)\n",
    "new_valid = [i for i in range(len(new_dataset)) if i not in new_errors]\n",
    "print(f\"Found {len(new_valid)} valid samples out of {len(new_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebf41ea",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running IIA on 3 new data samples\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  0: IIA = 0.000 (0/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 10: IIA = 0.000 (0/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20: IIA = 0.000 (0/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24: IIA = 0.333 (1/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 28: IIA = 1.000 (3/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 31: IIA = 1.000 (3/3)\n",
      "============================================================\n",
      "Peak IIA: 1.000 at layer 28\n",
      "GT2 PASS: True\n"
     ]
    }
   ],
   "source": [
    "# Run IIA experiment on new data - use 3 trial examples\n",
    "test_indices = new_valid[:3]\n",
    "patch_layers = [0, 10, 20, 24, 28, 31]\n",
    "\n",
    "print(f\"\\nRunning IIA on {len(test_indices)} new data samples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gt2_accs = {}\n",
    "for layer_idx in patch_layers:\n",
    "    correct, total = 0, 0\n",
    "    for bi in test_indices:\n",
    "        batch = new_dataset[bi]\n",
    "        counterfactual_prompt = batch[\"counterfactual_prompt\"]\n",
    "        clean_prompt = batch[\"clean_prompt\"]\n",
    "        target = batch[\"target\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with model.trace(counterfactual_prompt):\n",
    "                cf_out = model.model.layers[layer_idx].output[0][0, -1].save()\n",
    "            \n",
    "            with model.trace(clean_prompt):\n",
    "                model.model.layers[layer_idx].output[0][0, -1] = cf_out\n",
    "                pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "\n",
    "            pred_text = model.tokenizer.decode([pred]).lower().strip()\n",
    "            if pred_text == target.lower().strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"Layer {layer_idx:2d}: IIA = {acc:.3f} ({correct}/{total})\")\n",
    "    gt2_accs[layer_idx] = acc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "gt2_peak_layer = max(gt2_accs, key=gt2_accs.get)\n",
    "gt2_peak_iia = gt2_accs[gt2_peak_layer]\n",
    "print(f\"Peak IIA: {gt2_peak_iia:.3f} at layer {gt2_peak_layer}\")\n",
    "print(f\"GT2 PASS: {gt2_peak_iia >= 0.33}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86417e13",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GT3: Method Generalizability Assessment\n",
      "============================================================\n",
      "\n",
      "The paper proposes a NEW METHOD: Causal Abstraction with Interchange Interventions\n",
      "for identifying layer-specific mechanisms in language models.\n",
      "\n",
      "Key components:\n",
      "1. Create counterfactual pairs (clean vs modified input)\n",
      "2. Perform layer-wise activation patching\n",
      "3. Measure Interchange Intervention Accuracy (IIA)\n",
      "4. Identify layers where specific information is encoded\n",
      "\n",
      "To test GT3, we apply this method to a DIFFERENT but related task:\n",
      "- Original task: Belief tracking (Theory of Mind)\n",
      "- New task: Simple object location tracking (no belief, just factual)\n",
      "\n",
      "If the method generalizes, we should be able to identify layer-specific\n",
      "patterns for factual information retrieval.\n",
      "\n",
      "Creating simple object tracking dataset...\n"
     ]
    }
   ],
   "source": [
    "# GT3: Method Generalizability\n",
    "# The paper proposes the causal abstraction method with interchange interventions\n",
    "# to identify layer-specific mechanisms for belief tracking\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GT3: Method Generalizability Assessment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The key method is: Causal Abstraction with Interchange Interventions\n",
    "# This can be applied to any task where we want to identify where specific \n",
    "# information is encoded in the model\n",
    "\n",
    "# For GT3, we test if the method can be applied to a SIMILAR but DIFFERENT task\n",
    "# Similar task: Object location tracking (simpler than belief tracking)\n",
    "# The method should reveal similar layer-specific patterns\n",
    "\n",
    "print(\"\"\"\n",
    "The paper proposes a NEW METHOD: Causal Abstraction with Interchange Interventions\n",
    "for identifying layer-specific mechanisms in language models.\n",
    "\n",
    "Key components:\n",
    "1. Create counterfactual pairs (clean vs modified input)\n",
    "2. Perform layer-wise activation patching\n",
    "3. Measure Interchange Intervention Accuracy (IIA)\n",
    "4. Identify layers where specific information is encoded\n",
    "\n",
    "To test GT3, we apply this method to a DIFFERENT but related task:\n",
    "- Original task: Belief tracking (Theory of Mind)\n",
    "- New task: Simple object location tracking (no belief, just factual)\n",
    "\n",
    "If the method generalizes, we should be able to identify layer-specific\n",
    "patterns for factual information retrieval.\n",
    "\"\"\")\n",
    "\n",
    "# Create a simple object tracking task (no belief component)\n",
    "simple_template = \"\"\"Story: {char1} puts a {obj} in the {loc1}. Then {char1} moves the {obj} to the {loc2}.\n",
    "Question: Where is the {obj}?\n",
    "Answer:\"\"\"\n",
    "\n",
    "# For this, we'll use the same intervention technique but on a simpler task\n",
    "print(\"Creating simple object tracking dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3ef53b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example location tracking sample:\n",
      "Clean: Story: Frank puts a pen in the office. Then Frank moves the pen to the garage.\n",
      "Question: Where is the pen?\n",
      "Answer:\n",
      "Clean answer: garage\n",
      "CF answer: office\n"
     ]
    }
   ],
   "source": [
    "# Create a simple factual retrieval task - object location tracking\n",
    "# This is a different task but uses the same interchange intervention method\n",
    "\n",
    "locations = [\"kitchen\", \"bedroom\", \"bathroom\", \"garage\", \"garden\", \"basement\", \"attic\", \"office\"]\n",
    "objects_simple = [\"ball\", \"book\", \"key\", \"phone\", \"wallet\", \"remote\", \"watch\", \"pen\"]\n",
    "names = [\"Alice\", \"Bob\", \"Carol\", \"Dave\", \"Eve\", \"Frank\"]\n",
    "\n",
    "def create_location_tracking_sample(names, objects, locations):\n",
    "    name = random.choice(names)\n",
    "    obj = random.choice(objects)\n",
    "    loc1, loc2 = random.sample(locations, 2)\n",
    "    \n",
    "    # Clean prompt - object ends in loc2\n",
    "    clean_prompt = f\"Story: {name} puts a {obj} in the {loc1}. Then {name} moves the {obj} to the {loc2}.\\nQuestion: Where is the {obj}?\\nAnswer:\"\n",
    "    clean_answer = loc2\n",
    "    \n",
    "    # Counterfactual - swap locations\n",
    "    cf_prompt = f\"Story: {name} puts a {obj} in the {loc2}. Then {name} moves the {obj} to the {loc1}.\\nQuestion: Where is the {obj}?\\nAnswer:\"\n",
    "    cf_answer = loc1\n",
    "    \n",
    "    return {\n",
    "        \"clean_prompt\": clean_prompt,\n",
    "        \"clean_ans\": clean_answer,\n",
    "        \"counterfactual_prompt\": cf_prompt,\n",
    "        \"counterfactual_ans\": cf_answer,\n",
    "        \"target\": cf_answer  # What we expect after patching\n",
    "    }\n",
    "\n",
    "# Generate samples\n",
    "random.seed(456)\n",
    "location_samples = [create_location_tracking_sample(names, objects_simple, locations) for _ in range(10)]\n",
    "\n",
    "print(\"Example location tracking sample:\")\n",
    "print(f\"Clean: {location_samples[0]['clean_prompt']}\")\n",
    "print(f\"Clean answer: {location_samples[0]['clean_ans']}\")\n",
    "print(f\"CF answer: {location_samples[0]['counterfactual_ans']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b140b31b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on location tracking task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples: 0/10\n"
     ]
    }
   ],
   "source": [
    "# Test model on location tracking task first\n",
    "print(\"Testing model on location tracking task...\")\n",
    "valid_location_samples = []\n",
    "\n",
    "for i, sample in enumerate(location_samples):\n",
    "    with torch.no_grad():\n",
    "        # Test clean\n",
    "        with model.trace(sample[\"clean_prompt\"]):\n",
    "            clean_pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "        clean_text = model.tokenizer.decode([clean_pred]).lower().strip()\n",
    "        \n",
    "        # Test counterfactual\n",
    "        with model.trace(sample[\"counterfactual_prompt\"]):\n",
    "            cf_pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "        cf_text = model.tokenizer.decode([cf_pred]).lower().strip()\n",
    "        \n",
    "        clean_correct = clean_text == sample[\"clean_ans\"].lower()\n",
    "        cf_correct = cf_text == sample[\"counterfactual_ans\"].lower()\n",
    "        \n",
    "        if clean_correct and cf_correct:\n",
    "            valid_location_samples.append(i)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Valid samples: {len(valid_location_samples)}/{len(location_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ba9a2a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model outputs on location task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Story: Frank puts a pen in the office. Then Frank moves the pen to the garage.\n",
      "Question: Where is the pen?\n",
      "Answer:\n",
      "Expected: garage\n",
      "Got: ' The'\n"
     ]
    }
   ],
   "source": [
    "# The model doesn't answer correctly - let's check what it outputs\n",
    "print(\"Checking model outputs on location task...\")\n",
    "sample = location_samples[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.trace(sample[\"clean_prompt\"]):\n",
    "        pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "    pred_text = model.tokenizer.decode([pred])\n",
    "    \n",
    "print(f\"Prompt: {sample['clean_prompt']}\")\n",
    "print(f\"Expected: {sample['clean_ans']}\")\n",
    "print(f\"Got: '{pred_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce79612",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Instruction: Answer the question based on the story.\n",
      "\n",
      "Story: Dave puts a ball in the office. Then Dave moves the ball to the bedroom.\n",
      "\n",
      "Question: Where is the ball now?\n",
      "Answer: The ball is in the\n",
      "Expected: bedroom\n",
      "Got: ' bedroom'\n"
     ]
    }
   ],
   "source": [
    "# Need to adjust the prompt format - model expects more structured output\n",
    "# Let's try a different format similar to the belief tracking prompts\n",
    "\n",
    "def create_location_tracking_sample_v2(names, objects, locations):\n",
    "    name = random.choice(names)\n",
    "    obj = random.choice(objects)\n",
    "    loc1, loc2 = random.sample(locations, 2)\n",
    "    \n",
    "    # Match format closer to belief tracking\n",
    "    clean_prompt = f\"Instruction: Answer the question based on the story.\\n\\nStory: {name} puts a {obj} in the {loc1}. Then {name} moves the {obj} to the {loc2}.\\n\\nQuestion: Where is the {obj} now?\\nAnswer: The {obj} is in the\"\n",
    "    clean_answer = loc2\n",
    "    \n",
    "    cf_prompt = f\"Instruction: Answer the question based on the story.\\n\\nStory: {name} puts a {obj} in the {loc2}. Then {name} moves the {obj} to the {loc1}.\\n\\nQuestion: Where is the {obj} now?\\nAnswer: The {obj} is in the\"\n",
    "    cf_answer = loc1\n",
    "    \n",
    "    return {\n",
    "        \"clean_prompt\": clean_prompt,\n",
    "        \"clean_ans\": clean_answer,\n",
    "        \"counterfactual_prompt\": cf_prompt,\n",
    "        \"counterfactual_ans\": cf_answer,\n",
    "        \"target\": cf_answer\n",
    "    }\n",
    "\n",
    "random.seed(789)\n",
    "location_samples_v2 = [create_location_tracking_sample_v2(names, objects_simple, locations) for _ in range(10)]\n",
    "\n",
    "# Test\n",
    "sample = location_samples_v2[0]\n",
    "with torch.no_grad():\n",
    "    with model.trace(sample[\"clean_prompt\"]):\n",
    "        pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "    pred_text = model.tokenizer.decode([pred])\n",
    "    \n",
    "print(f\"Prompt: {sample['clean_prompt']}\")\n",
    "print(f\"Expected: {sample['clean_ans']}\")\n",
    "print(f\"Got: '{pred_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4083680b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on location tracking task v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid samples: 10/10\n"
     ]
    }
   ],
   "source": [
    "# Great! Now test for valid samples\n",
    "print(\"Testing model on location tracking task v2...\")\n",
    "valid_location_samples = []\n",
    "\n",
    "for i, sample in enumerate(location_samples_v2):\n",
    "    with torch.no_grad():\n",
    "        with model.trace(sample[\"clean_prompt\"]):\n",
    "            clean_pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "        clean_text = model.tokenizer.decode([clean_pred]).lower().strip()\n",
    "        \n",
    "        with model.trace(sample[\"counterfactual_prompt\"]):\n",
    "            cf_pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
    "        cf_text = model.tokenizer.decode([cf_pred]).lower().strip()\n",
    "        \n",
    "        clean_correct = clean_text == sample[\"clean_ans\"].lower()\n",
    "        cf_correct = cf_text == sample[\"counterfactual_ans\"].lower()\n",
    "        \n",
    "        if clean_correct and cf_correct:\n",
    "            valid_location_samples.append(i)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Valid samples: {len(valid_location_samples)}/{len(location_samples_v2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-21-02_GenEval4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
