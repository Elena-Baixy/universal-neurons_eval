{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa11f03",
   "metadata": {},
   "source": [
    "# Replication: Vector Arithmetic in Concept and Token Subspaces\n",
    "\n",
    "This notebook replicates the experiment from the paper \"Vector Arithmetic in Concept and Token Subspaces\" (NeurIPS 2025 Mechanistic Interpretability Workshop).\n",
    "\n",
    "**Goal**: Show that concept and token induction heads can identify subspaces for more accurate parallelogram arithmetic (e.g., Athens â€“ Greece + China = Beijing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6049ac5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True, Device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os, json, torch, gc\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "repo_path = '/net/scratch2/smallyan/arithmetic_eval'\n",
    "cache_path = os.path.join(repo_path, 'cache')\n",
    "data_path = os.path.join(repo_path, 'data')\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf10cd2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-2-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d5b625f2514afca2be6523586c9ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Config: hidden_size=4096, layers=32, heads=32\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from nnsight import LanguageModel\n",
    "print(\"Loading Llama-2-7b...\")\n",
    "model = LanguageModel('meta-llama/Llama-2-7b-hf', device_map='cuda', dispatch=True)\n",
    "print(f\"Model loaded. Config: hidden_size={model.config.hidden_size}, layers={model.config.num_hidden_layers}, heads={model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f6b0e4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded head scores. Top concept: (14, 1, 0.0010720472782850266), Top token: (16, 19, 0.0021728137508034706)\n"
     ]
    }
   ],
   "source": [
    "# Load head ordering scores\n",
    "llama2_cache = os.path.join(cache_path, 'causal_scores', 'Llama-2-7b-hf')\n",
    "with open(os.path.join(llama2_cache, 'concept_copying_len30_n1024.json'), 'r') as f:\n",
    "    concept_scores = json.load(f)\n",
    "with open(os.path.join(llama2_cache, 'token_copying_len30_n1024.json'), 'r') as f:\n",
    "    token_scores = json.load(f)\n",
    "\n",
    "concept_sorted = sorted([(d['layer'], d['head_idx'], d['score']) for d in concept_scores], key=lambda t: t[2], reverse=True)\n",
    "token_sorted = sorted([(d['layer'], d['head_idx'], d['score']) for d in token_scores], key=lambda t: t[2], reverse=True)\n",
    "print(f\"Loaded head scores. Top concept: {concept_sorted[0]}, Top token: {token_sorted[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46505b49",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Core functions\n",
    "def get_ov_sum(model, head_ordering: str, k: int = 80):\n",
    "    \"\"\"Construct lens matrix by summing OV matrices from top-k heads.\"\"\"\n",
    "    if head_ordering == 'raw':\n",
    "        return None\n",
    "    \n",
    "    head_dim = model.config.hidden_size // model.config.num_attention_heads\n",
    "    \n",
    "    if head_ordering == 'all':\n",
    "        to_sum = [(l, h) for l in range(model.config.num_hidden_layers) for h in range(model.config.num_attention_heads)]\n",
    "    else:\n",
    "        sorted_heads = concept_sorted if head_ordering == 'concept' else token_sorted\n",
    "        to_sum = [(l, h) for l, h, _ in sorted_heads][:k]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ov_sum = torch.zeros((4096, 4096), device='cuda')\n",
    "        for l, h in to_sum:\n",
    "            V = model.model.layers[l].self_attn.v_proj.weight[h * head_dim : (h+1) * head_dim]\n",
    "            O = model.model.layers[l].self_attn.o_proj.weight[:, h * head_dim : (h+1) * head_dim]\n",
    "            ov_sum += torch.matmul(O, V)\n",
    "    return ov_sum\n",
    "\n",
    "def get_word_rep(word: str, model, layer_idx: int, ov_sum, prefix: str = ''):\n",
    "    \"\"\"Get word representation at given layer, optionally transformed.\"\"\"\n",
    "    text = prefix + word.strip()\n",
    "    with torch.no_grad():\n",
    "        with model.trace(text):\n",
    "            state = model.model.layers[layer_idx].output[0].squeeze()[-1].detach().save()\n",
    "    return torch.matmul(ov_sum, state) if ov_sum is not None else state\n",
    "\n",
    "def load_task(task_name: str, dataset: str = 'word2vec'):\n",
    "    filepath = os.path.join(data_path, dataset, f'{task_name}.txt')\n",
    "    with open(filepath, 'r') as f:\n",
    "        return [l for l in f.read().split('\\n')[1:] if l.strip()]\n",
    "\n",
    "def run_experiment(task_name: str, layer: int, head_ordering: str, k: int = 80, dataset: str = 'word2vec'):\n",
    "    \"\"\"Run parallelogram experiment.\"\"\"\n",
    "    task_lines = load_task(task_name, dataset)\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    \n",
    "    # Get OV sum\n",
    "    ov_sum = get_ov_sum(model, head_ordering, k)\n",
    "    \n",
    "    # Get all word representations\n",
    "    words = set()\n",
    "    for line in task_lines:\n",
    "        words.update(line.split(sep))\n",
    "    neighbors = {w: get_word_rep(w, model, layer, ov_sum) for w in words}\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct, total = 0, 0\n",
    "    for line in task_lines:\n",
    "        parts = line.split(sep)\n",
    "        if len(parts) != 4:\n",
    "            continue\n",
    "        a, b, a_prime, b_prime = parts\n",
    "        result = neighbors[a] - neighbors[b] + neighbors[b_prime]\n",
    "        best = max(neighbors.keys(), key=lambda w: torch.cosine_similarity(result, neighbors[w], dim=0).item())\n",
    "        if best == a_prime:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    # Clean up\n",
    "    del neighbors, ov_sum\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return correct / total if total > 0 else 0, total\n",
    "\n",
    "print(\"Functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83941d",
   "metadata": {},
   "source": [
    "## Experiment 1: Capital-Common-Countries (Semantic Task)\n",
    "\n",
    "Expected: Concept lens should achieve ~90% accuracy, outperforming raw (~16%) and token (~7%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35acfcf8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing capital-common-countries, layer 20, concept lens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy = 0.8953 (n=506)\n",
      "Expected: accuracy = 0.8953\n"
     ]
    }
   ],
   "source": [
    "# Test single configuration first\n",
    "print(\"Testing capital-common-countries, layer 20, concept lens...\")\n",
    "acc, n = run_experiment('capital-common-countries', layer=20, head_ordering='concept')\n",
    "print(f\"Result: accuracy = {acc:.4f} (n={n})\")\n",
    "\n",
    "# Compare with expected from cache\n",
    "with open(os.path.join(cache_path, 'parallelograms/word2vec/no_prefix/concept/capital-common-countries/layer20_results.json'), 'r') as f:\n",
    "    expected = json.load(f)\n",
    "print(f\"Expected: accuracy = {expected['nn_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12b251f",
   "metadata": {
    "execution_status": "running"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capital-Common-Countries Results:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw      layer 16: 0.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw      layer 20: 0.1581\n"
     ]
    }
   ],
   "source": [
    "# Run for key layers (16 and 20) across all orderings for capital task\n",
    "capital_results = {}\n",
    "key_layers = [16, 20]\n",
    "orderings = ['raw', 'concept', 'token', 'all']\n",
    "\n",
    "print(\"Capital-Common-Countries Results:\")\n",
    "print(\"-\" * 50)\n",
    "for ordering in orderings:\n",
    "    capital_results[ordering] = {}\n",
    "    for layer in key_layers:\n",
    "        acc, n = run_experiment('capital-common-countries', layer=layer, head_ordering=ordering)\n",
    "        capital_results[ordering][layer] = acc\n",
    "        print(f\"{ordering:8} layer {layer}: {acc:.4f}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-20-22_ArithmeticReplication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
