{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6f9a87",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c590392",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to repository: /net/scratch2/smallyan/universal-neurons_eval\n",
      "\n",
      "Directory contents:\n",
      "total 3985\n",
      "drwxrwx---  8 smallyan smallyan      28 Jan  9 16:56 .\n",
      "drwxrwx--- 40 smallyan smallyan      42 Jan 11 07:09 ..\n",
      "-rw-rw----  1 smallyan smallyan   12692 Dec 22 21:49 activations.py\n",
      "drwxrwx---  3 smallyan smallyan      14 Dec 23 23:57 analysis\n",
      "-rw-rw----  1 smallyan smallyan    8160 Dec 22 21:49 attention_deactivation.py\n",
      "-rw-rw----  1 smallyan smallyan    7845 Dec 22 21:49 attention_deactivation_qpos.py\n",
      "-rw-rw----  1 smallyan smallyan    4519 Dec 22 21:49 CodeWalkthrough.md\n",
      "-rw-rw----  1 smallyan smallyan   10191 Dec 22 21:49 correlations_fast.py\n",
      "-rw-rw----  1 smallyan smallyan   14345 Dec 22 21:49 correlations_parallel.py\n",
      "-rw-rw----  1 smallyan smallyan   13230 Dec 22 21:49 correlations.py\n",
      "drwxrwx---  5 smallyan smallyan       5 Dec 22 21:49 dataframes\n",
      "-rw-r-----  1 smallyan smallyan 3861499 Dec 23 01:40 documentation.pdf\n",
      "-rw-rw----  1 smallyan smallyan    6106 Dec 22 21:49 entropy_intervention.py\n",
      "-rw-rw----  1 smallyan smallyan    4598 Dec 22 21:49 explain.py\n",
      "drwxrwx---  7 smallyan smallyan      15 Jan  9 16:56 .git\n",
      "-rw-rw----  1 smallyan smallyan    3219 Dec 22 21:49 .gitignore\n",
      "-rw-rw----  1 smallyan smallyan    5895 Dec 22 21:49 intervention.py\n",
      "-rw-rw----  1 smallyan smallyan    1067 Dec 22 21:49 LICENSE\n",
      "-rw-rw----  1 smallyan smallyan    6186 Dec 22 21:49 make_dataset.py\n",
      "drwxrwx---  2 smallyan smallyan      14 Dec 22 21:49 paper_notebooks\n",
      "-rw-rw----  1 smallyan smallyan    5740 Dec 23 01:57 plan.md\n",
      "drwxrwx---  2 smallyan smallyan      15 Dec 24 00:05 __pycache__\n",
      "-rw-rw----  1 smallyan smallyan     214 Dec 22 21:49 requirements.txt\n",
      "drwxrwx---  2 smallyan smallyan      24 Dec 22 21:49 slurm\n",
      "-rw-rw----  1 smallyan smallyan    8201 Dec 22 21:49 summary.py\n",
      "-rw-rw----  1 smallyan smallyan   22864 Dec 22 21:49 summary_viewer.py\n",
      "-rw-rw----  1 smallyan smallyan    3237 Dec 22 21:49 utils.py\n",
      "-rw-rw----  1 smallyan smallyan   14365 Dec 22 21:49 weights.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now navigate to the target repository\n",
    "repo_path = '/net/scratch2/smallyan/universal-neurons_eval'\n",
    "os.chdir(repo_path)\n",
    "print(f\"Changed to repository: {os.getcwd()}\")\n",
    "\n",
    "# List the directory contents\n",
    "import subprocess\n",
    "result = subprocess.run(['ls', '-la'], capture_output=True, text=True)\n",
    "print(\"\\nDirectory contents:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb63c47",
   "metadata": {},
   "source": [
    "# Universal Neurons in GPT2 Language Models - Replication\n",
    "\n",
    "This notebook replicates the experiments from \"Universal Neurons in GPT2 Language Models\" by Gurnee et al. (2024).\n",
    "\n",
    "## Experiment Overview\n",
    "\n",
    "The original study investigates the universality of individual neurons across GPT2 language models trained from different random seeds. The key hypothesis is that universal neurons (those that consistently activate on the same inputs across different models) are more likely to be monosemantic and interpretable.\n",
    "\n",
    "### Key Experiments:\n",
    "1. **Neuron correlation analysis**: Computing pairwise Pearson correlations of neuron activations across models\n",
    "2. **Statistical properties analysis**: Analyzing activation and weight statistics of universal vs non-universal neurons\n",
    "3. **Neuron family taxonomization**: Classifying neurons into families based on their behavior\n",
    "4. **Functional analysis**: Studying prediction, suppression, and partition neurons through weight analysis\n",
    "5. **Causal interventions**: Testing effects of fixing neuron activations\n",
    "\n",
    "### Replication Scope\n",
    "\n",
    "Due to computational constraints, this replication will focus on:\n",
    "1. Demonstrating the neuron correlation computation methodology on a small subset\n",
    "2. Analyzing statistical properties of neurons\n",
    "3. Reproducing the key analytical framework even if not at full scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9645474f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n",
      "CUDA device count: 1\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a66417e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ transformer-lens is installed\n",
      "✓ einops is installed\n",
      "✓ datasets is installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ circuitsvis is installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if needed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages_to_check = [\n",
    "    'transformer-lens',\n",
    "    'einops',\n",
    "    'datasets',\n",
    "    'circuitsvis'\n",
    "]\n",
    "\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✓ {package} is installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4242e",
   "metadata": {},
   "source": [
    "## Part 1: Load Models and Setup\n",
    "\n",
    "We'll load GPT-2 small models to demonstrate the correlation analysis. The original study used multiple models trained from different random seeds, but we'll work with available pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746e5734",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 small model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "\n",
      "Model configuration:\n",
      "  Layers: 12\n",
      "  MLP dimension: 3072\n",
      "  Attention heads: 12\n",
      "  Model dimension: 768\n",
      "  Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import einops\n",
    "\n",
    "# Load a smaller GPT-2 model for demonstration\n",
    "print(\"Loading GPT-2 small model...\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  MLP dimension: {model.cfg.d_mlp}\")\n",
    "print(f\"  Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"  Model dimension: {model.cfg.d_model}\")\n",
    "print(f\"  Vocabulary size: {model.cfg.d_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362513d",
   "metadata": {},
   "source": [
    "## Part 2: Activation Collection\n",
    "\n",
    "The original study computed neuron activations over 100 million tokens from the Pile test set. For this replication, we'll use a smaller sample to demonstrate the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d6820f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc33d8f9be4e4b81828dc831ddb0ddc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560152cb96f04b4dbad8699f9002de79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small dataset for demonstration\n",
    "# Using a subset of the pile or similar dataset\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# For replication, we'll use a small sample from a text dataset\n",
    "# The original uses 100M tokens from Pile test set\n",
    "try:\n",
    "    # Try loading pile if available\n",
    "    dataset = load_dataset(\"EleutherAI/pile\", split=\"test\", streaming=True)\n",
    "    dataset_iter = iter(dataset)\n",
    "    \n",
    "    # Collect some samples\n",
    "    num_samples = 100  # Much smaller than original for demo\n",
    "    texts = []\n",
    "    for i, sample in enumerate(dataset_iter):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        texts.append(sample['text'])\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} samples from Pile test set\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load Pile dataset ({e}), using alternative...\")\n",
    "    # Fallback to OpenWebText or another dataset\n",
    "    dataset = load_dataset(\"openwebtext\", split=\"train\", streaming=True)\n",
    "    dataset_iter = iter(dataset)\n",
    "    \n",
    "    num_samples = 100\n",
    "    texts = []\n",
    "    for i, sample in enumerate(dataset_iter):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        texts.append(sample['text'])\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} samples from OpenWebText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b517f7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Use a simpler approach with sample texts for demonstration\n",
    "# In the original study, they use 100M tokens from Pile test set\n",
    "# For this replication, we'll create a smaller representative sample\n",
    "\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"The theory of relativity was developed by Albert Einstein.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning models use neural networks with multiple layers.\",\n",
    "    \"The capital of France is Paris, a beautiful city.\",\n",
    "    \"Mathematics is the foundation of computer science and engineering.\",\n",
    "    \"Climate change is one of the most pressing challenges of our time.\",\n",
    "    \"The internet has revolutionized how we communicate and share information.\"\n",
    "] * 20  # Repeat to get more samples\n",
    "\n",
    "print(f\"Created {len(sample_texts)} sample texts for demonstration\")\n",
    "print(f\"\\nNote: Original study uses 100M tokens from Pile test set.\")\n",
    "print(f\"This is a simplified demonstration with ~{sum(len(t.split()) for t in sample_texts)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5739df9c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Actually create the texts\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"The theory of relativity was developed by Albert Einstein.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning models use neural networks with multiple layers.\",\n",
    "    \"The capital of France is Paris, a beautiful city.\",\n",
    "    \"Mathematics is the foundation of computer science and engineering.\",\n",
    "    \"Climate change is one of the most pressing challenges of our time.\",\n",
    "    \"The internet has revolutionized how we communicate and share information.\"\n",
    "] * 20\n",
    "\n",
    "print(f\"Created {len(sample_texts)} sample texts\")\n",
    "print(f\"Total tokens (approximate): {sum(len(t.split()) for t in sample_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33607393",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "print(\"Test print\")\n",
    "x = 5\n",
    "print(f\"x = {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fee1c98",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Restart with clean implementation\n",
    "import sys\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Create sample texts for demonstration\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"The theory of relativity was developed by Albert Einstein.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "] * 10\n",
    "\n",
    "total_words = sum(len(t.split()) for t in sample_texts)\n",
    "print(f\"Created {len(sample_texts)} sample texts with ~{total_words} words\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2026-01-11-12-32_CircuitAnalysisReplication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
